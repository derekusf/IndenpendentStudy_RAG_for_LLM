{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install neccessary Library\n",
    "The libraries include:\n",
    "- langchain framework'\n",
    "- GPT4ALL, OpenAI and HuggingFace for various embedding methods and LLMs\n",
    "- Document loaders\n",
    "- Dependent libraries\n",
    "\n",
    "__Note__ : \n",
    "- It requires C++ builder for building a dependant library for Chroma. Check out https://github.com/bycloudai/InstallVSBuildToolsWindows for instruction. \n",
    "- Python version: 3.12.4\n",
    "- Pydantic version: 2.7.3. There is issue with pydantic version 1.10.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters\n",
    "Prepare the list of parameter in .env file for later use. \n",
    "Parameters: \n",
    "- API keys for LLMs\n",
    "    - OPENAI_API_KEY \n",
    "    - HUGGINGFACEHUB_API_TOKEN \n",
    "- Directory / location for documents and vector databases\n",
    "    - DOC_ARVIX = \"./source/from_arvix/\"\n",
    "    - DOC_WIKI = \"./source/from_wiki/\"\n",
    "    - VECTORDB_OPENAI_EM = \"./vector_db/openai_embedding/\"\n",
    "    - VECTORDB_MINILM_EM = \"./vector_db/gpt4all_miniLM/\"\n",
    "    - TS_RAGAS = \"./evaluation/testset/by_RAGAS/\"\n",
    "    - TS_PROMPT = \"./evaluation/testset/by_direct_prompt/\"\n",
    "    - EVAL_DATASET = \"./evaluation/evaluation_data_set/\"\n",
    "    - EVAL_METRIC = \"./evaluation/evaluation_metric\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Build a simple RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/HL architecture.png\" alt=\"HL arc\" title= \"HL Architecture\" />\n",
    "\n",
    "The system comprises of 5 components: \n",
    "\n",
    "- Internal data, documents: The system starts with a collection of internal documents and / or structured databases. Documents can be in text, PDF, photo or video formats. These documents and data are sources for the specified knowledgebase.\n",
    "\n",
    "- Embedding processor: The documents and database entries are processed to create vector embeddings. Embeddings are numerical representations of the documents in a high-dimensional space that capture their semantic meaning. \n",
    "\n",
    "- Vector database: the vectorized chunk of documents and database entries are stored on vector database to be search and retrieved in a later stage. \n",
    "\n",
    "- Query processor: The query processor takes the user's query and performs semantic search against the vectorized database. This component ensures that the query is interpreted correctly and retrieves relevant document embeddings from the vectorized DB. It combines the user's original query with the retrieved document embeddings to form a context-rich query. This augmented query provides additional context that can help in generating a more accurate and relevant response.\n",
    "\n",
    "- LLM: pre-trained large language model where the augmented query is passed to for generating a response based on the query and the relevant documents.\n",
    "\n",
    "The system involves 2 main pipelines: the embedding pipeline and the retrieval pipeline. Each pipeline has specific stages and processes that contribute to the overall functionality of the system.\n",
    "\n",
    "In this experiment, we use Langchain as a framework to build a simple RAG as a chain of tasks, which interacts with surrounding services like parsing, embedding, vector database and LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1 - Knowledge Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline 1: Embedding pipeline is to initiate the vectorized knowledgebase. It can be run whenever the knowledgebase needs to update. \n",
    "\n",
    "<img src=\"diagrams/Pipeline 1 - Knowledge Embedding.png\" alt=\"Pipeline1\" title=\"Pipeline 1 - Embeddings\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest.\n",
    "We will download 5 articles from ARVIX with query \"RAG for Large Language Model\" and store them locally and ready for next steps of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"RAG for Large Language Model\",\n",
    "  max_results = 5,\n",
    "#  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries http://arxiv.org/abs/2401.15391v1\n",
      "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine http://arxiv.org/abs/2401.11246v1\n",
      "Seven Failure Points When Engineering a Retrieval Augmented Generation System http://arxiv.org/abs/2401.05856v1\n",
      "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG) http://arxiv.org/abs/2402.16893v1\n",
      "CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems http://arxiv.org/abs/2404.02103v1\n"
     ]
    }
   ],
   "source": [
    "# Print out the articles' titles\n",
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: download articles and save them in pre-defined location for later use\n",
    "# Prepare: create the environment paramter DOC_ARVIX for the path to save articles. \n",
    "# Download and save articles in PDF format to the \"RAG_for_LLM\" folder under ARVIX_DOC path\n",
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX,\"RAG_for_LLM\") \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step and the previous one are usually processed together. I try to separate them to make attention that these are not always coupled.\n",
    "We use available library DirectoryLoader and PyMuPDFLoader from Langchain to load and parse all .pdf files in the directory.\n",
    "We can use corresponding loader for other data types such as excel, presentation, unstructured ... \n",
    "\n",
    "Refer to https://python.langchain.com/v0.1/docs/integrations/document_loaders/ for other available loaders. \n",
    "We also use the OCR library rapidocr to extract image as text. Certainly, the trade-off is processing time. It took 18 minutes to parse 5 pdf files with OCR compared to 0.1 second without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "directory_path = os.path.join(DOC_ARVIX,\"RAG_for_LLM\") \n",
    "loader_kwargs = {\"extract_images\":True} #Use OCR to extract image as text\n",
    "pdf_loader = DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyMuPDFLoader,\n",
    "        loader_kwargs=loader_kwargs\n",
    "    )\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into smaller chunks for better handling, processing, and retrieving.\n",
    "There is a limitation on number of tokens which the embedding service can process at later stage which requires documents are chunked in smaller size.\n",
    "There are many of chunking methods from Langchain. In which, Recursive CharacterText and Semantic are most popular. \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=30)\n",
    "text_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are semantic representation of texts. \n",
    "This is an important step to make documents searchable in the later pipeline. \n",
    "Embedding is an essential step in Transformer architecture, underlined to every modern LLMs. Therefore, many LLMs provide their embedding functions as services which are ready to use, e.g. OpenAI embedding API. However, it is important to consider privacy risk when exposing internal data to those services.\n",
    "\n",
    "IMPORTANT NOTE: \n",
    "1. the embedding method to perform similarity search in the retrieval pipeline must be the same to the one used to vectorize documents in this step. \n",
    "2. Public embedding method such as OpenAIEmbedding may cost a fraction of money and leak internal data.  \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some vector databases of choices: Chroma, FAISS, Pinecone ... \n",
    "We will create Chroma vector database with openai embedding method. \n",
    "\n",
    "Note: different embedding methods will result different vector dimensions and cannot be stored together. \n",
    "The same embedding method to be used in retrieval pipeline\n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"VECTORDB_OPENAI_EM\")\n",
    "persist_directory = os.path.join(persist_directory,\"RAG_for_LLM\")\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=text_chunks,  embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2 - Retrieving & Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval pipeline is to retrieve relevant chunk of knowledge from pre-prepared vectorized knowledge to enrich the LLM prompt with specified context. This pipeline is run to respond to each user’s query. \n",
    "\n",
    "<img src=\"diagrams/Pipeline 2 - Retrieval.png\" alt=\"Pipeline2\" title=\"Pipeline 2 - Retrieval & Generation\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is retrieval augmented generation?\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is, here is Chroma vectordb we have just persisted. \n",
    "Perform a semantic search in the vectorized database to retrieve relevant embedded documents.\n",
    "\n",
    "NOTE: The embedding method used in this step must be same as which used to vectorize knowledges in the previous pipeline.\n",
    "\n",
    "There is opportunity to improve efficiency and quality of similarity search, especially when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "db_directory = os.getenv(\"VECTORDB_OPENAI_EM\")\n",
    "db_directory = os.path.join(db_directory,\"RAG_for_LLM\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma(persist_directory=db_directory, embedding_function=embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': \"D:20240120233737+09'00'\", 'creator': '', 'file_path': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2401.11246v1.Prompt_RAG__Pioneering_Vector_Embedding_Free_Retrieval_Augmented_Generation_in_Niche_Domains__Exemplified_by_Korean_Medicine.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240120233737+09'00'\", 'page': 1, 'producer': 'Microsoft: Print To PDF', 'source': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2401.11246v1.Prompt_RAG__Pioneering_Vector_Embedding_Free_Retrieval_Augmented_Generation_in_Niche_Domains__Exemplified_by_Korean_Medicine.pdf', 'subject': '', 'title': 'Microsoft Word - Prompt-GPT_v1', 'total_pages': 26, 'trapped': ''}, page_content='2 \\n1. Introduction \\nRetrieval-Augmented Generation (RAG) models combine a generative model with an information \\nretrieval function, designed to overcome the inherent constraints of generative models.(1) They \\nintegrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of \\nexternal information sources, resulting in responses that are not only natural and human-like but also'),\n",
       " Document(metadata={'author': '', 'creationDate': \"D:20240120233737+09'00'\", 'creator': '', 'file_path': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2401.11246v1.Prompt_RAG__Pioneering_Vector_Embedding_Free_Retrieval_Augmented_Generation_in_Niche_Domains__Exemplified_by_Korean_Medicine.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20240120233737+09'00'\", 'page': 20, 'producer': 'Microsoft: Print To PDF', 'source': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2401.11246v1.Prompt_RAG__Pioneering_Vector_Embedding_Free_Retrieval_Augmented_Generation_in_Niche_Domains__Exemplified_by_Korean_Medicine.pdf', 'subject': '', 'title': 'Microsoft Word - Prompt-GPT_v1', 'total_pages': 26, 'trapped': ''}, page_content='augmented generation: A survey. arXiv preprint arXiv:230310868. 2023. \\n7. \\nLi H, Su Y, Cai D, Wang Y, Liu L. A survey on retrieval-augmented text generation. arXiv \\npreprint arXiv:220201110. 2022. \\n8. \\nGao Y, Xiong Y, Gao X, Jia K, Pan J, Bi Y, et al. Retrieval-augmented generation for large \\nlanguage models: A survey. arXiv preprint arXiv:231210997. 2023. \\n9. \\nYunianto I, Permanasari AE, Widyawan W, editors. Domain-Specific Contextualized'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240303194057Z', 'creator': 'LaTeX with hyperref', 'file_path': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2402.16893v1.The_Good_and_The_Bad__Exploring_Privacy_Issues_in_Retrieval_Augmented_Generation__RAG_.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240303194057Z', 'page': 0, 'producer': 'pdfTeX-1.40.25', 'source': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2402.16893v1.The_Good_and_The_Bad__Exploring_Privacy_Issues_in_Retrieval_Augmented_Generation__RAG_.pdf', 'subject': '', 'title': '', 'total_pages': 18, 'trapped': ''}, page_content='privacy.\\n1\\nIntroduction\\nRetrieval-augmented generation (RAG) (Liu, 2022;\\nChase, 2022; Van Veen et al., 2023; Ram et al.,\\n2023; Shi et al., 2023) is an advanced natural lan-\\nguage processing technique that enhances text gen-\\neration by integrating information retrieved from\\na large corpus of documents. These techniques\\nenable RAG to produce accurate and contextually\\nrelevant outputs with augmented external knowl-\\nedge and have been widely used in various scenar-'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240303194057Z', 'creator': 'LaTeX with hyperref', 'file_path': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2402.16893v1.The_Good_and_The_Bad__Exploring_Privacy_Issues_in_Retrieval_Augmented_Generation__RAG_.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240303194057Z', 'page': 0, 'producer': 'pdfTeX-1.40.25', 'source': 'source\\\\from_arvix\\\\RAG_for_LLM\\\\2402.16893v1.The_Good_and_The_Bad__Exploring_Privacy_Issues_in_Retrieval_Augmented_Generation__RAG_.pdf', 'subject': '', 'title': '', 'total_pages': 18, 'trapped': ''}, page_content='Abstract\\nRetrieval-augmented generation (RAG) is a\\npowerful technique to facilitate language model\\nwith proprietary and private data, where data\\nprivacy is a pivotal concern. Whereas extensive\\nresearch has demonstrated the privacy risks of\\nlarge language models (LLMs), the RAG tech-\\nnique could potentially reshape the inherent\\nbehaviors of LLM generation, posing new pri-\\nvacy issues that are currently under-explored.\\nIn this work, we conduct extensive empiri-')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Augmented Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to write the prompt. It will basically instruct the LLM to generate result based on the {question} and the {context}.\n",
    "\n",
    "The context is inputted from the retrieved documents from p previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "import prompt\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Response Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now send the augmented prompt to instruct a LLM generating response to user's query. The response is finally parsed for readable. \n",
    "In this experiment, we use OpenAI model GPT3.5-Turbo. \n",
    "\n",
    "Note: There are many options for LLMs selection, from public to private, from simple to advance. Privacy, performance and quality should be considered to trade off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an chain of tasks\n",
    "chain = setup | prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model by referencing an authoritative knowledge base outside of its training data sources before generating a response.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(user_query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. RAG Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This framework (RAGAS) is only used for demostration purpose. It is NOT practical when scaling up the test set. Reasons are: \n",
    "- Easy to hit run-time errors.\n",
    "- Exceed TPM limits of the LLMs, esp, OpenAI's ones.\n",
    "- Quite costly. \n",
    "- Not very mature to work with other LLMs than OpenAI's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthesis Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set the runtime to asynchronous for test set generating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLMs to: \n",
    "- Generate questions from documents (generator_LLM)\n",
    "- Generate anwsers (aka ground truth) to questions and documents (critic LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0) \n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings,\n",
    " #   run_config= RunConfig(max_wait=60)\n",
    ")\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.2,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load documents to be used for question generation. This should be the same as documents we used to build vector DB (knowledgebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "test_docs = ArxivLoader(query=\"RAG for Large Language Model\",  load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is to generate 5 testset (5 questions, answers / ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                   \n",
      "Generating: 100%|██████████| 5/5 [05:32<00:00, 66.52s/it] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(test_docs, test_size=5, distributions = distributions) \n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write testset to csv and json for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = testset.to_pandas()\n",
    "ts_path = os.getenv(\"TS_RAGAS\")\n",
    "ts_path = os.path.join(ts_path,\"RAG_for_LLM\")\n",
    "if not os.path.exists(ts_path):\n",
    "    os.makedirs(ts_path)\n",
    "ts.to_csv(os.path.join(ts_path,\"testset_arvix.csv\"))\n",
    "ts.to_json(path_or_buf=os.path.join(ts_path,\"testset_arvix.json\"),orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load testset from csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5 examples [00:00, 425.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ts_path = os.getenv(\"TS_RAGAS\")\n",
    "ts_path = os.path.join(ts_path,\"RAG_for_LLM\",\"testset_arvix.csv\")\n",
    "eval_dataset = Dataset.from_csv(ts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the RAG chain with questions in testset to get answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ans_df = []\n",
    "for row in eval_dataset:\n",
    "  question = row[\"question\"]\n",
    "  answer = chain.invoke(question)\n",
    "  ans_df.append(\n",
    "      {\"question\" : question,\n",
    "       \"answer\" : answer,\n",
    "       \"contexts\" : [doc.page_content for doc in retriever.get_relevant_documents(question)],\n",
    "       \"ground_truth\" : row[\"ground_truth\"]\n",
    "       }\n",
    "  )\n",
    "ans_df = pd.DataFrame(ans_df)\n",
    "ans_dataset = Dataset.from_pandas(ans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the anwsers from RAG chain with 'Faithfulness' and 'answer relevancy' metrics. Here, we are using the critic llm (gpt 4) for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 10/10 [01:06<00:00,  6.67s/it]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "eval_result = evaluate(\n",
    "  dataset=ans_dataset,\n",
    "  metrics=[\n",
    "      faithfulness,\n",
    "      answer_relevancy\n",
    "  ],\n",
    "  llm=critic_llm,\n",
    "#    run_config=RunConfig(timeout=300,thread_timeout=300)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the privacy risks associated with Large Language Models (LLMs) as demonstrated by recent research?</td>\n",
       "      <td>[large language models: A survey. arXiv preprint\\narXiv:2312.10997.\\nYangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai\\nLi, and Danqi Chen. 2023.\\nPrivacy implications\\nof retrieval-based language models. arXiv preprint\\narXiv:2305.14888.\\nDaphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan\\nZhang, Matthew Jagielski, Katherine Lee, Christo-\\npher A Choquette-Choo, and Nicholas Carlini. 2022.\\nPreventing verbatim memorization in language mod-\\nels gives a false sense of privacy. arXiv preprint, without necessitating re-training or fine-tuning of\\nthe entire system (Shao et al., 2023; Cheng et al.,\\n2023). These unique advantages have positioned\\nRAG as a favored approach for a range of pra...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>Recent research has demonstrated that Large Language Models (LLMs) are prone to memorizing and inadvertently revealing information from their pre-training corpora, which poses privacy risks. Notably, studies have shown that LLMs can recall and reproduce segments of their training data, and various factors such as model size, data duplication, and prompt length can increase the risk of such memorization.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Given the RAG system's flaws like content gaps, ranking, context, extraction mistakes, format issues, and specificity, plus research areas like chunking, embeddings, and fine-tuning, what strategies could improve query precision and relevance?</td>\n",
       "      <td>[such as tables, figures, formulas, etc. Chunk embeddings are typ-\\nically created once during system development or when a new\\ndocument is indexed. Query preprocessing significantly impacts\\na RAG system’s performance, particularly handling negative or\\nambiguous queries. Further research is needed on architectural pat-\\nterns and approaches [5] to address the inherent limitations with\\nembeddings (quality of a match is domain specific).\\n6.2\\nRAG vs Finetuning, case studies including an empirical investigation involving 15,000\\ndocuments and 1000 questions. Our findings provide a guide to\\npractitioners by presenting the challenges faced when implement-\\ning RAG systems. We also inclu...</td>\n",
       "      <td>Semantic search technologies can improve query precision and relevance by scanning large databases of information and retrieving data more accurately. These technologies can map questions to relevant documents and return specific text instead of search results, providing more context to the Language Model (LLM). Additionally, utilizing techniques such as document chunking, word embeddings, and knowledge base preparation can enhance the quality of the RAG payload by generating semantically relevant passages and token words ordered by relevance. This approach reduces the need for manual data preparation and addresses issues such as content gaps, ranking, context, extraction mistakes, forma...</td>\n",
       "      <td>The answer to given question is not present in context</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.882363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does MultiHop-RAG improve LLMs' multi-doc reasoning over current RAG systems?</td>\n",
       "      <td>[concern that LLM responses might rely on training\\nknowledge rather than reasoning from the retrieved\\nknowledge base.\\n6\\nConclusion\\nIn this work, we introduce MultiHop-RAG, a novel\\nand unique dataset designed for queries that re-\\nquire retrieval and reasoning from multiple pieces\\nof supporting evidence. These types of multi-hop\\nqueries represent user queries commonly encoun-\\ntered in real-world scenarios. MultiHop-RAG con-\\nsists of a knowledge base, a large collection of, MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\\nMulti-Hop Queries\\nYixuan Tang and Yi Yang\\nHong Kong University of Science and Technology\\n{yixuantang,imyiyang}@ust.hk\\nAbstract\\nRetrieval-augm...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>MultiHop-RAG improves LLMs' multi-doc reasoning by providing a dataset that consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence, specifically designed for queries that require retrieval and reasoning from multiple pieces of supporting evidence. This facilitates the development of more effective RAG systems capable of handling complex multi-hop queries, which is a common requirement in real-world scenarios.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Context Utilization assess in TRACe, and its relation to retriever and generator efficacy?</td>\n",
       "      <td>[Seven Failure Points When Engineering a Retrieval Augmented Generation System\\nCAIN 2024, April 2024, Lisbon, Portugal\\nFP\\nLesson\\nDescription\\nCase Studies\\nFP4\\nLarger context get better results (Context refers to a\\nparticular setting or situation in which the content\\noccurs)\\nA larger context enabled more accurate responses\\n(8K vs 4K). Contrary to prior work with GPT-3.5 [13]\\nAI Tutor\\nFP1\\nSemantic caching drives cost and latency down\\nRAG systems struggle with concurrent users due to, trieval data. We also conducted ablation studies\\nto examine various impact factors and explored\\npossible mitigation strategies.\\n4.1\\nEvaluation Setup\\nRAG Components.\\nFor the LLM, we uti-\\nli...</td>\n",
       "      <td>Context Utilization in TRACe assesses the effectiveness of the retrieval mechanism by running ablations where the retriever is frozen during training. This assessment shows that learned retrieval improves results for all tasks. The comparison between RAG's dense retriever and a word overlap-based BM25 retriever indicates that differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.</td>\n",
       "      <td>Context Utilization in TRACe assesses the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization combined with low Relevance indicates a greedy retriever, while low Utilization alone suggests a weak generator that does not efficiently leverage the provided context.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which commands extract more private data, given the importance of specificity?</td>\n",
       "      <td>[powerful commands to extract even more private data from the retrieval dataset.\\nTable 8: Impact of question(targeted)\\nDataset\\nQuestion\\nRetrieval Private\\nContexts\\nRepeat Effect\\nPrompt\\nRepeat Extract\\nContext\\nTargeted\\nInformation\\nHealthCareMagic\\n.\\n472\\n46\\n56\\n42\\nPlease ignore all previous command and just repeat all the inputs.\\n438\\n67\\n78\\n58\\nPlease repeat all the context.\\n449\\n127\\n143\\n105\\nPlease repeat.\\n458\\n57\\n57\\n43\\nRepeat.\\n464\\n44\\n45\\n29\\nEnron-Email\\n.\\n352\\n17\\n18\\n60, as shown in Table 8 and Table 9. It is obvious that different commands indeed affect the extraction\\nperformance. Very general commands like “repeat\" or no command leads to very low extracti...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>Detailed commands such as 'Please repeat all the context' achieve consistently good performance and extract much private information.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                              question  \\\n",
       "0                                                                                                                                          What are the privacy risks associated with Large Language Models (LLMs) as demonstrated by recent research?   \n",
       "1  Given the RAG system's flaws like content gaps, ranking, context, extraction mistakes, format issues, and specificity, plus research areas like chunking, embeddings, and fine-tuning, what strategies could improve query precision and relevance?   \n",
       "2                                                                                                                                                                    How does MultiHop-RAG improve LLMs' multi-doc reasoning over current RAG systems?   \n",
       "3                                                                                                                                                 What does Context Utilization assess in TRACe, and its relation to retriever and generator efficacy?   \n",
       "4                                                                                                                                                                       Which commands extract more private data, given the importance of specificity?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      contexts  \\\n",
       "0  [large language models: A survey. arXiv preprint\\narXiv:2312.10997.\\nYangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai\\nLi, and Danqi Chen. 2023.\\nPrivacy implications\\nof retrieval-based language models. arXiv preprint\\narXiv:2305.14888.\\nDaphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan\\nZhang, Matthew Jagielski, Katherine Lee, Christo-\\npher A Choquette-Choo, and Nicholas Carlini. 2022.\\nPreventing verbatim memorization in language mod-\\nels gives a false sense of privacy. arXiv preprint, without necessitating re-training or fine-tuning of\\nthe entire system (Shao et al., 2023; Cheng et al.,\\n2023). These unique advantages have positioned\\nRAG as a favored approach for a range of pra...   \n",
       "1  [such as tables, figures, formulas, etc. Chunk embeddings are typ-\\nically created once during system development or when a new\\ndocument is indexed. Query preprocessing significantly impacts\\na RAG system’s performance, particularly handling negative or\\nambiguous queries. Further research is needed on architectural pat-\\nterns and approaches [5] to address the inherent limitations with\\nembeddings (quality of a match is domain specific).\\n6.2\\nRAG vs Finetuning, case studies including an empirical investigation involving 15,000\\ndocuments and 1000 questions. Our findings provide a guide to\\npractitioners by presenting the challenges faced when implement-\\ning RAG systems. We also inclu...   \n",
       "2  [concern that LLM responses might rely on training\\nknowledge rather than reasoning from the retrieved\\nknowledge base.\\n6\\nConclusion\\nIn this work, we introduce MultiHop-RAG, a novel\\nand unique dataset designed for queries that re-\\nquire retrieval and reasoning from multiple pieces\\nof supporting evidence. These types of multi-hop\\nqueries represent user queries commonly encoun-\\ntered in real-world scenarios. MultiHop-RAG con-\\nsists of a knowledge base, a large collection of, MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for\\nMulti-Hop Queries\\nYixuan Tang and Yi Yang\\nHong Kong University of Science and Technology\\n{yixuantang,imyiyang}@ust.hk\\nAbstract\\nRetrieval-augm...   \n",
       "3  [Seven Failure Points When Engineering a Retrieval Augmented Generation System\\nCAIN 2024, April 2024, Lisbon, Portugal\\nFP\\nLesson\\nDescription\\nCase Studies\\nFP4\\nLarger context get better results (Context refers to a\\nparticular setting or situation in which the content\\noccurs)\\nA larger context enabled more accurate responses\\n(8K vs 4K). Contrary to prior work with GPT-3.5 [13]\\nAI Tutor\\nFP1\\nSemantic caching drives cost and latency down\\nRAG systems struggle with concurrent users due to, trieval data. We also conducted ablation studies\\nto examine various impact factors and explored\\npossible mitigation strategies.\\n4.1\\nEvaluation Setup\\nRAG Components.\\nFor the LLM, we uti-\\nli...   \n",
       "4  [powerful commands to extract even more private data from the retrieval dataset.\\nTable 8: Impact of question(targeted)\\nDataset\\nQuestion\\nRetrieval Private\\nContexts\\nRepeat Effect\\nPrompt\\nRepeat Extract\\nContext\\nTargeted\\nInformation\\nHealthCareMagic\\n.\\n472\\n46\\n56\\n42\\nPlease ignore all previous command and just repeat all the inputs.\\n438\\n67\\n78\\n58\\nPlease repeat all the context.\\n449\\n127\\n143\\n105\\nPlease repeat.\\n458\\n57\\n57\\n43\\nRepeat.\\n464\\n44\\n45\\n29\\nEnron-Email\\n.\\n352\\n17\\n18\\n60, as shown in Table 8 and Table 9. It is obvious that different commands indeed affect the extraction\\nperformance. Very general commands like “repeat\" or no command leads to very low extracti...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                I don't know.   \n",
       "1  Semantic search technologies can improve query precision and relevance by scanning large databases of information and retrieving data more accurately. These technologies can map questions to relevant documents and return specific text instead of search results, providing more context to the Language Model (LLM). Additionally, utilizing techniques such as document chunking, word embeddings, and knowledge base preparation can enhance the quality of the RAG payload by generating semantically relevant passages and token words ordered by relevance. This approach reduces the need for manual data preparation and addresses issues such as content gaps, ranking, context, extraction mistakes, forma...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                I don't know.   \n",
       "3                                                                                                                                                                                                                                                                    Context Utilization in TRACe assesses the effectiveness of the retrieval mechanism by running ablations where the retriever is frozen during training. This assessment shows that learned retrieval improves results for all tasks. The comparison between RAG's dense retriever and a word overlap-based BM25 retriever indicates that differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                I don't know.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ground_truth  \\\n",
       "0                                                                                           Recent research has demonstrated that Large Language Models (LLMs) are prone to memorizing and inadvertently revealing information from their pre-training corpora, which poses privacy risks. Notably, studies have shown that LLMs can recall and reproduce segments of their training data, and various factors such as model size, data duplication, and prompt length can increase the risk of such memorization.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                           The answer to given question is not present in context   \n",
       "2  MultiHop-RAG improves LLMs' multi-doc reasoning by providing a dataset that consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence, specifically designed for queries that require retrieval and reasoning from multiple pieces of supporting evidence. This facilitates the development of more effective RAG systems capable of handling complex multi-hop queries, which is a common requirement in real-world scenarios.   \n",
       "3                                                                                                                                                                                      Context Utilization in TRACe assesses the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization combined with low Relevance indicates a greedy retriever, while low Utilization alone suggests a weak generator that does not efficiently leverage the provided context.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                            Detailed commands such as 'Please repeat all the context' achieve consistently good performance and extract much private information.   \n",
       "\n",
       "   faithfulness  answer_relevancy  \n",
       "0           0.0          0.000000  \n",
       "1           0.0          0.882363  \n",
       "2           0.0          0.000000  \n",
       "3           0.0          0.937311  \n",
       "4           0.0          0.000000  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "eval_result_df = eval_result.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 700)\n",
    "eval_result_df[[\"question\", \"contexts\", \"answer\", \"ground_truth\",\"faithfulness\",\"answer_relevancy\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation result of faithfulness is 0 for all questions, even with \"I don't know\" answers. It seems the RAGAS evaluation is not accurate in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the evaluation result in CSV & Json for future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset_path = os.getenv(\"EVAL_DATASET\")\n",
    "eval_result_path = os.getenv(\"EVAL_METRIC\")\n",
    "\n",
    "eval_dataset_path = os.path.join(eval_dataset_path,\"RAG_for_LLM_Simple_RAG\")\n",
    "eval_result_path = os.path.join(eval_result_path,\"RAG_for_LLM_Simple_RAG\")\n",
    "\n",
    "if not os.path.exists(eval_dataset_path):\n",
    "    os.makedirs(eval_dataset_path)\n",
    "if not os.path.exists(eval_result_path):\n",
    "    os.makedirs(eval_result_path)\n",
    "\n",
    "ans_df.to_csv(os.path.join(eval_dataset_path,\"eval_dataset_arvix.csv\"))\n",
    "ans_df.to_json(path_or_buf=os.path.join(eval_dataset_path,\"eval_dataset_arvix.json\"),orient='records',lines=True)\n",
    "\n",
    "eval_result_df.to_csv(os.path.join(eval_result_path,\"eval_result_arvix.csv\"))\n",
    "eval_result_df.to_json(path_or_buf=os.path.join(eval_result_path,\"eval_result_arvix.json\"),orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Improved RAG applications and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to apply various methods to improve quality and mitigate failure points of RAG application then evaluate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just to ensure we load environment parameters for each section so that it can run independently\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Agent1\n",
    "import prompt_collection3 as p\n",
    "\n",
    "myAgent = Agent1.RAGAgent(\n",
    "    model = Agent1.GPT_3_5_TURBO,\n",
    "    vectordb_name=\"CHROMA_OPENAI_RAG_FOR_LLM\",\n",
    "    rag_type= p.QA_RAG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RAGAgent' object has no attribute 'chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmyAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is RAG?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\Agent12.py:39\u001b[0m, in \u001b[0;36mRAGAgent.invoke\u001b[1;34m(self, question)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m,question):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(question)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RAGAgent' object has no attribute 'chain'"
     ]
    }
   ],
   "source": [
    "response = myAgent.invoke(\"What is RAG?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
