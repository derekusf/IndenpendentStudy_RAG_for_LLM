{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install neccessary Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) ARXIV for searching and loading documents from ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) TQDM for progress indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4ALL for Local LLM and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet huggingface_hub\n",
    "!pip install --upgrade --quiet langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe to embedding flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"ReAct for Large Language Model\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning http://arxiv.org/abs/2407.07094v1\n",
      "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation http://arxiv.org/abs/2407.07093v1\n",
      "V-VIPE: Variational View Invariant Pose Embedding http://arxiv.org/abs/2407.07092v1\n",
      "General Relativistic effects and the NIR variability of Sgr A* II: A systematic approach to temporal asymmetry http://arxiv.org/abs/2407.07091v1\n",
      "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes http://arxiv.org/abs/2407.07090v1\n",
      "Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task Arithmetic http://arxiv.org/abs/2407.07089v1\n",
      "Safe and Reliable Training of Learning-Based Aerospace Controllers http://arxiv.org/abs/2407.07088v1\n",
      "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation http://arxiv.org/abs/2407.07087v1\n",
      "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models http://arxiv.org/abs/2407.07086v1\n",
      "On some conjectural determinants of Sun involving residues http://arxiv.org/abs/2407.07085v1\n"
     ]
    }
   ],
   "source": [
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning', 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation', 'V-VIPE: Variational View Invariant Pose Embedding', 'General Relativistic effects and the NIR variability of Sgr A* II: A systematic approach to temporal asymmetry', '3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes', 'Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task Arithmetic', 'Safe and Reliable Training of Learning-Based Aerospace Controllers', 'CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation', 'Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models', 'On some conjectural determinants of Sun involving residues']\n"
     ]
    }
   ],
   "source": [
    "print([r.title for r in all_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import ArxivLoader\n",
    "#base_docs = ArxivLoader(query=\"ReAct LLM\", load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARVIX_DOC = os.getenv(\"ARVIX_DOC\") \n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=ARVIX_DOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 1. text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "DOCUMENT = os.getenv(\"DOCUMENT\")\n",
    "txt_path = DOCUMENT+\"rag.txt\"\n",
    "txt_loader = TextLoader(txt_path)\n",
    "text_documents = txt_loader.load()\n",
    "#text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 2. PDF document\n",
    "\n",
    "We use PyMuPDFLoader in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_path = DOCUMENT+ \"*.pdf\"\n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_documents = []\n",
    "for file in os.listdir(os.getenv(\"ARVIX_DOC\")):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(os.getenv(\"ARVIX_DOC\"), file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pdf_documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 3. Batch Loading Directly from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "batch_docs = ArxivLoader(query=\"ReAct for Large Language Model\",  load_max_docs=10).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.xml import UnstructuredXMLLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Define a dictionary to map file extensions to their respective loaders\n",
    "loaders = {\n",
    "    '.pdf': PyMuPDFLoader,\n",
    "    '.xml': UnstructuredXMLLoader,\n",
    "    '.csv': CSVLoader,\n",
    "}\n",
    "\n",
    "# Define a function to create a DirectoryLoader for a specific file type\n",
    "def create_directory_loader(file_type, directory_path):\n",
    "    return DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=f\"**/*{file_type}\",\n",
    "        loader_cls=loaders[file_type],\n",
    "    )\n",
    "\n",
    "# Create DirectoryLoader instances for each file type\n",
    "pdf_loader = create_directory_loader('.pdf', os.getenv(\"ARVIX_DOC\"))\n",
    "\n",
    "# Load the files\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(text_documents)\n",
    "#documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using openAI embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Using gpt4all embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 45.9M/45.9M [00:06<00:00, 7.66MiB/s]\n",
      "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 855MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Memory vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "#vectorstore = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the vectordb with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE_GPT4ALL\")\n",
    "\n",
    "#Create vector database with local embedding method gpt4all. \n",
    "#Note different embedding methods will result different vector dimensions and cannot be stored together\n",
    "#The same embedding method to be used in retrieval pipeline\n",
    "vectordb = Chroma.from_documents(documents=chunks,  embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 - Retrieving & Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent here\n",
    "class RAGAgent: \n",
    "    \n",
    "    def __init__(self,\n",
    "                 llm, embeddings, vectordb) -> None:\n",
    "        from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "        from langchain.prompts import ChatPromptTemplate\n",
    "        from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "        self.llm = llm\n",
    "        self.embeddings = embeddings\n",
    "        self.vectordb = vectordb\n",
    "        self.retriever = vectordb.as_retriever()\n",
    "        \n",
    "        setup = RunnableParallel(context=self.retriever, question=RunnablePassthrough())\n",
    "\n",
    "        template = \"\"\"\n",
    "        Answer the question based on the context below. \n",
    "        If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        \n",
    "        parser = StrOutputParser()\n",
    "\n",
    "        self.chain = setup | prompt | llm | parser\n",
    "        \n",
    "    def invoke(self,question):\n",
    "        self.chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is retrieval augmented generation\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is. Here the on memory vectorstore is used. \n",
    "There is opportunity to improve efficiency of search when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vectorstore.as_retriever()\n",
    "\n",
    "#Load vectordb from persisted store\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE_GPT4ALL\")\n",
    "newvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = newvectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': 'D:20240710005619Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07087v1.CopyBench__Measuring_Literal_and_Non_Literal_Reproduction_of_Copyright_Protected_Text_in_Language_Model_Generation.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005619Z', 'page': 5, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07087v1.CopyBench__Measuring_Literal_and_Non_Literal_Reproduction_of_Copyright_Protected_Text_in_Language_Model_Generation.pdf', 'subject': '', 'title': '', 'total_pages': 23, 'trapped': ''}, page_content='the prompt. In the fact recall task, the prompt in-\\nstructs the model to generate a short answer. To\\nfacilitate a fair comparison between base models\\nand instruction-tuned models, we incorporate an\\ninstruction and in-context learning demonstrations\\ninto our prompts. Refer to Section A.2 for more\\ndetails.\\n3.4\\nHuman Analysis of Automatic Event\\nCopying Evaluation\\nTo verify the alignment between automatic event\\ncopying evaluation and human judgment, we con-'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005618Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005618Z', 'page': 24, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}, page_content='E\\nAblation Details\\nThe Hypothesis Evaluation + Hypothesis Refinement model had a different evaluation procedure than\\nthe other models. Rather than computing values based on predicting the opponent’s inventory, here\\nwe use extrinsic reward and counterfactual reward. If a hypothesis is used online for goal selection,\\nthen the rewards received in the next interaction can be directly used for evaluating it. For the other'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005618Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005618Z', 'page': 21, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}, page_content='the previous step to lists of each entity type in a tuple with the step it was observed. For example:\\n‘yellow_box’: [((13, 3), ‘Step: 1087’), ((13, 4), ‘Step: 1087’), ((7, 3), ‘Step: 1091’)]. The LLM is\\nprompted that its memory can be outdated and therefore should take the step it was last observed into\\naccount.\\nThe second data structure in the memory system contains a list of the agent’s inventories and rewards'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005623Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07089v1.Fine_Tuning_Linear_Layers_Only_Is_a_Simple_yet_Effective_Way_for_Task_Arithmetic.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005623Z', 'page': 8, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07089v1.Fine_Tuning_Linear_Layers_Only_Is_a_Simple_yet_Effective_Way_for_Task_Arithmetic.pdf', 'subject': '', 'title': '', 'total_pages': 16, 'trapped': ''}, page_content='[40, 15, 41, 2, 1], which help avoid catastrophic forgetting [13, 14, 42, 43] and provide better starting\\npoints for subsequent fine-tuning [44, 45]. These benefits also extend to models trained from scratch,\\nprovided they are properly aligned before merging [46, 47]. This technique has been observed\\nto enhance downstream performance, highlighting the potential of weight interpolation and task\\narithmetic.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Augmented Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Response Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using on-cloud OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Using Local LLM GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\"C:\\\\Users\\\\derek\\\\Meta-Llama-3-8B-Instruct.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "model = GPT4All(model=local_path, verbose=False)\n",
    "parser = StrOutputParser()\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "#model = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(user_query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthesis Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.embeddings.base import BaseRagasEmbeddings, LangchainEmbeddingsWrapper\n",
    "from ragas.llms import BaseRagasLLM, LangchainLLMWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "def chat_factory() -> BaseChatModel:\n",
    "\n",
    "    llm = HuggingFaceHub(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        task=\"text-generation\",\n",
    "        model_kwargs={\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"top_k\": 30,\n",
    "            \"temperature\": 0.1,\n",
    "            \"repetition_penalty\": 1.03,\n",
    "        },\n",
    "        huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    )\n",
    "    chat = ChatHuggingFace(llm=llm)\n",
    "    return chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\derek\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Add custom llms \n",
    "generator_llm = chat_factory()\n",
    "critic_llm = chat_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# documents = load your documents\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0) \n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings,\n",
    " #   run_config= RunConfig(max_wait=60)\n",
    ")\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.2,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_documents = []\n",
    "for file in os.listdir(os.getenv(\"ARVIX_DOC\")):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(os.getenv(\"ARVIX_DOC\"), file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pdf_documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-109:                                  \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 304, in hf_raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\requests\\models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 631, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 109, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 691, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 651, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 836, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_community\\chat_models\\huggingface.py\", line 155, in _agenerate\n",
      "    llm_result = await self.llm._agenerate(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1341, in _agenerate\n",
      "    await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1300, in _acall\n",
      "    return await run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 557, in run_in_executor\n",
      "    return await asyncio.get_running_loop().run_in_executor(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 287, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 548, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py\", line 139, in _call\n",
      "    response = self.client.post(\n",
      "               ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py\", line 273, in post\n",
      "    hf_raise_for_status(response)\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py\", line 371, in hf_raise_for_status\n",
      "    raise HfHubHTTPError(str(e), response=response) from e\n",
      "huggingface_hub.utils._errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: qq7rt42C-uW9GiMIO5bZs)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(pdf_documents, test_size=10, distributions = distributions) \n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Test Dataset by Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "#format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\derek\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint \n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "repo_id = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "\n",
    "customer_llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "question_generation_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\nRITUPARNA CHALIHA AND GAUTAM KALITA\\nusing quadratic Gauss sums. Following these, Vsemirnov [14, 15] used a sophisticated matrix decompo-\\nsition to conﬁrm a challenging conjecture of Chapman [4] on the determinant\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x12j −i\\np\\n\\x13\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n1≤i,j≤p+1\\n2\\n.\\nIn [11], Sun concentrated on determinants of the form\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x12f(i, j)\\np\\n\\x13\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n1≤i,j≤p−1\\n2\\n,\\nwhere f(x, y) is a quadratic form, and investigated their quadratic residue properties. In particular, for\\np ∤d, Sun [11] studied the determinant\\nS(d, p) =\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n\\x12i2 + dj2\\np\\n\\x13\\x0c\\n\\x0c\\n\\x0c\\n\\x0c\\n1≤i,j≤p−1\\n2\\n,\\nand proved that\\n\\x12S(d, p)\\np\\n\\x13\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n\\x10\\n−1\\np\\n\\x11\\n,\\nif\\n\\x10\\nd\\np\\n\\x11\\n= 1;\\n0,\\nif\\n\\x10\\nd\\np\\n\\x11\\n= −1.\\nIn addition, Sun [11] also posed a number of conjectures related to the determinant S(d, p). In recent\\nyears, some of these conjectures and their generalizations have been proved by many mathematicians,\\nsuch as Krachun et. al. [7], Grinberg et. al. [5], Wu [17], and Wu-Wang [16].\\nFor n, k ∈N with 2 ≤k ≤p−1\\n2 , throughout the paper, let p be an odd prime such that p ≡1 (mod k)\\nand\\nSn,k(d, p) = |(αi + dαj)n|1≤i,j≤p−1\\nk ,\\nwhere αi are distinct k-th residues modulo p. Note that S p−1\\n2\\n,2(d, p) ≡S(d, p) (mod p). From [6, Theorem\\n1.8], it is known that for p > 3,\\n S p−3\\n2\\n,2 (d, p)\\np\\n!\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n\\x10\\nd\\np\\n\\x11 p−1\\n4 ,\\nif p ≡1 (mod 4);\\n\\x10\\nd\\np\\n\\x11 p−3\\n4 (−1)|{0<k< p\\n2 :( k\\np)=−1}| ,\\nif p ≡3 (mod 4).\\nMoreover, for n < p−3\\n2 , we have from [8, Lemma 9] that\\nSn,2(d, p) = 0.\\nIn [12], Sun considered the determinant Sn,2(d, p) for n > p−1\\n2 , and proved a number of results related\\nto them under the condition\\n\\x10\\nd\\np\\n\\x11\\n= −1. Following these, Wu, She and Wang [18] proved a conjecture of\\nSun [11, Conjecture 4.5] related the Legendre symbol\\n\\x12 S p+1\\n2\\n,2(d,p)\\np\\n\\x13\\n. Recently, Ren and Sun [10] studied\\nthe determinant Sn,2(d, p) for n > p−1\\n2\\nunder the condition\\n\\x10\\nd\\np\\n\\x11\\n= 1. In the following theorems, we\\ngeneralize some of the results of Ren and Sun [10] to the determinant Sn,k(d, p).\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each context, create a question that is specific to the context. Avoid creating generic or general questions.\n",
    "\n",
    "question: a question about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "setup = RunnableParallel(context=RunnablePassthrough())\n",
    "\n",
    "question_generation_chain = setup | prompt_template | question_generation_llm | question_output_parser\n",
    "\n",
    "response = question_generation_chain.invoke(pdf_documents[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [04:37<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "question_ans_context = []\n",
    "\n",
    "for text in tqdm(pdf_documents):\n",
    "  try:\n",
    "    response = question_generation_chain.invoke(text.page_content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  response[\"context\"] = text.page_content\n",
    "  question_ans_context.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "answer_generation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "#setup = RunnableParallel(question = RunnablePassthrough(), context=RunnablePassthrough())\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "answer_generation_chain = (\n",
    "    {\"question\": itemgetter(\"question\"), \"context\": itemgetter(\"context\") }\n",
    "    | prompt_template \n",
    "    | answer_generation_llm \n",
    "    | answer_output_parser\n",
    ")\n",
    "response = answer_generation_chain.invoke({\"question\":question_ans_context[0][\"question\"],\"context\":question_ans_context[0][\"context\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [18:04<00:00,  6.57s/it]\n"
     ]
    }
   ],
   "source": [
    "for record in tqdm(question_ans_context):\n",
    "  try:\n",
    "    response = answer_generation_chain.invoke({\"question\":record[\"question\"],\"context\":record[\"context\"]})\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  record[\"answer\"] = response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the value of Sn,k(d, p)/p according to Lemma 2.6 in the given context?',\n",
       " 'context': 'ON SOME CONJECTURAL DETERMINANTS OF SUN INVOLVING RESIDUES\\n11\\nProof of Theorem 1.3. From Lemma 2.6, we have\\n\\x12Sn,k(d, p)\\np\\n\\x13\\n=\\n\\x12an,k(d, p)\\np\\n\\x132 \\x12bn,k(d, p)\\np\\n\\x13\\n.\\nSince n is odd and p ≡1 (mod 2k), Lemma 2.6 provides\\n\\x12bn,k(d, p)\\np\\n\\x13\\n=\\n\\x12d\\np\\n\\x13 p−1\\n2k\\n\\x12χk(d)\\np\\n\\x13 n−1\\n2\\n\\x12−1\\np\\n\\x13 p−1\\n2k\\n=\\n\\x12−d\\np\\n\\x13 p−1\\n2k\\n,\\nand hence\\n\\x12Sn,k(d, p)\\np\\n\\x13\\n=\\n\\x12an,k(d, p)\\np\\n\\x132 \\x12−d\\np\\n\\x13 p−1\\n2k\\n.\\n(9)\\n(a) Let k be even. Since\\nχk(d) ≡d\\np−1\\nk\\n≡1 (mod p),\\nwe have\\nd\\np−1\\n2\\n= (d\\np−1\\nk )\\nk\\n2 ≡1 (mod p).\\nAs a result,\\n\\x12\\n−d\\np\\n\\x13 p−1\\n2k\\n= 1,\\nand hence (9) yields\\n\\x12Sn,k(d, p)\\np\\n\\x13\\n=\\n\\x12an,k(d, p)\\np\\n\\x132\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n1,\\nif an,k(d, p) ̸= 0;\\n0,\\nif an,k(d, p) = 0.\\nThus we obtain the desired result.\\n(b) If k is odd, then it is easy to see that\\n\\x12\\n−d\\np\\n\\x13 p−1\\n2k\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\n1,\\nif p ≡1 (mod 4k)\\n−\\n\\x10\\nd\\np\\n\\x11\\n,\\nif p ≡2k + 1 (mod 4k).\\nUsing this in (9) , and then noting that\\n\\x12Sn,k(d, p)\\np\\n\\x13\\n= 0\\nif and only if\\n\\x12an,k(d, p)\\np\\n\\x13\\n= 0,\\nwe complete the proof of the theorem.\\n□\\n',\n",
       " 'answer': 'The value of Sn,k(d, p)/p according to Lemma 2.6 in the given context is given by the expression (an,k(d, p)/p)^2 * (-d/p)^(p-1)/(2k). For k even, this simplifies to (an,k(d, p)/p)^2, which is 1 if an,k(d, p) is not equal to 0, and 0 if an,k(d, p) is equal to 0. For k odd, the value depends on the congruence of p modulo 4k; it is 1 if p is congruent to 1 modulo 4k, and -d/p if p is congruent to 2k + 1 modulo 4k.'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_ans_context[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "question_ans_context = pd.DataFrame(question_ans_context)\n",
    "question_ans_context = question_ans_context.rename(columns={\"answer\" : \"ground_truth\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_ans_context.to_csv(\"eval_dataset_arvix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "eval_dataset = Dataset.from_csv(\"eval_dataset_arvix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ragas_dataset(rag_pipeline, retriever, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    question = row[\"question\"]\n",
    "    answer = rag_pipeline.invoke(question)\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : question,\n",
    "         \"answer\" : answer,\n",
    "         \"contexts\" : [doc.page_content for doc in retriever.get_relevant_documents(question)],\n",
    "         \"ground_truth\" : row[\"ground_truth\"]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset,generator_llm):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "    ],\n",
    "    llm=generator_llm,\n",
    "    run_config=RunConfig(timeout=300,thread_timeout=300)\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE_GPT4ALL\")\n",
    "newvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "myAgent = RAGAgent(model,embeddings,newvectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/165 [00:00<?, ?it/s]c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "100%|██████████| 165/165 [02:45<00:00,  1.01s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What determinant does the paper study involving residues?</td>\n",
       "      <td>[with aij ∈R, we denote the determinant by |M| or |[aij]1≤i,j≤n|. Let p be an odd prime and χℓdenotes\\na multiplicative character of order ℓmodulo p. For example, χ2(·) =\\n\u0010\\n·\\np\\n\u0011\\nis the usual Legendre symbol.\\nIn this paper, we study some conjectural determinants involving residues. Determinants with Legendre\\nsymbol entries were ﬁrst considered by Lehmer [9], where he used a general method to determine the, [11] Z.-W. Sun, On some determinants with Legendre symbol entries, Finite Fields Appl. 56 (2019), 285–307.\\n[12] Z.-W. Sun, Some determinants involving quadratic residues modulo primes, arXiv:2401.14301 (2024).\\n[13] Z.-W. Sun, Quadratic residues and related permutations and ide...</td>\n",
       "      <td>The paper studies some conjectural determinants involving residues.</td>\n",
       "      <td>The paper studies the determinant Sm,k(d, p), which is defined for an odd prime p and integers d, k, m with gcd(p, d) = 1 and 2 ≤ k ≤ (p−1)/2. The determinant is constructed using distinct k-th power residues modulo p, denoted by αi, and is given by Sm,k(d, p) = |(αi − αj)m|1≤i,j≤(p−1)/k. The paper deduces residue properties for this determinant as a generalization of certain results of Sun and proves some of Sun's related conjectures. Specifically, it addresses the conjectures involving the determinants S(1+p−1)/2,2(−1, p)/p and S(3+p−1)/2,2(−1, p)/p, as well as the number of primes p such that p divides Sm+(p−1)/k,k(−1, p), confirming another conjecture related to Sm+(p−1)/2,2(−1, p).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What determinant did Sun study for p ∤d?</td>\n",
       "      <td>[generalize some of the results of Ren and Sun [10] to the determinant Sn,k(d, p)., ON SOME CONJECTURAL DETERMINANTS OF SUN INVOLVING RESIDUES\\n13\\nUsing this in (11), we obtain\\n\\n\\nq\\nS p−1\\nk\\n+1,k(−1, p)\\np\\n\\n=\\n \\n( p−1\\nk\\n+ 1)( p−1\\nk\\n+ 2)\\np\\n!  \u0000 p−1\\nk\\n\u0001\\n!!\\np\\n!\\n\\n\\n\\n\\n\\nY\\n1≤i&lt;j≤p−1\\nk\\n(αi −αj)\\np\\n\\n\\n\\n\\n.\\nHence the result follows.\\nCase II: Let p ≡2k + 1 (mod 4k). In this case, we have from (12) that\\n\\n\\n\\n\\n\\n\\n\\n\\np−1\\n2k −2\\nY\\nl=0\\n\u0012 p−1\\nk\\n+ 1\\n2 + l\\n\u0013\\np\\n\\n\\n\\n\\n\\n\\n\\n\\n=\\n \\n( p−1\\nk\\n−1)!!\\np\\n!\\n.\\nUsing this in (11), we obtain\\n\\n\\nq\\nS p−1\\nk\\n+1,k(−1, p)\\np\\n\\n=\\n \\n( p−1\\nk\\n+ 2)\\np\\n!  \\n( p−1, arXiv:2407.07085v1 ...</td>\n",
       "      <td>The determinant that Sun studied for p ∤ d is Sn,2(d, p).</td>\n",
       "      <td>Sun studied the determinant S(d, p) = |(i^2 + dj^2)_p| for 1≤i,j≤(p−1)/2, where p is an odd prime not dividing d.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Conjecture 1.5 in the context of Sun's conjectures?</td>\n",
       "      <td>[and p ≡1 (mod 4). In addition, Sun [12] also posed a number of conjectures related to the determinant\\nSm+ p−1\\n2\\n,2(−1, p).\\nConjecture 1.5. [12, Conjecture 6.3] For any prime p ≡1 (mod 4), we have\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n= (−1)|{0&lt;k&lt; p\\n4 :( k\\np)=−1}| \u0010p\\n3\\n\u0011\\n.\\nConjecture 1.6. [12, Conjecture 6.4] For any prime p ≡1 (mod 4), we have\\n\\n\\nq\\nS3+ p−1\\n2\\n,2(−1, p)\\np\\n\\n= (−1)|{0&lt;k&lt; p\\n4 :( k\\np)=−1}|\\n \\np\\n4 + (−1)\\np−1\\n4\\n!\\n.\\nConjecture 1.7. [12, Conjecture 6.5] For any positive odd integer m, the set, some conjectures of Sun related to\\n\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n\\nand\\n\\n\\n\\nq\\nS3+ p−1\\n2\\n,2(−1, p)\\np\\n\\n\\n.\\nIn addition, we invest...</td>\n",
       "      <td>Conjecture 1.5 in the context of Sun's conjectures is: \\nFor any prime p ≡1 (mod 4), we have the equation involving determinants.</td>\n",
       "      <td>Conjecture 1.5 states that for any prime p congruent to 1 modulo 4, the Legendre symbol of S1+(p-1)/2,2(-1, p) over p is equal to (-1) raised to the power of the cardinality of the set of all k less than p/4 for which the Legendre symbol (k/p) is -1, times the Legendre symbol of 3 over p.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the definition of the e-factorial, denoted by a!(e)?</td>\n",
       "      <td>[)\\np\\n\u0011\\n,\\notherwise.\\nFor k = 2, Theorem 1.9 yields\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n=\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\u0010\\n3\\np\\n\u0011 \u0010 p−1\\n2\\n!!\\np\\n\u0011 \u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p ≡1 (mod 8);\\n\u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p = 5;\\n\u0010\\n6\\np\\n\u0011 \u0010 p−3\\n2\\n!!\\np\\n\u0011 \u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p ≡5 (mod 8) and p ̸= 5.\\n(1)\\nFrom Lemma 2.3, we note that\\n\u00122\\np\\n\u0013\\n=\\n p−1\\n2 !\\np\\n!\\n=\\n p−1\\n2 !! · p−3\\n2 !!\\np\\n!\\n,\\nand hence\\n p−1\\n2 !!\\np\\n!\\n=\\n p−3\\n2 !!\\np\\n! \u00122\\np\\n\u0013\\n.\\n(2)\\nMoreover, Lemma 2.5 and Lemma 2.3 together provide\\n \\nT\\n\u0000 p−1\\n2\\n\u0001\\np\\n!\\n=\\n\u00122\\np\\n\u0013\\n.\\n(3), p\\n!\\n=\\n\u00122\\np\\n\u0013\\n.\\n(3)\\nPlugging (2) and (3) in (1), and then using Lemma 2.4 and Theorem 2...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>The e-factorial of a number 'a', denoted by a!(e), is defined as the product of the sequence of numbers from 'a' down to 1, where each term is 'e' less than the previous term, until the term is less than or equal to 'e'. If 'a' is greater than 'e', then a!(e) = a * (a - e)!(e). If 'a' is between 1 and 'e' inclusive, then a!(e) = a. This definition is a generalization of the factorial function, with the standard factorial being the special case where e = 1.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the value of q when p ≡1 (mod 4k) in Theorem 1.11?</td>\n",
       "      <td>[k\\n+ 1, p−1\\nk\\n+ 2, · · · , 2(p−1)\\nk\\n−1} is odd and d ∈Z with χk(d) = 1.\\n(a) If k is even, then\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n̸= −1.\\n(b) If k is odd, then\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n̸=\\n\\n\\n\\n\\n\\n−1,\\nif p ≡1 (mod 4k);\\n\u0010\\nd\\np\\n\u0011\\n,\\nif p ≡2k + 1 (mod 4k).\\nRemark 1.4. For k = 2, Theorem 1.3 readily provides [10, Theorem 1.2].\\nIn [12], Sun showed that\\nSm+ p−1\\n2\\n,2(−1, p) ≡0 (mod p)\\nwhen m is even and p ≡3 (mod 4), and Sm+ p−1\\n2\\n,2(−1, p) is an integer square modulo p when m is odd, −d\\np\\n\u0013 p−1\\n2k\\n= 1,\\nand hence (9) yields\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n=\\n\u0012an,k(d, p)\\np\\n\u00132\\n=\\n\\n\\n\\n\\n\\n1,\\nif an,k(d, p) ̸= 0;\\n0,\\nif an,k(d, p) = 0.\\nThus we obtain the desired result.\\n(b) If k is odd,...</td>\n",
       "      <td>The value of q when p ≡1 (mod 4k) in Theorem 1.11 is \\uf8eb\\uf8edq.</td>\n",
       "      <td>The value of q in Theorem 1.11 when p ≡1 (mod 4k) is given by the expression q = (k(3k - 1)(4k - 1) / (6k^3 + (3k - 1)(2k - 1)(k - 1))) * ((p-1)/k - 1)!! / T((p-1)/k).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>What are the limitations of traditional general models and domain-specific models that AnyTaskTune addresses?</td>\n",
       "      <td>[compared the performance of AnyTaskTune against various models including closed-source large\\nlanguage models (LLMs), open-source LLMs, and domain-specific models. Critically, our experiments\\nmaintained a strict separation between training and testing datasets to ensure unbiased evaluation and\\nreproducibility of results.\\n3.1\\nExperimental Setup\\nOur experiments were structured as follows:\\n• Model Base: We utilized Qwen2-7B [1] as the base model for AnyTaskTune training. This, outputs need to be diverse and comprehensive, businesses require standardized and controllable\\nsolutions. For most enterprises and organizations, their needs are highly specific and contextualized,\\nwhich cann...</td>\n",
       "      <td>AnyTaskTune addresses the limitations of traditional general models and domain-specific models by emphasizing precision and specificity, which are critical for real-world applications.</td>\n",
       "      <td>The limitations of traditional general models and domain-specific models that AnyTaskTune addresses include a lack of precision and specificity for real-world applications. Traditional general models are often not tailored to the particular characteristics and requirements of specific operational domains, which can lead to suboptimal performance when applied to specialized tasks. Domain-specific models, while more focused, may still not be fine-tuned with the necessary accuracy and efficiency required for complex and nuanced tasks. AnyTaskTune overcomes these limitations by using Explicit Data Sets to create clear and directive input-output pairs, allowing for precise model tailoring. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Who provided the main ideas, data construction, model training, and paper writing for DataTager?</td>\n",
       "      <td>[Authorship, Credit Attribution, and Acknowledgements\\nPlease cite this work as “DataTager(2024)”.\\nAuthorship and Contributions\\nPaper Writing\\n• Jiaxi Cui - Founder, Provided the main ideas, data construction, model training, paper writing.\\n• Wentao Zhang - Organized ideas and outline, and contributed to paper writing.\\nEngineering\\n• Xudong Tong - Software engineering at DataTager.\\n• Zhenwei Zhang - Tencent - Software engineering at DataTager.\\nOther Contributions, ∗Please cite this work as “DataTager(2024)\". Full authorship contribution statements appear at the end of\\nthe document. Correspondence regarding this technical report can be sent to report@datatager.com\\nPreprint. Under ...</td>\n",
       "      <td>Jiaxi Cui provided the main ideas, data construction, model training, and paper writing for DataTager.</td>\n",
       "      <td>Jiaxi Cui provided the main ideas, data construction, model training, and paper writing for DataTager.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>What is the title of the technical report published in 2023 by J. Bai et al.?</td>\n",
       "      <td>[∗Please cite this work as “DataTager(2024)\". Full authorship contribution statements appear at the end of\\nthe document. Correspondence regarding this technical report can be sent to report@datatager.com\\nPreprint. Under review.\\narXiv:2407.07094v1  [cs.CL]  9 Jul 2024, (11)\\nArticle number, page 4 of 13, References\\n[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji,\\nM. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan,\\nJ. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang,\\nY. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>Qwen technical report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>What is the name of the large language model focused on the medical domain?</td>\n",
       "      <td>[[23] W. Zhu and X. Wang.\\nChatmed:\\nA chinese medical large language model.\\nhttps://github.com/michael-wzhu/ChatMed, 2023.\\n11, [2] Z. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong, J. Peng, X. Huang, and Z. Wei. Disc-medllm:\\nBridging general large language models and real-world medical consultation, 2023.\\n[3] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu,\\nX. Dong, H. Duan, Q. Fan, Z. Fei, Y. Gao, J. Ge, C. Gu, Y. Gu, T. Gui, A. Guo, Q. Guo, C. He,\\nY. Hu, T. Huang, T. Jiang, P. Jiao, Z. Jin, Z. Lei, J. Li, J. Li, L. Li, S. Li, W. Li, Y. Li, H. Liu, J. Liu,, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M....</td>\n",
       "      <td>The name of the large language model focused on the medical domain is Chatmed.</td>\n",
       "      <td>The large language model focused on the medical domain mentioned in the context is IvyGPT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>What is the name of the Chinese medical large language model created by W. Zhu and X. Wang?</td>\n",
       "      <td>[[23] W. Zhu and X. Wang.\\nChatmed:\\nA chinese medical large language model.\\nhttps://github.com/michael-wzhu/ChatMed, 2023.\\n11, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu,\\nK. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao,\\nT. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024.\\n[12] R. Wang, Y. Duan, C. Lam, J. Chen, J. Xu, H. Chen, X. Liu, P. C.-I. Pang, and T. Tan. Ivygpt:\\nInteractive chinese pathway language model in medical domain.\\nIn CAAI International, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie,\\nJ. Dai, K. Fang, L. S...</td>\n",
       "      <td>Chatmed</td>\n",
       "      <td>The name of the Chinese medical large language model created by W. Zhu and X. Wang is Chatmed.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                          question  \\\n",
       "0                                                        What determinant does the paper study involving residues?   \n",
       "1                                                                         What determinant did Sun study for p ∤d?   \n",
       "2                                                      What is Conjecture 1.5 in the context of Sun's conjectures?   \n",
       "3                                                     What is the definition of the e-factorial, denoted by a!(e)?   \n",
       "4                                                       What is the value of q when p ≡1 (mod 4k) in Theorem 1.11?   \n",
       "..                                                                                                             ...   \n",
       "160  What are the limitations of traditional general models and domain-specific models that AnyTaskTune addresses?   \n",
       "161               Who provided the main ideas, data construction, model training, and paper writing for DataTager?   \n",
       "162                                  What is the title of the technical report published in 2023 by J. Bai et al.?   \n",
       "163                                    What is the name of the large language model focused on the medical domain?   \n",
       "164                    What is the name of the Chinese medical large language model created by W. Zhu and X. Wang?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        contexts  \\\n",
       "0    [with aij ∈R, we denote the determinant by |M| or |[aij]1≤i,j≤n|. Let p be an odd prime and χℓdenotes\\na multiplicative character of order ℓmodulo p. For example, χ2(·) =\\n\u0010\\n·\\np\\n\u0011\\nis the usual Legendre symbol.\\nIn this paper, we study some conjectural determinants involving residues. Determinants with Legendre\\nsymbol entries were ﬁrst considered by Lehmer [9], where he used a general method to determine the, [11] Z.-W. Sun, On some determinants with Legendre symbol entries, Finite Fields Appl. 56 (2019), 285–307.\\n[12] Z.-W. Sun, Some determinants involving quadratic residues modulo primes, arXiv:2401.14301 (2024).\\n[13] Z.-W. Sun, Quadratic residues and related permutations and ide...   \n",
       "1    [generalize some of the results of Ren and Sun [10] to the determinant Sn,k(d, p)., ON SOME CONJECTURAL DETERMINANTS OF SUN INVOLVING RESIDUES\\n13\\nUsing this in (11), we obtain\\n\\n\\nq\\nS p−1\\nk\\n+1,k(−1, p)\\np\\n\\n=\\n \\n( p−1\\nk\\n+ 1)( p−1\\nk\\n+ 2)\\np\\n!  \u0000 p−1\\nk\\n\u0001\\n!!\\np\\n!\\n\\n\\n\\n\\n\\nY\\n1≤i<j≤p−1\\nk\\n(αi −αj)\\np\\n\\n\\n\\n\\n.\\nHence the result follows.\\nCase II: Let p ≡2k + 1 (mod 4k). In this case, we have from (12) that\\n\\n\\n\\n\\n\\n\\n\\n\\np−1\\n2k −2\\nY\\nl=0\\n\u0012 p−1\\nk\\n+ 1\\n2 + l\\n\u0013\\np\\n\\n\\n\\n\\n\\n\\n\\n\\n=\\n \\n( p−1\\nk\\n−1)!!\\np\\n!\\n.\\nUsing this in (11), we obtain\\n\\n\\nq\\nS p−1\\nk\\n+1,k(−1, p)\\np\\n\\n=\\n \\n( p−1\\nk\\n+ 2)\\np\\n!  \\n( p−1, arXiv:2407.07085v1 ...   \n",
       "2    [and p ≡1 (mod 4). In addition, Sun [12] also posed a number of conjectures related to the determinant\\nSm+ p−1\\n2\\n,2(−1, p).\\nConjecture 1.5. [12, Conjecture 6.3] For any prime p ≡1 (mod 4), we have\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n= (−1)|{0<k< p\\n4 :( k\\np)=−1}| \u0010p\\n3\\n\u0011\\n.\\nConjecture 1.6. [12, Conjecture 6.4] For any prime p ≡1 (mod 4), we have\\n\\n\\nq\\nS3+ p−1\\n2\\n,2(−1, p)\\np\\n\\n= (−1)|{0<k< p\\n4 :( k\\np)=−1}|\\n \\np\\n4 + (−1)\\np−1\\n4\\n!\\n.\\nConjecture 1.7. [12, Conjecture 6.5] For any positive odd integer m, the set, some conjectures of Sun related to\\n\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n\\nand\\n\\n\\n\\nq\\nS3+ p−1\\n2\\n,2(−1, p)\\np\\n\\n\\n.\\nIn addition, we invest...   \n",
       "3    [)\\np\\n\u0011\\n,\\notherwise.\\nFor k = 2, Theorem 1.9 yields\\n\\n\\nq\\nS1+ p−1\\n2\\n,2(−1, p)\\np\\n\\n=\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\u0010\\n3\\np\\n\u0011 \u0010 p−1\\n2\\n!!\\np\\n\u0011 \u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p ≡1 (mod 8);\\n\u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p = 5;\\n\u0010\\n6\\np\\n\u0011 \u0010 p−3\\n2\\n!!\\np\\n\u0011 \u0012\\nT( p−1\\n2 )\\np\\n\u0013\\n,\\nif p ≡5 (mod 8) and p ̸= 5.\\n(1)\\nFrom Lemma 2.3, we note that\\n\u00122\\np\\n\u0013\\n=\\n p−1\\n2 !\\np\\n!\\n=\\n p−1\\n2 !! · p−3\\n2 !!\\np\\n!\\n,\\nand hence\\n p−1\\n2 !!\\np\\n!\\n=\\n p−3\\n2 !!\\np\\n! \u00122\\np\\n\u0013\\n.\\n(2)\\nMoreover, Lemma 2.5 and Lemma 2.3 together provide\\n \\nT\\n\u0000 p−1\\n2\\n\u0001\\np\\n!\\n=\\n\u00122\\np\\n\u0013\\n.\\n(3), p\\n!\\n=\\n\u00122\\np\\n\u0013\\n.\\n(3)\\nPlugging (2) and (3) in (1), and then using Lemma 2.4 and Theorem 2...   \n",
       "4    [k\\n+ 1, p−1\\nk\\n+ 2, · · · , 2(p−1)\\nk\\n−1} is odd and d ∈Z with χk(d) = 1.\\n(a) If k is even, then\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n̸= −1.\\n(b) If k is odd, then\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n̸=\\n\\n\\n\\n\\n\\n−1,\\nif p ≡1 (mod 4k);\\n\u0010\\nd\\np\\n\u0011\\n,\\nif p ≡2k + 1 (mod 4k).\\nRemark 1.4. For k = 2, Theorem 1.3 readily provides [10, Theorem 1.2].\\nIn [12], Sun showed that\\nSm+ p−1\\n2\\n,2(−1, p) ≡0 (mod p)\\nwhen m is even and p ≡3 (mod 4), and Sm+ p−1\\n2\\n,2(−1, p) is an integer square modulo p when m is odd, −d\\np\\n\u0013 p−1\\n2k\\n= 1,\\nand hence (9) yields\\n\u0012Sn,k(d, p)\\np\\n\u0013\\n=\\n\u0012an,k(d, p)\\np\\n\u00132\\n=\\n\\n\\n\\n\\n\\n1,\\nif an,k(d, p) ̸= 0;\\n0,\\nif an,k(d, p) = 0.\\nThus we obtain the desired result.\\n(b) If k is odd,...   \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...   \n",
       "160  [compared the performance of AnyTaskTune against various models including closed-source large\\nlanguage models (LLMs), open-source LLMs, and domain-specific models. Critically, our experiments\\nmaintained a strict separation between training and testing datasets to ensure unbiased evaluation and\\nreproducibility of results.\\n3.1\\nExperimental Setup\\nOur experiments were structured as follows:\\n• Model Base: We utilized Qwen2-7B [1] as the base model for AnyTaskTune training. This, outputs need to be diverse and comprehensive, businesses require standardized and controllable\\nsolutions. For most enterprises and organizations, their needs are highly specific and contextualized,\\nwhich cann...   \n",
       "161  [Authorship, Credit Attribution, and Acknowledgements\\nPlease cite this work as “DataTager(2024)”.\\nAuthorship and Contributions\\nPaper Writing\\n• Jiaxi Cui - Founder, Provided the main ideas, data construction, model training, paper writing.\\n• Wentao Zhang - Organized ideas and outline, and contributed to paper writing.\\nEngineering\\n• Xudong Tong - Software engineering at DataTager.\\n• Zhenwei Zhang - Tencent - Software engineering at DataTager.\\nOther Contributions, ∗Please cite this work as “DataTager(2024)\". Full authorship contribution statements appear at the end of\\nthe document. Correspondence regarding this technical report can be sent to report@datatager.com\\nPreprint. Under ...   \n",
       "162  [∗Please cite this work as “DataTager(2024)\". Full authorship contribution statements appear at the end of\\nthe document. Correspondence regarding this technical report can be sent to report@datatager.com\\nPreprint. Under review.\\narXiv:2407.07094v1  [cs.CL]  9 Jul 2024, (11)\\nArticle number, page 4 of 13, References\\n[1] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji,\\nM. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan,\\nJ. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang,\\nY. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou...   \n",
       "163  [[23] W. Zhu and X. Wang.\\nChatmed:\\nA chinese medical large language model.\\nhttps://github.com/michael-wzhu/ChatMed, 2023.\\n11, [2] Z. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong, J. Peng, X. Huang, and Z. Wei. Disc-medllm:\\nBridging general large language models and real-world medical consultation, 2023.\\n[3] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu,\\nX. Dong, H. Duan, Q. Fan, Z. Fei, Y. Gao, J. Ge, C. Gu, Y. Gu, T. Gui, A. Guo, Q. Guo, C. He,\\nY. Hu, T. Huang, T. Jiang, P. Jiao, Z. Jin, Z. Lei, J. Li, J. Li, L. Li, S. Li, W. Li, Y. Li, H. Liu, J. Liu,, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M....   \n",
       "164  [[23] W. Zhu and X. Wang.\\nChatmed:\\nA chinese medical large language model.\\nhttps://github.com/michael-wzhu/ChatMed, 2023.\\n11, M. Wiethoff, D. Willner, C. Winter, S. Wolrich, H. Wong, L. Workman, S. Wu, J. Wu, M. Wu,\\nK. Xiao, T. Xu, S. Yoo, K. Yu, Q. Yuan, W. Zaremba, R. Zellers, C. Zhang, M. Zhang, S. Zhao,\\nT. Zheng, J. Zhuang, W. Zhuk, and B. Zoph. Gpt-4 technical report, 2024.\\n[12] R. Wang, Y. Duan, C. Lam, J. Chen, J. Xu, H. Chen, X. Liu, P. C.-I. Pang, and T. Tan. Ivygpt:\\nInteractive chinese pathway language model in medical domain.\\nIn CAAI International, F. Deng, F. Wang, F. Liu, G. Ai, G. Dong, H. Zhao, H. Xu, H. Sun, H. Zhang, H. Liu, J. Ji, J. Xie,\\nJ. Dai, K. Fang, L. S...   \n",
       "\n",
       "                                                                                                                                                                                       answer  \\\n",
       "0                                                                                                                         The paper studies some conjectural determinants involving residues.   \n",
       "1                                                                                                                                   The determinant that Sun studied for p ∤ d is Sn,2(d, p).   \n",
       "2                                                           Conjecture 1.5 in the context of Sun's conjectures is: \\nFor any prime p ≡1 (mod 4), we have the equation involving determinants.   \n",
       "3                                                                                                                                                                               I don't know.   \n",
       "4                                                                                                                         The value of q when p ≡1 (mod 4k) in Theorem 1.11 is \\uf8eb\\uf8edq.   \n",
       "..                                                                                                                                                                                        ...   \n",
       "160  AnyTaskTune addresses the limitations of traditional general models and domain-specific models by emphasizing precision and specificity, which are critical for real-world applications.   \n",
       "161                                                                                    Jiaxi Cui provided the main ideas, data construction, model training, and paper writing for DataTager.   \n",
       "162                                                                                                                                                                             I don't know.   \n",
       "163                                                                                                            The name of the large language model focused on the medical domain is Chatmed.   \n",
       "164                                                                                                                                                                                   Chatmed   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ground_truth  \n",
       "0        The paper studies the determinant Sm,k(d, p), which is defined for an odd prime p and integers d, k, m with gcd(p, d) = 1 and 2 ≤ k ≤ (p−1)/2. The determinant is constructed using distinct k-th power residues modulo p, denoted by αi, and is given by Sm,k(d, p) = |(αi − αj)m|1≤i,j≤(p−1)/k. The paper deduces residue properties for this determinant as a generalization of certain results of Sun and proves some of Sun's related conjectures. Specifically, it addresses the conjectures involving the determinants S(1+p−1)/2,2(−1, p)/p and S(3+p−1)/2,2(−1, p)/p, as well as the number of primes p such that p divides Sm+(p−1)/k,k(−1, p), confirming another conjecture related to Sm+(p−1)/2,2(−1, p).  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Sun studied the determinant S(d, p) = |(i^2 + dj^2)_p| for 1≤i,j≤(p−1)/2, where p is an odd prime not dividing d.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                              Conjecture 1.5 states that for any prime p congruent to 1 modulo 4, the Legendre symbol of S1+(p-1)/2,2(-1, p) over p is equal to (-1) raised to the power of the cardinality of the set of all k less than p/4 for which the Legendre symbol (k/p) is -1, times the Legendre symbol of 3 over p.  \n",
       "3                                                                                                                                                                                                                                                   The e-factorial of a number 'a', denoted by a!(e), is defined as the product of the sequence of numbers from 'a' down to 1, where each term is 'e' less than the previous term, until the term is less than or equal to 'e'. If 'a' is greater than 'e', then a!(e) = a * (a - e)!(e). If 'a' is between 1 and 'e' inclusive, then a!(e) = a. This definition is a generalization of the factorial function, with the standard factorial being the special case where e = 1.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The value of q in Theorem 1.11 when p ≡1 (mod 4k) is given by the expression q = (k(3k - 1)(4k - 1) / (6k^3 + (3k - 1)(2k - 1)(k - 1))) * ((p-1)/k - 1)!! / T((p-1)/k).  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ...  \n",
       "160  The limitations of traditional general models and domain-specific models that AnyTaskTune addresses include a lack of precision and specificity for real-world applications. Traditional general models are often not tailored to the particular characteristics and requirements of specific operational domains, which can lead to suboptimal performance when applied to specialized tasks. Domain-specific models, while more focused, may still not be fine-tuned with the necessary accuracy and efficiency required for complex and nuanced tasks. AnyTaskTune overcomes these limitations by using Explicit Data Sets to create clear and directive input-output pairs, allowing for precise model tailoring. Th...  \n",
       "161                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Jiaxi Cui provided the main ideas, data construction, model training, and paper writing for DataTager.  \n",
       "162                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Qwen technical report  \n",
       "163                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The large language model focused on the medical domain mentioned in the context is IvyGPT.  \n",
       "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The name of the Chinese medical large language model created by W. Zhu and X. Wang is Chatmed.  \n",
       "\n",
       "[165 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "rag_eval_dataset = create_ragas_dataset(chain, retriever, eval_dataset)\n",
    "ans_result_pd = rag_eval_dataset.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 700)\n",
    "ans_result_pd[[\"question\", \"contexts\", \"answer\", \"ground_truth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 64.00ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "428597"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_eval_dataset.to_csv(\"rag1_eval_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset = Dataset.from_csv(\"rag1_eval_ds.csv\")\n",
    "for i in range(len(rag_eval_dataset)):\n",
    "    a = rag_eval_dataset[i][\"contexts\"]\n",
    "    rag_eval_dataset[i][\"contexts\"] = a.strip('\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 26/165 [01:17<10:05,  4.36s/it]No statements were generated from the answer.\n",
      "Evaluating:  19%|█▉        | 31/165 [01:37<09:15,  4.14s/it]No statements were generated from the answer.\n",
      "Evaluating:  24%|██▎       | 39/165 [02:14<07:15,  3.46s/it]\n",
      "Exception in thread Thread-13:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 631, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 109, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 134, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\metrics\\base.py\", line 127, in ascore\n",
      "    score = await asyncio.wait_for(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 520, in wait_for\n",
      "    return await fut\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\metrics\\_faithfulness.py\", line 266, in _ascore\n",
      "    nli_result = await self.llm.generate(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 691, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 651, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 836, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 674, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1599, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4-turbo-preview in organization org-IK8mpHva4T4MJMeZULAv14xw on tokens per min (TPM): Limit 30000, Used 29909, Requested 1358. Please try again in 2.534s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_pd \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ragas_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_eval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgenerator_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m result_pd \u001b[38;5;241m=\u001b[39m result_pd\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m700\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mevaluate_ragas_dataset\u001b[1;34m(ragas_dataset, generator_llm)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_ragas_dataset\u001b[39m(ragas_dataset,generator_llm):\n\u001b[1;32m---> 10\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mragas_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mthread_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\evaluation.py:255\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    253\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    258\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    259\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    260\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\evaluation.py:237\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    235\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "result_pd = evaluate_ragas_dataset(rag_eval_dataset,generator_llm)\n",
    "result_pd = result_pd.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 700)\n",
    "result_pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
