{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install neccessary Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) ARXIV for searching and loading documents from ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) TQDM for progress indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4ALL for Local LLM and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet huggingface_hub\n",
    "!pip install --upgrade --quiet langchain_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe to embedding flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"ReAct for Large Language Model\",\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning http://arxiv.org/abs/2407.07094v1\n",
      "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation http://arxiv.org/abs/2407.07093v1\n",
      "V-VIPE: Variational View Invariant Pose Embedding http://arxiv.org/abs/2407.07092v1\n",
      "General Relativistic effects and the NIR variability of Sgr A* II: A systematic approach to temporal asymmetry http://arxiv.org/abs/2407.07091v1\n",
      "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes http://arxiv.org/abs/2407.07090v1\n",
      "Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task Arithmetic http://arxiv.org/abs/2407.07089v1\n",
      "Safe and Reliable Training of Learning-Based Aerospace Controllers http://arxiv.org/abs/2407.07088v1\n",
      "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation http://arxiv.org/abs/2407.07087v1\n",
      "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models http://arxiv.org/abs/2407.07086v1\n",
      "On some conjectural determinants of Sun involving residues http://arxiv.org/abs/2407.07085v1\n"
     ]
    }
   ],
   "source": [
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning', 'FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation', 'V-VIPE: Variational View Invariant Pose Embedding', 'General Relativistic effects and the NIR variability of Sgr A* II: A systematic approach to temporal asymmetry', '3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes', 'Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task Arithmetic', 'Safe and Reliable Training of Learning-Based Aerospace Controllers', 'CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation', 'Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models', 'On some conjectural determinants of Sun involving residues']\n"
     ]
    }
   ],
   "source": [
    "print([r.title for r in all_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.document_loaders import ArxivLoader\n",
    "#base_docs = ArxivLoader(query=\"ReAct LLM\", load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARVIX_DOC = os.getenv(\"ARVIX_DOC\") \n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=ARVIX_DOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 1. text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "DOCUMENT = os.getenv(\"DOCUMENT\")\n",
    "txt_path = DOCUMENT+\"rag.txt\"\n",
    "txt_loader = TextLoader(txt_path)\n",
    "text_documents = txt_loader.load()\n",
    "#text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 2. PDF document\n",
    "\n",
    "We use PyMuPDFLoader in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_path = DOCUMENT+ \"*.pdf\"\n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_documents = []\n",
    "for file in os.listdir(os.getenv(\"ARVIX_DOC\")):\n",
    "    if file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(os.getenv(\"ARVIX_DOC\"), file)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pdf_documents.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 3. Batch Loading Directly from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "batch_docs = ArxivLoader(query=\"ReAct for Large Language Model\",  load_max_docs=10).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.document_loaders.xml import UnstructuredXMLLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Define a dictionary to map file extensions to their respective loaders\n",
    "loaders = {\n",
    "    '.pdf': PyMuPDFLoader,\n",
    "    '.xml': UnstructuredXMLLoader,\n",
    "    '.csv': CSVLoader,\n",
    "}\n",
    "\n",
    "# Define a function to create a DirectoryLoader for a specific file type\n",
    "def create_directory_loader(file_type, directory_path):\n",
    "    return DirectoryLoader(\n",
    "        path=directory_path,\n",
    "        glob=f\"**/*{file_type}\",\n",
    "        loader_cls=loaders[file_type],\n",
    "    )\n",
    "\n",
    "# Create DirectoryLoader instances for each file type\n",
    "pdf_loader = create_directory_loader('.pdf', os.getenv(\"ARVIX_DOC\"))\n",
    "\n",
    "# Load the files\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(text_documents)\n",
    "#documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using openAI embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Using gpt4all embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 45.9M/45.9M [00:06<00:00, 7.66MiB/s]\n",
      "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 855MiB/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Memory vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "#vectorstore = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist the vectordb with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE_GPT4ALL\")\n",
    "\n",
    "#Create vector database with local embedding method gpt4all. \n",
    "#Note different embedding methods will result different vector dimensions and cannot be stored together\n",
    "#The same embedding method to be used in retrieval pipeline\n",
    "vectordb = Chroma.from_documents(documents=chunks,  embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 2 - Retrieving & Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is retrieval augmented generation\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is. Here the on memory vectorstore is used. \n",
    "There is opportunity to improve efficiency of search when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {'allow_download': 'True'}\n",
    "embeddings = GPT4AllEmbeddings(\n",
    "    model_name=model_name,\n",
    "    gpt4all_kwargs=gpt4all_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vectorstore.as_retriever()\n",
    "\n",
    "#Load vectordb from persisted store\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE_GPT4ALL\")\n",
    "newvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = newvectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'creationDate': 'D:20240710005619Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07087v1.CopyBench__Measuring_Literal_and_Non_Literal_Reproduction_of_Copyright_Protected_Text_in_Language_Model_Generation.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005619Z', 'page': 5, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07087v1.CopyBench__Measuring_Literal_and_Non_Literal_Reproduction_of_Copyright_Protected_Text_in_Language_Model_Generation.pdf', 'subject': '', 'title': '', 'total_pages': 23, 'trapped': ''}, page_content='the prompt. In the fact recall task, the prompt in-\\nstructs the model to generate a short answer. To\\nfacilitate a fair comparison between base models\\nand instruction-tuned models, we incorporate an\\ninstruction and in-context learning demonstrations\\ninto our prompts. Refer to Section A.2 for more\\ndetails.\\n3.4\\nHuman Analysis of Automatic Event\\nCopying Evaluation\\nTo verify the alignment between automatic event\\ncopying evaluation and human judgment, we con-'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005618Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005618Z', 'page': 24, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}, page_content='E\\nAblation Details\\nThe Hypothesis Evaluation + Hypothesis Refinement model had a different evaluation procedure than\\nthe other models. Rather than computing values based on predicting the opponent’s inventory, here\\nwe use extrinsic reward and counterfactual reward. If a hypothesis is used online for goal selection,\\nthen the rewards received in the next interaction can be directly used for evaluating it. For the other'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005618Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005618Z', 'page': 21, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07086v1.Hypothetical_Minds__Scaffolding_Theory_of_Mind_for_Multi_Agent_Tasks_with_Large_Language_Models.pdf', 'subject': '', 'title': '', 'total_pages': 36, 'trapped': ''}, page_content='the previous step to lists of each entity type in a tuple with the step it was observed. For example:\\n‘yellow_box’: [((13, 3), ‘Step: 1087’), ((13, 4), ‘Step: 1087’), ((7, 3), ‘Step: 1091’)]. The LLM is\\nprompted that its memory can be outdated and therefore should take the step it was last observed into\\naccount.\\nThe second data structure in the memory system contains a list of the agent’s inventories and rewards'),\n",
       " Document(metadata={'author': '', 'creationDate': 'D:20240710005623Z', 'creator': 'LaTeX with hyperref', 'file_path': 'arvix_document\\\\2407.07089v1.Fine_Tuning_Linear_Layers_Only_Is_a_Simple_yet_Effective_Way_for_Task_Arithmetic.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20240710005623Z', 'page': 8, 'producer': 'pdfTeX-1.40.25', 'source': 'arvix_document\\\\2407.07089v1.Fine_Tuning_Linear_Layers_Only_Is_a_Simple_yet_Effective_Way_for_Task_Arithmetic.pdf', 'subject': '', 'title': '', 'total_pages': 16, 'trapped': ''}, page_content='[40, 15, 41, 2, 1], which help avoid catastrophic forgetting [13, 14, 42, 43] and provide better starting\\npoints for subsequent fine-tuning [44, 45]. These benefits also extend to models trained from scratch,\\nprovided they are properly aligned before merging [46, 47]. This technique has been observed\\nto enhance downstream performance, highlighting the potential of weight interpolation and task\\narithmetic.')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Augmented Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Response Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using on-cloud OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Using Local LLM GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = (\"C:\\\\Users\\\\derek\\\\Meta-Llama-3-8B-Instruct.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "model = GPT4All(model=local_path, verbose=False)\n",
    "parser = StrOutputParser()\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "#model = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: I don\\'t know. \\nPlease provide more context or clarify what you mean by \"retrieval augmented generation\". Is it a specific method or concept in natural language processing? If so, please provide more information about it. \\n\\nNote that the provided documents are PDFs and contain text related to various topics such as language models, multi-agent tasks, and task arithmetic. However, none of these texts explicitly discuss \"retrieval augmented generation\". Therefore, I am unable to answer your question based on this context alone. If you can provide more information or clarify what you mean by the term, I may be able to help further.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(user_query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthesis Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# generator with openai models\n",
    "# generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) \n",
    "# critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_huggingface import HuggingFaceEndpoint \n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\derek\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\derek\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "generator_llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0.5,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")\n",
    "critic_llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_length=128,\n",
    "    temperature=0,\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1806: RuntimeWarning: coroutine 'Executor.wrap_callable_with_index.<locals>.wrapped_callable_async' was never awaited\n",
      "  def _save_to_state_dict(self, destination, prefix, keep_vars):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Exception in thread Thread-23:                                      \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\json_load.py\", line 107, in _asafe_load\n",
      "    _json = self._load_all_jsons(text)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\json_load.py\", line 146, in _load_all_jsons\n",
      "    _json = json.loads(text[start:end])\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 631, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 109, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 50, in extract\n",
      "    keyphrases = await json_loader.safe_load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\json_load.py\", line 132, in safe_load\n",
      "    return await _asafe_load_with_retry(text=text, llm=llm, callbacks=callbacks)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\json_load.py\", line 112, in _asafe_load\n",
      "    results = await llm.agenerate_text(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 178, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 882, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 866, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1341, in _agenerate\n",
      "    await self._acall(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py\", line 289, in _acall\n",
      "    response = await self.async_client.post(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 291, in post\n",
      "    raise error\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\huggingface_hub\\inference\\_generated\\_async_client.py\", line 260, in post\n",
      "    response.raise_for_status()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\aiohttp\\client_reqrep.py\", line 1070, in raise_for_status\n",
      "    raise ClientResponseError(\n",
      "aiohttp.client_exceptions.ClientResponseError: 429, message='Too Many Requests', url=URL('https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.2,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.4\n",
    "}\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(chunks, test_size=10, distributions = distributions) \n",
    "except Exception as e:\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler Testset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_generator = TestsetGenerator.with_openai()\n",
    "\n",
    "testset = simple_generator.generate_with_langchain_docs(chunks, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtestset\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on our RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "for query in questions:\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_pd = result.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 700)\n",
    "result_pd[[\"question\", \"contexts\", \"answer\", \"ground_truth\",\"faithfulness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The RAG (Reinforced Augmented Generation) model uses an input sequence x to retrieve text documents z and use them as additional context when generating a target sequence y. It consists of two components: (i) a retriever pη(z|x) that returns distributions over text passages given a query x, and (ii) a generator pθ(yi|x,z,y1:i−1) parametrized by θ. The model can be used for tasks such as fact verification.\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load the data from the context into a DataFrame.\n",
      "\n",
      "Answer: I don't know how to load this specific data, but you could use Python's `pandas` library to create a DataFrame:\n",
      "\n",
      "```\n",
      "data = [\n",
      "    {\"page_content\": \"the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2\\...\", \n",
      "     \"metadata\": {...}},\n",
      "    ...\n",
      "]\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "```  ```\n",
      "Answer: I don't know how to load this specific data, but you could use Python's `pandas` library to create a DataFrame:\n",
      "\n",
      "```\n",
      "data = [\n",
      "    {\"page_content\": \"the non-parametric memory can be replaced to update the models’\n",
      "Answer: Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\n",
      "Human:  I don't know\n",
      "Answer the question based on the context below. If you can' t answer the question, reply \"I don' t know\". Context: [Document(page_content='How does Retrieval-Augmented Generation work?\\nWithout RAG, the LLM takes the user input and creates a response based on information it was trained on—or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The L\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        user_input = input(\"Enter a query: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            response = chain.invoke(user_input)  \n",
    "            print(response)\n",
    "        except Exception as err:\n",
    "            print('Exception occurred. Please try again', str(err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
