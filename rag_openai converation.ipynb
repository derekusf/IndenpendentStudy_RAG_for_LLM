{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "DOCUMENT = os.getenv(\"DOCUMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 1. text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "txt_path = DOCUMENT+\"rag.txt\"\n",
    "txt_loader = TextLoader(txt_path)\n",
    "text_documents = txt_loader.load()\n",
    "#text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 2. PDF document\n",
    "\n",
    "We use PyMuPDFLoader in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_path = DOCUMENT+ \"2005.11401v4.pdf\"\n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(text_documents)\n",
    "#documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_chunks + pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using openAI embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to persist the vectordb with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      2\u001b[0m persist_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTORAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m Chroma\u001b[38;5;241m.\u001b[39mfrom_documents(documents\u001b[38;5;241m=\u001b[39m\u001b[43mpdf_chunks\u001b[49m,  embedding\u001b[38;5;241m=\u001b[39membeddings, persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory)\n\u001b[1;32m      4\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"STORAGE\")\n",
    "vectordb = Chroma.from_documents(documents=pdf_chunks,  embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipline 2. Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_query = \"What is retrieval augmented generation\"\n",
    "user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is. Here the on memory vectorstore is used. \n",
    "There is opportunity to improve efficiency of search when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vectorstore.as_retriever()\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "#Load vectordb from persisted store\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"STORAGE\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "newvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = newvectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Augmented Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Which country is United State',\n",
       " 'context': [Document(page_content='19', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './document/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 18, 'producer': 'pdfTeX-1.40.21', 'source': './document/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='19', metadata={'author': '', 'creationDate': \"D:20210413004838Z00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': './document/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20240620010108Z00'00'\", 'page': 18, 'producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext, AppendMode 1.1', 'source': './document/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='country to host this international sports competition twice.‚Äù As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciÔ¨Åcity. We deÔ¨Åne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciÔ¨Åcity as high mutual dependence between the input and output [33]. We follow', metadata={'author': '', 'creationDate': \"D:20210413004838Z00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': './document/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': \"D:20240620010108Z00'00'\", 'page': 4, 'producer': 'macOS Version 14.5 (Build 23F79) Quartz PDFContext, AppendMode 1.1', 'source': './document/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
       "  Document(page_content='country to host this international sports competition twice.‚Äù As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\\nmatching entities and has higher correlation with human judgment for question generation than\\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\\none for speciÔ¨Åcity. We deÔ¨Åne factuality as whether a statement can be corroborated by trusted external\\nsources, and speciÔ¨Åcity as high mutual dependence between the input and output [33]. We follow', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': './document/2005.11401v4.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 4, 'producer': 'pdfTeX-1.40.21', 'source': './document/2005.11401v4.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''})],\n",
       " 'answer': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"Which country is United State\"\n",
    "response = rag_chain.invoke({\"input\":user_query})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    model, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Conversation Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a method that enhances the output of large language models by incorporating external knowledge sources for generating responses. It enables large language models to access authoritative information outside their original training data to improve the relevance and accuracy of their output. RAG extends the capabilities of language models without requiring retraining, making it a cost-effective way to enhance their performance.\n",
      "To implement Retrieval-Augmented Generation (RAG), developers can introduce an information retrieval component that pulls data from external sources based on user input. This retrieved information, along with the user query, is then provided to the large language model for generating responses. By connecting the model to live social media feeds, news sites, or other updated sources, developers can ensure the model provides the latest and most relevant information to users, enhancing trust and accuracy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is RAG?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"How to implement it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "print(ai_msg_1[\"answer\"])\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2. Case 1 - Specific knowledge query first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) optimizes large language models by referencing external authoritative knowledge bases to enhance responses without retraining. RAG extends the capabilities of large language models to specific domains or internal knowledge bases for improved accuracy and relevance. It allows for presenting accurate information with source attribution, increasing user trust in generative AI solutions.\n",
      "Implementing Retrieval-Augmented Generation (RAG) involves redirecting a large language model to retrieve information from authoritative knowledge sources. Developers can connect the model to live data feeds, news sites, or other updated sources for real-time information. By providing the latest research, statistics, or news to the generative models, RAG ensures the output remains current and relevant.\n",
      "Vector databases play a crucial role in storing and matching vector representations of user queries and database entries for efficient retrieval of relevant information. They enable quick retrieval of similar vectors, allowing systems like smart chatbots to provide accurate and contextually relevant responses. By leveraging vector databases, systems can efficiently manage memory and disk footprints while enhancing the accuracy and speed of information retrieval processes.\n",
      "Another method besides text generation is utilizing pre-trained language models for various natural language processing tasks. These models, such as BERT or GPT, can be fine-tuned on specific datasets to perform tasks like sentiment analysis, question-answering, named entity recognition, and more. By leveraging pre-trained language models, developers can benefit from transfer learning and achieve state-of-the-art results in various NLP applications.\n",
      "Some examples of pre-trained language models that can be fine-tuned for specific NLP tasks include BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pre-trained Transformer 3), RoBERTa (A Robustly Optimized BERT Approach), and T5 (Text-to-Text Transfer Transformer). These models have been widely used in the research and industry for tasks like text classification, language translation, summarization, and more, showcasing their versatility and effectiveness across various NLP applications.\n",
      "If you have any more questions in the future, feel free to ask. Have a great day!\n",
      "The United States is located in North America. It is bordered by Canada to the north, Mexico to the south, the Atlantic Ocean to the east, and the Pacific Ocean to the west.\n",
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter a query: \")\n",
    "    \n",
    "\n",
    "    if question == \"exit\":\n",
    "            break\n",
    "    \n",
    "    response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "    chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    "    )\n",
    "    print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2. Case 2 - common knowledge query first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n",
      "I don't know.\n",
      "The United States is a country in North America.\n",
      "The capital of the United States is Washington, D.C.\n",
      "The capital of England is London.\n",
      "Retrieval-Augmented Generation (RAG) optimizes large language models to incorporate external knowledge sources for generating responses.\n",
      "RAG can be implemented by introducing an information retrieval component that pulls relevant data from external sources based on user input before generating a response.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    question = input(\"Enter a query: \")\n",
    "    \n",
    "\n",
    "    if question == \"exit\":\n",
    "            break\n",
    "    \n",
    "    response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "    chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=response[\"answer\"]),\n",
    "    ]\n",
    "    )\n",
    "    print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run f30edaea-eb47-418f-9a15-e3b1e9752789 not found for run 5ce879cb-37c2-4cee-a8b8-1833fdd0cc77. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 02f8e42b-bb76-408d-806e-dd832c7f9bd7 not found for run 74bc8a04-9955-489f-b3c5-1e9d3cc21f87. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) optimizes large language models by referencing external knowledge bases. RAG enhances the capabilities of large language models without retraining them. It improves output relevance, accuracy, and usefulness in various contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e1973177-1906-485a-91dd-8a46067c8e6e not found for run 65d5e329-9a48-425a-9637-86fd702aac02. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG implementation involves introducing an information retrieval component to pull relevant data. This data is combined with user input and given to the large language model (LLM) for generating responses. Implementing RAG enhances the LLM's output by incorporating external knowledge sources without the need for retraining the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 1e5fc832-b2ab-4114-b92d-40bd73153223 not found for run 76e8cfef-5724-4f5a-bc4d-ba4831dc0a55. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run a5b14192-4ff3-4049-8c45-fdcc367e9bee not found for run 10935454-82b2-430a-904f-275aa90bead1. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 09d068eb-b274-4e12-b865-8477f0c485f7 not found for run 27184a4f-27fb-41b5-b3c8-c4abdea1509a. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run f3440b76-2aad-45d5-9c07-02c8617158df not found for run a51de99c-c99b-407d-85f9-86b3c762b98b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besides text generation, other methods in natural language processing include text classification, named entity recognition, sentiment analysis, machine translation, and question-answering systems. These methods serve various purposes such as information extraction, summarization, and language understanding tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 7b890905-b196-401b-9312-304b5428a739 not found for run 4b6ccd1e-8b15-4fc4-8615-27261b8eb90f. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector databases play a crucial role in semantic search technologies by storing vector representations of documents or data. These databases enable efficient matching of user queries to relevant vectors, allowing for accurate retrieval of information. The vector representations help in calculating relevance scores and retrieving semantically related documents for improved search results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run b9e2e058-75d4-44f6-9534-62eba833abe5 not found for run a3274270-182f-4dde-86db-092ff64bfa98. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector databases store numerical representations of data, enabling efficient information retrieval based on similarity metrics. They are used in various applications, such as semantic search and recommendation systems, to match queries with relevant vectors. These databases play a vital role in enhancing the performance of AI models by enabling faster and more accurate retrieval of information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 293639de-43a7-49f4-b58d-f3ccc7209061 not found for run 5f849fc1-a626-40ca-98a8-74934f4ed86b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One example of a vector database product is Milvus, an open-source vector similarity search engine by Zilliz. Milvus is designed for storing and processing vectors efficiently, enabling applications like recommendation systems, image search, and natural language processing tasks. It provides scalable and high-performance vector storage and retrieval capabilities for AI applications.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 38c0bb07-eebe-4438-a896-21ce16612bf5 not found for run 08ecb111-dfc1-4f31-87ad-057fea98aa3b. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States, commonly known as the U.S., is a country primarily located in North America.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 59e32d1b-9666-49ae-9ecb-8178d4f1c43e not found for run 433473f1-8fdc-46ea-ac66-2b2167057fff. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    question = input(\"Enter a query: \")\n",
    "\n",
    "    if question == \"exit\":\n",
    "            break\n",
    "    \n",
    "    response = conversational_rag_chain.invoke(\n",
    "        {\"input\": question},\n",
    "        config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "        },\n",
    "    )  \n",
    "    print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Where is US?\n",
      "\n",
      "AI: I don't know.\n",
      "\n",
      "User: What is RAG?\n",
      "\n",
      "AI: Retrieval-Augmented Generation (RAG) optimizes large language models by referencing external knowledge bases. RAG enhances the capabilities of large language models without retraining them. It improves output relevance, accuracy, and usefulness in various contexts.\n",
      "\n",
      "User: How to implement it?\n",
      "\n",
      "AI: RAG implementation involves introducing an information retrieval component to pull relevant data. This data is combined with user input and given to the large language model (LLM) for generating responses. Implementing RAG enhances the LLM's output by incorporating external knowledge sources without the need for retraining the model.\n",
      "\n",
      "User: what is the capital of US?\n",
      "\n",
      "AI: I don't know.\n",
      "\n",
      "User: Where is United State?\n",
      "\n",
      "AI: I don't know.\n",
      "\n",
      "User: Which country is United State?\n",
      "\n",
      "AI: I don't know.\n",
      "\n",
      "User: What are other method besides text generation?\n",
      "\n",
      "AI: Besides text generation, other methods in natural language processing include text classification, named entity recognition, sentiment analysis, machine translation, and question-answering systems. These methods serve various purposes such as information extraction, summarization, and language understanding tasks.\n",
      "\n",
      "User: What is the role of vector database?\n",
      "\n",
      "AI: Vector databases play a crucial role in semantic search technologies by storing vector representations of documents or data. These databases enable efficient matching of user queries to relevant vectors, allowing for accurate retrieval of information. The vector representations help in calculating relevance scores and retrieving semantically related documents for improved search results.\n",
      "\n",
      "User: What are they?\n",
      "\n",
      "AI: Vector databases store numerical representations of data, enabling efficient information retrieval based on similarity metrics. They are used in various applications, such as semantic search and recommendation systems, to match queries with relevant vectors. These databases play a vital role in enhancing the performance of AI models by enabling faster and more accurate retrieval of information.\n",
      "\n",
      "User: Any sample product?\n",
      "\n",
      "AI: One example of a vector database product is Milvus, an open-source vector similarity search engine by Zilliz. Milvus is designed for storing and processing vectors efficiently, enabling applications like recommendation systems, image search, and natural language processing tasks. It provides scalable and high-performance vector storage and retrieval capabilities for AI applications.\n",
      "\n",
      "User: Which country is US?\n",
      "\n",
      "AI: The United States, commonly known as the U.S., is a country primarily located in North America.\n",
      "\n",
      "User: What is its capital?\n",
      "\n",
      "AI: The capital of the United States is Washington, D.C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 4. Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
