,context,question,ground_truth
0,"Seven Failure Points When Engineering a Retrieval Augmented
Generation System
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
{scott.barnett,stefanus.kurniawan,srikanth.thudumu,zach.brannelly,mohamed.abdelrazek}@deakin.edu.au
Applied Artificial Intelligence Institute
Geelong, Australia
ABSTRACT
Software engineers are increasingly adding semantic search capabil-
ities to applications using a strategy known as Retrieval Augmented
Generation (RAG). A RAG system involves finding documents that
semantically match a query and then passing the documents to a
large language model (LLM) such as ChatGPT to extract the right
answer using an LLM. RAG systems aim to: a) reduce the problem
of hallucinated responses from LLMs, b) link sources/references
to generated responses, and c) remove the need for annotating
documents with meta-data. However, RAG systems suffer from lim-
itations inherent to information retrieval systems and from reliance
on LLMs. In this paper, we present an experience report on the
failure points of RAG systems from three case studies from separate
domains: research, education, and biomedical. We share the lessons
learned and present 7 failure points to consider when designing a
RAG system. The two key takeaways arising from our work are: 1)
validation of a RAG system is only feasible during operation, and
2) the robustness of a RAG system evolves rather than designed in
at the start. We conclude with a list of potential research directions
on RAG systems for the software engineering community.
CCS CONCEPTS
• Software and its engineering →Empirical software valida-
tion.
KEYWORDS
Retrieval Augmented Generation, RAG, SE4AI, Case Study
ACM Reference Format:
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mo-
hamed Abdelrazek . 2024. Seven Failure Points When Engineering a Retrieval
Augmented Generation System. In Proceedings of 3rd International Confer-
ence on AI Engineering — Software Engineering for AI (CAIN 2024). ACM,
New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
1
INTRODUCTION
The new advancements of Large Language Models (LLMs), includ-
ing ChatGPT, have given software engineers new capabilities to
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CAIN 2024, April 2024, Lisbon, Portugal
© 2024 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn
build new HCI solutions, complete complex tasks, summarise docu-
ments, answer questions in a given artefact(s), and generate new
content. However, LLMs suffer from limitations when it comes
to up-to-date knowledge or domain-specific knowledge currently
captured in company’s repositories. Two options to address this
problem are: a) Finetuning LLMs (continue training an LLM using
domain specific artifacts) which requires managing or serving a
fine-tuned LLM; or b) use Retrieval-Augmented Generation (RAG)
Systems that rely on LLMs for generation of answers using existing
(extensible) knowledge artifacts. Both options have pros and cons
related to privacy/security of data, scalability, cost, skills required,
etc. In this paper, we focus on the RAG option.
Retrieval-Augmented Generation (RAG) systems offer a com-
pelling solution to this challenge. By integrating retrieval mecha-
nisms with the generative capabilities of LLMs, RAG systems can
synthesise contextually relevant, accurate, and up-to-date informa-
tion. A Retrieval-Augmented Generation (RAG) system combines
information retrieval capabilities, and generative prowess of LLMs.
The retrieval component focuses on retrieving relevant information
for a user query from a data store. The generation component fo-
cuses on using the retrieved information as a context to generate an
answer for the user query. RAG systems are an important use case
as all unstructured information can now be indexed and available
to query reducing development time no knowledge graph creation
and limited data curation and cleaning.
Software engineers building RAG systems are expected to pre-
process domain knowledge captured as artifacts in different formats,
store processed information in appropriate data store (vector data-
base), implement or integrate the right query-artifact matching
strategy, rank matched artifacts, and call the LLMs API passing in
user queries and context documents. New advances for building
RAG systems are constantly emerging [8, 12] but how they relate
and perform for a specific application context has to be discovered.
In this work we present the lessons learned and 7 failure points
arising from 3 case studies. The purpose of this paper is to provide
1) a reference to practitioners and 2) to present a research road
map for RAG systems. To the best of our knowledge, we present
the first empirical insight into the challenges with creating robust
RAG systems. As advances in LLMs continue to take place, the
software engineering community has a responsibility to provide
knowledge on how to realise robust systems with LLMs. This work
is an important step for robustness in building RAG systems.
Research questions for this work include:
• What are the failure points that occur when engineering a RAG
system? (section 5) We present an empirical experiment using
the BioASQ data set to report on potential failure points. The
experiment involved 15,000 documents and 1000 question
arXiv:2401.05856v1  [cs.SE]  11 Jan 2024
",Question: What is the name of the Large Language Model mentioned in the text?,Answer: ChatGPT.
1,"CAIN 2024, April 2024, Lisbon, Portugal
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
and answer pairs. We indexed all documents then ran the
queries and stored the generated responses using GPT-4. All
question and answer pairs were then validated with OpenAI
evals 1. Manual inspection (all discrepancies, all flagged as
incorrect, and a sample of correct labels) was analysed to
identify the patterns.
• What are the key considerations when engineering a RAG
system? (section 6) We present the lessons learned from three
case studies involving the implementation of a RAG system.
This presents the challenges faced and insights gained.
Contributions arising from this work include:
• A catalogue of failure points (FP) that occur in RAG systems.
• An experience report from 3 case studies of implementing a
RAG system. Two currently running at Deakin University.
• A research direction for RAG systems based on the lessons
learned from the 3 case studies.
2
RELATED WORK
Retrieval augmented generation encompasses using documents
to augment large language models through pre-training and at
inference time [7, 9, 12]. Due to the compute cost, data preparation
time and required resources using RAG without training or fine-
tuning is an attractive proposition. However, challenges arise when
using large language models for information extraction such as
performance with long text [8].
A recent survey [19] showed that large language models are
used across the RAG pipeline including retriever, data generation,
rewriter, and reader. Our work complements this survey by taking
a software engineering perspective to shine a light on what issues
engineers will face and what software engineering research is nec-
essary to realise solutions with the current state-of-the-art RAG
systems.
Emerging work has looked at benchmarking RAG systems [3]
but not at the failures occurring during implementation. Software
engineering research has investigated the use of RAG systems for
code-related tasks [15]. However, the application of RAG systems
is broader than software engineering tasks. This paper comple-
ments existing work by presenting challenges faced during the
implementation of a RAG system with a focus on practitioners.
Errors and failures that arise from RAG systems overlap with
other information retrieval systems including 1) no metrics for
query rewriting, 2) document re-ranking, and 3) effective content
summarisation [19]. Our results confirm this The unique aspects
are related to the semantic and generative nature of the use of large
language models including evaluating factual accuracy [16].
3
RETRIEVAL AUGMENTED GENERATION
With the explosion in popularity of large language model services
such as ChatGPT2, Claude3, and Bard 4, people have explored their
use as a question and answering systems. While the performance
is impressive [16] there are two fundamental challenges: 1) hallu-
cinations - where the LLM produces a response that looks right
1https://github.com/openai/evals
2https://chat.openai.com/
3https://claude.ai/
4https://bard.google.com/
but is incorrect, and 2) unbounded - no way to direct or update
the content of the output (other than through prompt engineering).
A RAG system is an information retrieval approach designed to
overcome the limitations of using a LLM directly.
RAG works by taking a natural language query is converted into
an embedding which is used to semantically search a set of docu-
ments. Retrieved documents are then passed to a large language
model to generate an answer. An overview of a RAG system is
shown in Figure 1 as two separate processes, Index and Query. See
this survey for more details [19]
3.1
Index Process
In a RAG system, the retrieval system works using embeddings
that provide a compressed semantic representation of the docu-
ment. An embedding is expressed as a vector of numbers. During
the Index process each document is split into smaller chunks that
are converted into an embedding using an embedding model. The
original chunk and the embedding are then indexed in a database.
Software engineers face design decisions around how best to chunk
the document and how large a chunk should be. If chunks are too
small certain questions cannot be answered, if the chunks are too
long then the answers include generated noise.
Different types of documents require different chunking and pro-
cessing stages. For example, video content requires a transcription
pipeline to extract the audio and convert to text prior to encoding
(see subsection 4.2. The choice of which embedding to use also
matters as changing the embedding strategy requires re-indexing
all chunks. An embedding should be chosen based on the ability to
semantically retrieve correct responses. This process depends on
the size of the chunks, the types of questions expected, the structure
of the content and the application domain.
3.2
Query Process
The Query process takes place at run time. A question expressed
as natural language is first converted into a general query. To gen-
eralise the query a large language model is used which enables
additional context such as previous chat history to be included
in the new query. An embedding is then calculated from the new
query to use for locating relevant documents from the database.
Top-k similar documents are retrieved using a similarity method
such as cosine similarity (vector databases have techniques such as
inverted indexes to speed up retrieval time). The intuition is that
chunks that are semantically close to the query are likely to contain
the answer.
Retrieved documents are then re-ranked to maximise the likeli-
hood that the chunk with the answer is located near the top. The
next stage is the Consolidator which is responsible for processing
the chunks. This stage is needed to overcome the limitations of
large language models 1) token limit and 2) rate limit. Services such
as OpenAI have hard limits on the amount of text to include in a
prompt. This restricts the number of chunks to include in a prompt
to extract out an answer and a reduction strategy is needed to chain
prompts to obtain an answer. These online services also restrict the
number of tokens to use within a time frame restricting the latency
of a system. Software engineers need to consider these tradeoffs
when designing a RAG system.
",Question: What is the name of the University where two RAG systems are currently running?,Answer: Deakin University.
2,"Seven Failure Points When Engineering a Retrieval Augmented Generation System
CAIN 2024, April 2024, Lisbon, Portugal
Figure 1: Indexing and Query processes required for creating a Retrieval Augmented Generation (RAG) system. The indexing
process is typically done at development time and queries at runtime. Failure points identified in this study are shown in red
boxes. All required stages are underlined. Figure expanded from [19].
The final stage of a RAG pipeline is when the answer is extracted
from the generated text. Readers are responsible for filtering the
noise from the prompt, adhering to formatting instructions (i.e. an-
swer the question as a list of options), and producing the output to
return for the query. Implementation of a RAG system requires cus-
tomising multiple prompts to process questions and answers. This
process ensures that questions relevant for the domain are returned.
The use of large language models to answer real time questions
from documents opens up new application domains where question
and answering is new capability. Thus, RAG systems are difficult
to test as no data exists and needs to be experimentally discov-
ered through either a) synthetic data generation, or b) piloting the
system with minimal testing.
4
CASE STUDIES
This study conducted three case studies to discover the challenges
that arise when implementing RAG systems. A summary of each of
the case studies is shown in Table 1. All scripts, data, and examples
of each of the failure points for the BioASQ case study are available
online 5. The other two case studies have been excluded due to
confidentiality concerns.
4.1
Cognitive Reviewer
Cognitive Reviewer is a RAG system designed to support researchers
in analysing scientific documents. Researchers specify a research
question or objective and then upload a collection of related re-
search papers. All of the documents are then ranked in accordance
with the stated objective for the researcher to manually review.
The researcher can also ask questions directly against all of the
documents. Cognitive Reviewer is currently used by PhD students
from Deakin University to support their literature reviews. The
Cognitive Reviewer does the Index process at run time and relies
5https://figshare.com/s/fbf7805b5f20d7f7e356
on a robust data processing pipeline to handle uploaded documents
i.e. no quality control possible at development time. This system
also uses a ranking algorithm to sort the uploaded documents.
4.2
AI Tutor
The AI Tutor is a RAG system where students ask questions about
the unit and answers are sourced from the learning content. Stu-
dents are able to verify the answers by accessing a sources list from
where the answer came from. The AI Tutor works by integrating
into Deakin’s learning management system, indexing all of the
content including PDF documents, videos, and text documents. As
part of the Index process, videos are transcribed using the deep
learning model Whisper [17] before being chunked. The AI Tutor
was developed between August 2023 to November 2023 for a pilot
in a unit with 200 students that commenced the 30th of October
2023. Our intention is to present the lessons learned during imple-
mentation and present a followup findings at the conclusion of the
pilot. This RAG pipeline includes a rewriter to generalise queries.
We implemented a chat interface where previous dialogue between
the user and the AI Tutor was used as part of the context for each
question. The rewriter considers this context and rewrites the query
to resolve ambiguous requests such as ‘Explain this concept further.’
4.3
Biomedical Question and Answer
The previous case studies focused on documents with smaller con-
tent sizes. To explore the issues at a larger scale we created a RAG
system using the BioASQ [10] dataset comprised of questions, links
to document, and answers. The answers to questions were one of
yes/no, text summarisation, factoid, or list. This dataset was pre-
pared by biomedical experts and contains domain specific question
and answer pairs. We downloaded 4017 open access documents
from the BioASQ dataset and had a total of 1000 questions. All
documents were indexed and the questions asked against the RAG
system. The generated questions were then evaluated using the
",Question: What is the name of the deep learning model used for video transcription in the AI Tutor system?,Answer: Whisper
3,"CAIN 2024, April 2024, Lisbon, Portugal
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
Case Study
Domain
Doc Types
Dataset Size
RAG Stages
Sample Questions
Cognitive
Reviewer*
Research
PDFs
(Any size)
Chunker, Rewriter, Re-
triever, Reader
What are the key points covered in
this paper?
AI Tutor*
Education
Videos, HTML,
PDF
38
Chunker, Rewriter,
Retriever, Reader
What were the topics covered in
week 6?
BioASQ
Biomedical
Scientific PDFs
4017
Chunker,
Retriever,
Reader
Define pseudotumor cerebri. How
is it treated?
Table 1: A summary of the RAG case studies presented in this paper. Case studies marked with a * are running systems currently
in use.
OpenEvals technique implemented by OpenAI6. From the gener-
ated questions we manually inspected 40 issues and all issues that
the OpenEvals flagged as inaccurate. We found that the automated
evaluation was more pessimistic than a human rater for this domain.
However, one threat to validity with this finding is that BioASQ is
a domain specific dataset and the reviewers were not experts i.e.
the large language model may know more than a non-expert.
5
FAILURE POINTS OF RAG SYSTEMS
From the case studies we identified a set of failure points presented
below. The following section addresses the research question What
are the failure points that occur when engineering a RAG system?
FP1 Missing Content The first fail case is when asking a ques-
tion that cannot be answered from the available documents.
In the happy case the RAG system will respond with some-
thing like “Sorry, I don’t know"". However, for questions that
are related to the content but don’t have answers the system
could be fooled into giving a response.
FP2 Missed the Top Ranked Documents The answer to the
question is in the document but did not rank highly enough
to be returned to the user. In theory, all documents are ranked
and used in the next steps. However, in practice the top K
documents are returned where K is a value selected based
on performance.
FP3 Not in Context - Consolidation strategy Limitations
Documents with the answer were retrieved from the data-
base but did not make it into the context for generating an
answer. This occurs when many documents are returned
from the database and a consolidation process takes place to
retrieve the answer.
FP4 Not Extracted Here the answer is present in the context,
but the large language model failed to extract out the correct
answer. Typically, this occurs when there is too much noise
or contradicting information in the context.
FP5 Wrong Format The question involved extracting informa-
tion in a certain format such as a table or list and the large
language model ignored the instruction.
FP6 Incorrect Specificity The answer is returned in the re-
sponse but is not specific enough or is too specific to address
the user’s need. This occurs when the RAG system designers
have a desired outcome for a given question such as teach-
ers for students. In this case, specific educational content
should be provided with answers not just the answer. Incor-
rect specificity also occurs when users are not sure how to
ask a question and are too general.
6https://github.com/openai/evals
FP7 Incomplete Incomplete answers are not incorrect but miss
some of the information even though that information was in
the context and available for extraction. An example question
such as “What are the key points covered in documents
A, B and C?” A better approach is to ask these questions
separately.
6
LESSONS AND FUTURE RESEARCH
DIRECTIONS
The lessons learned from the three case studies are shown in Table 2.
We present our findings for the research question: What are the
key considerations when engineering a RAG system? Based on our
takeaways we identified multiple potential research areas linked to
RAG as follows:
6.1
Chunking and Embeddings
Chunking documents sounds trivial. However, the quality of chunk-
ing affects the retrieval process in many ways and in particular
on the embeddings of the chunk then affects the similarity and
matching of chunks to user queries. There are two ways of chunk-
ing: heuristics based (using punctuation, end of paragraph, etc.),
and semantic chunking (using the semantics in the text to inform
start-end of a chunk). Further research should explore the tradeoffs
between these methods and their effects on critical downstream
processes like embedding and similarity matching. A systematic
evaluation framework comparing chunking techniques on metrics
like query relevance and retrieval accuracy would benefit the field.
Embeddings represent another active research area, including
generating embeddings for multimedia and multimodal chunks
such as tables, figures, formulas, etc. Chunk embeddings are typ-
ically created once during system development or when a new
document is indexed. Query preprocessing significantly impacts
a RAG system’s performance, particularly handling negative or
ambiguous queries. Further research is needed on architectural pat-
terns and approaches [5] to address the inherent limitations with
embeddings (quality of a match is domain specific).
6.2
RAG vs Finetuning
LLMs are great world models due to the amount of training data, and
finetuning tasks applied on the model before it’s released. However,
these models are general-purpose models (may not know the very
specifics of your domain) and also not up to date (there is a cutoff
date on their knowledge). Fine-tuning and RAG offer two potential
customisation pathways, each with distinct tradeoffs. Finetuning
requires curating internal datasets to adapt and train the LLM on.
However, all your data are baked into the model and you need to
",Question: What are the failure points that occur when engineering a RAG system?,"Answer: FP1 Missing Content, FP2 Missed the Top Ranked Documents, FP3 Not in Context - Consolidation strategy Limitations, FP4 Not Extracted, FP5 Wrong Format, FP6 Incorrect Specificity, FP7 Incomplete."
4,"Seven Failure Points When Engineering a Retrieval Augmented Generation System
CAIN 2024, April 2024, Lisbon, Portugal
FP
Lesson
Description
Case Studies
FP4
Larger context get better results (Context refers to a
particular setting or situation in which the content
occurs)
A larger context enabled more accurate responses
(8K vs 4K). Contrary to prior work with GPT-3.5 [13]
AI Tutor
FP1
Semantic caching drives cost and latency down
RAG systems struggle with concurrent users due to
rate limits and the cost of LLMs. Prepopulate the
semantic cache with frequently asked questions [1].
AI Tutor
FP5-7
Jailbreaks bypass the RAG system and hit the safety
training.
Research suggests fine-tuning LLMs reverses safety
training [11], test all fine-tuned LLMs for RAG sys-
tem.
AI Tutor
FP2, FP4
Adding meta-data improves retrieval.
Adding the file name and chunk number into the
retrieved context helped the reader extract the re-
quired information. Useful for chat dialogue.
AI Tutor
FP2, FP4-7
Open source embedding models perform better for
small text.
Opensource sentence embedding models performed
as well as closed source alternatives on small text.
BioASQ, AI Tutor
FP2-7
RAG systems require continuous calibration.
RAG systems receive unknown input at runtime
requiring constant monitoring.
AI Tutor, BioASQ
FP1, FP2
Implement a RAG pipeline for configuration.
A RAG system requires calibrating chunk size,
embedding strategy, chunking strategy, retrieval
strategy, consolidation strategy, context size, and
prompts.
Cognitive Reviewer,
AI Tutor, BioASQ
FP2, FP4
RAG pipelines created by assembling bespoke solu-
tions are suboptima.
End-to-end training enhances domain adaptation
in RAG systems [18].
BioASQ, AI Tutor
FP2-7
Testing performance characteristics are only possi-
ble at runtime.
Offline evaluation techniques such as G-Evals [14]
look promising but are premised on having access
to labelled question and answer pairs.
Cognitive Reviewer,
AI Tutor
Table 2: The lessons learned from the three case studies with key takeaways for future RAG implementations
sort out the security/privacy (who can access what). Furthermore,
as the foundation model itself evolves or you get new data to add to
the model, you will need to run finetuning again. On the other side,
RAG systems seem to offer a pragmatic solution allowing you to
chunk your data as needed and only use relevant chunks into the
context to ask the LLM to generate an answer from the included
context. This facilitates continuously updating the knowledge with
new documents and also gives the control over what chunks the user
is able to access. However, optimal strategies for chunk embedding,
retrieval, and contextual fusion remain active research. Further
work should systematically compare finetuning and RAG paradigms
across factors including accuracy, latency, operating costs, and
robustness.
6.3
Testing and Monitoring RAG systems
Software engineering best practices are still emerging for RAG sys-
tems. Software testing and test case generation are one of the areas
for refinement. RAG systems require questions and answers that are
application specific often unavailable when indexing unstructured
documents. Emerging work has considered using LLMs for gen-
erating questions from multiple documents [4]. How to generate
realistic domain relevant questions and answers remains an open
problem.
Once suitable test data is available quality metrics are also re-
quired to assist engineers in making quality tradeoffs. Using large
language models is expensive, introduces latency concerns, and has
performance characteristics that all change with each new release.
This characteristic has previously been studied for machine learn-
ing systems [5, 6] but the required adaptations (if any) have yet to
be applied to LLM based systems such as RAGs. Another idea is to
incorporate ideas from self-adaptive systems to support monitoring
and adapting RAG systems, preliminary work has started for other
machine learning applications [2].
7
CONCLUSION
RAG systems are a new information retrieval that leverages LLMs.
Software engineers increasingly interact with RAG systems a)
through implementing semantic search, or b) through new code-
dependent tasks. This paper presented the lessons learned from 3
case studies including an empirical investigation involving 15,000
documents and 1000 questions. Our findings provide a guide to
practitioners by presenting the challenges faced when implement-
ing RAG systems. We also included future research directions for
RAG systems related to 1) chunking and embeddings, 2) RAG vs
Finetuning, and 3) Testing and Monitoring. Large language models
are going to continue to obtain new capabilities of interest to engi-
neers and researchers. This paper presents the first investigation
into RAG systems from a software engineering perspective.
ACKNOWLEDGMENTS
To Amanda Edgar, Rajesh Vasa, Kon Mouzakis, Matteo Vergani,
Trish McCluskey, Kathryn Perus, Tara Draper, Joan Sutherland and
Ruary Ross for their support and involvement in making the AI
Tutor project possible.
",Question: In what city and country was the CAIN 2024 conference held?,"Answer: Lisbon, Portugal"
5,"CAIN 2024, April 2024, Lisbon, Portugal
Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, Mohamed Abdelrazek
REFERENCES
[1] Fu Bang. 2023. GPTCache: An Open-Source Semantic Cache for LLM Applications
Enabling Faster Answers and Cost Savings. In 3rd Workshop for Natural Language
Processing Open Source Software.
[2] Maria Casimiro, Paolo Romano, David Garlan, Gabriel Moreno, Eunsuk Kang, and
Mark Klein. 2022. Self-adaptive Machine Learning Systems: Research Challenges
and Opportunities. 133–155. https://doi.org/10.1007/978-3-031-15116-3_7
[3] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023.
Benchmarking
Large Language Models in Retrieval-Augmented Generation. arXiv preprint
arXiv:2309.01431 (2023).
[4] Mingda Chen, Xilun Chen, and Wen-tau Yih. 2023. Efficient Open Domain
Multi-Hop Question Answering with Few-Shot Data Synthesis. arXiv preprint
arXiv:2305.13691 (2023).
[5] Alex Cummaudo, Scott Barnett, Rajesh Vasa, and John Grundy. 2020. Threshy:
Supporting safe usage of intelligent web services. In Proceedings of the 28th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 1645–1649.
[6] Alex Cummaudo, Scott Barnett, Rajesh Vasa, John Grundy, and Mohamed Ab-
delrazek. 2020. Beware the evolving ‘intelligent’web service! An integration
architecture tactic to guard AI-first components. In Proceedings of the 28th ACM
Joint Meeting on European Software Engineering Conference and Symposium on
the Foundations of Software Engineering. 269–280.
[7] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.
Retrieval augmented language model pre-training. In International conference on
machine learning. PMLR, 3929–3938.
[8] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-
light: Efficient and effective retrieval-augmented text generation. In Proceedings
of the 46th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 1437–1447.
[9] Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with
generative models for open domain question answering.
arXiv preprint
arXiv:2007.01282 (2020).
[10] Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios
Paliouras. 2023. BioASQ-QA: A manually curated corpus for biomedical question
answering. Scientific Data 10 (2023), 170. Citation Key: 422.
[11] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. 2023. LoRA Fine-tuning
Efficiently Undoes Safety Training in Llama 2-Chat 70B. arXiv:2310.20624 [cs.LG]
[12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[13] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models
use long contexts. arXiv preprint arXiv:2307.03172 (2023).
[14] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang
Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment, may
2023. arXiv preprint arXiv:2303.16634 (2023).
[15] Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. Retrieval-based prompt selec-
tion for code-related few-shot learning. In Proceedings of the 45th International
Conference on Software Engineering (ICSE’23).
[16] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/ARXIV.2303.
08774
[17] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision.
In International Conference on Machine Learning. PMLR, 28492–28518.
[18] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-
arachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain
adaptation of retrieval augmented generation (RAG) models for open domain
question answering. Transactions of the Association for Computational Linguistics
11 (2023), 1–17.
[19] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chen-
long Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for
information retrieval: A survey. arXiv preprint arXiv:2308.07107 (2023).
",Question: What is the title of the workshop where Fu Bang presented his paper about GPTCache?,Answer: 3rd Workshop for Natural Language Processing Open Source Software.
6,"Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented 
Generation in Niche Domains, Exemplified by Korean Medicine 
 
Bongsu Kang1, Jundong Kim1, Tae-Rim Yun1, Chang-Eop Kim1, 2, * 
 
1Department of Physiology, College of Korean Medicine, Gachon University, Seongnam, Gyeonggi, 
Republic of Korea 
2Department of Neurobiology, Stanford University School of Medicine, Stanford, California, USA 
 
* Corresponding Author: Chang-Eop Kim 
Email: eopchang@gachon.ac.kr 
 
 
 
 
 
ABSTRACT 
 
We propose a natural language prompt-based retrieval augmented generation (Prompt-RAG), a novel 
approach to enhance the performance of generative large language models (LLMs) in niche domains. 
Conventional RAG methods mostly require vector embeddings, yet the suitability of generic LLM-
based embedding representations for specialized domains remains uncertain. To explore and exemplify 
this point, we compared vector embeddings from Korean Medicine (KM) and Conventional Medicine 
(CM) documents, finding that KM document embeddings correlated more with token overlaps and less 
with human-assessed document relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from 
conventional RAG models, operates without the need for embedding vectors. Its performance was 
assessed through a Question-Answering (QA) chatbot application, where responses were evaluated for 
relevance, readability, and informativeness. The results showed that Prompt-RAG outperformed 
existing models, including ChatGPT and conventional vector embedding-based RAGs, in terms of 
relevance and informativeness. Despite challenges like content structuring and response latency, the 
advancements in LLMs are expected to encourage the use of Prompt-RAG, making it a promising tool 
for other domains in need of RAG methods. 
 
Keywords: Retrieval augmented generation, Natural language process, Korean medicine, 
Conversational AI, Question-answering, GPT 
 
 
 
 
 
",Question: What is the email address of the corresponding author of the paper?,Answer: eopchang@gachon.ac.kr
7,"2 
1. Introduction 
Retrieval-Augmented Generation (RAG) models combine a generative model with an information 
retrieval function, designed to overcome the inherent constraints of generative models.(1) They 
integrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of 
external information sources, resulting in responses that are not only natural and human-like but also 
the latest, accurate, and contextually relevant to the query.(1-4) The interaction of the two modules 
(retrieval and generation) enables responses that would not be achievable with either module alone, 
making RAG more than just the sum of its components. This approach represents a significant milestone 
in the field of generative models by enabling the induction of high-quality responses in less-explored 
domains at a low expense.(5, 6)  
In the conventional RAG operation, the initial step involves converting input queries into vector 
embeddings, which are then used to retrieve relevant data from the vectorized database. Following this, 
the generative part of RAG utilizes the retrieved external data for producing contextually rich 
responses.(7) Thus, both the embedding and generative models are considered crucial factors in the 
performance of RAG, directly affecting the retrieval process.(8) However, in niche domains, the 
performance of generic LLM-based embedding models appears suboptimal compared to their 
effectiveness in more general fields. The lack of specialized training data in these domains results in 
embeddings that do not adequately capture the nuances and specificity of the domain(9), leading to less 
accurate and contextually relevant information retrieval. Despite the evident presence of these 
functional limitations, they have not been much identified through experiments, therefore the optimality 
of the conventional LLM-based vector embedding RAG methods for niche domains has remained in 
obscurity. Researchers have been aware of these shortcomings of LLMs and have explored 
supplementary processes such as fine-tuning to improve the performance.(8, 10-12) However, the cost 
of fine-tuning, especially when it involves adjusting the entire or majority of parameters in LLM, has 
rapidly become expensive, thereby increasing the demand for alternative solutions.(13-15)  
To address these challenges, we propose a novel methodology: Prompt-RAG. This new approach to 
RAG eliminates the reliance on vector embeddings, adopting a more direct and flexible retrieval process 
based on natural language prompts. It involves a large-scale pre-trained generative model that handles 
the entire steps from document retrieval to response generation without the need for a vector database 
or an algorithm for indexing and selecting vectors, thus having the processing structure of RAG greatly 
simplified. Therefore, it not only takes advantage of the RAG’s strength but also circumvents the 
limitations of conventional vector embedding-based methodology. Prompt-RAG is based on 
maximizing the use of the advanced natural language processing capabilities of LLMs. Especially using 
the latest GPT model, our method can compensate for the deficiencies in vector embedding-based RAG 
arising from the shortage of domain-specific knowledge. 
To examine the utility of Prompt-RAG in practice, we conducted two exemplary studies focusing on 
the Korean Medicine (KM) domain. KM, a branch of traditional East Asian medicine, has diverged 
from traditional Chinese medicine and Japanese Kampo medicine in aspects like physiological theories, 
treatments, and Sasang constitutional medicine.(16, 17) It was reported that GPT models have achieved 
excellent results in the United States Medical Licensing Examination (USMLE)(18-20), while 
",Question: What is the name of the traditional East Asian medicine branch that Korean Medicine has diverged from?,Answer: Traditional Chinese medicine and Japanese Kampo medicine.
8,"3 
ChatGPT’s scores on the Korean National Licensing Examination for Korean Medicine Doctors barely 
reached the passing threshold, underperforming in subjects unique to KM, especially Sasang 
constitutional medicine and public health & medicine-related law.(21) In this niche area, rich in 
specialized knowledge and distinct from Conventional Medicine (CM), we first demonstrated the 
functional suboptimality of LLM-based vector embeddings. Subsequently, we demonstrated Prompt-
RAG's effectiveness in this context. A Question-Answering (QA) chatbot based on Prompt-RAG was 
built using KM-specific documents, and our model’s performance was compared with that of ChatGPT 
and conventional vector embedding-based RAG models. This study not only highlights the challenges 
of conventional RAG methods in niche domains but also showcases the potential of Prompt-RAG as a 
more effective alternative. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What subjects did ChatGPT underperform in on the Korean National Licensing Examination for Korean Medicine Doctors?,Answer: Sasang constitutional medicine and public health & medicine-related law.
9,"4 
2. Design of Prompt-RAG 
In this study, we introduce Prompt-RAG, a novel approach distinct from the conventional vector 
embedding-based RAG. Prompt-RAG consists of three steps: preprocessing, heading selection, and 
retrieval-augmented generation. The overall scheme of Prompt-RAG might seem similar to that of 
conventional RAG methods. However, details in each step are quite distinguishable especially in that 
conventional RAGs rely on a complex multi-step process involving the vectorization of documents and 
algorithmic retrieval from a vector database for a generative model's response. The workflows of vector 
embedding-based RAG and our method are depicted in Figure 1. 
 
 
Figure. 1. Comparative workflows of two RAG models. (A) depicts the vector embedding-based RAG 
process. Relevant pieces of information are retrieved from a database of document embeddings through 
algorithms. The retrieved data are augmented in a generative model to produce a response. (B) illustrates 
the process of Prompt-RAG. An LLM-based generative model directly uses a table of contents for 
constructing a contextual reference, followed by generating a response with it. 
Abbreviation: RAG, Retrieval-augmented generation; LLM, Large-language model. 
 
1) Preprocessing 
Prompt-RAG initiates by extracting or creating a Table of Contents (ToC) from a user’s document(s), 
which is the main subject of the retrieval. The procedure can be done flexibly depending on the type of 
document and the user's preferences. One of the most ideal cases is that a ToC is already prepared, made 
by the author(s) of the document. And yet, even in the absence of a pre-determined ToC, it can be 
arbitrarily generated, for example, using a generative model or in a manual way, based on the 
document's quantitative, semantic, or individual divisions. It should be noted that the size of a ToC must 
not exceed the context window size of the generative model for heading selection. Consequently, some 
headings or details of the ToC (e.g., heading or page numbers, or hierarchical structure) might need to 
be removed in order to reduce the number of tokens. The body of the document should then be divided 
",Question: What is the abbreviation of LLM in the context of Prompt-RAG?,Answer: Large-language model.
10,"5 
into sections according to the headings and prepared for subsequent retrieval. 
 
2) Heading selection 
A prompt, which contains both a query and a ToC, is passed to an LLM-based generative model and 
the model is asked to autonomously select the headings most pertinent to the query or those that help 
the most to find information concerning the query. Multiple heading selections can be performed using 
the hierarchical structure of the headings, narrowing down from main headings to subheadings if a user 
wants to make use of all the headings from an oversized ToC. As this procedure is a preliminary step 
for making a reference for answer generation, the number of selected headings can be set in the prompt 
in advance depending on the budget and the context window size of the generative model for answer 
generation. It is recommended that the model produce a response in a structured format during heading 
selection to optimize efficiency for the following retrieval process as well as token usage.  
 
3) Retrieval-augmented generation 
Sections of the document under the selected headings are retrieved and concatenated as a reference 
for answer generation. Again, it should be noted that the size of a reference must be smaller than the 
context window size of the generative model for answer generation. Therefore, the size of a reference 
has to be reduced by truncation or summarization when overly large. After a reference is prepared, a 
prompt including both the query and the reference is forwarded into a generative model. In response, 
the model consults the augmentations to generate a response to the query. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What is the purpose of the heading selection step in the process?,Answer: The purpose of the heading selection step is to select the headings most pertinent to the query or those that help the most to find information concerning the query.
11,"6 
3. Experiments 
1) Comparative exploration of LLM-based vector embeddings in the KM and CM domains. 
This experiment aimed to identify and exemplify the relative representational defects of LLM-based 
vector embedding in niche domains compared to other well-established domains. To explain this point, 
we conducted a comparative analysis with vector embeddings from documents in KM and CM domains.  
For this experiment, we selected 10 documents each from KM and CM domains, specifically 
regarding their physiological contents. ‘Eastern Medicine Physiology'(22) served as the document pool 
for KM. This book, compiled in Korean, has been revised by professors from every Korean Medicine 
college in South Korea and is used as the principal textbook in the physiology curriculum. On the other 
hand, ‘Physiology'(23) was chosen for the CM domain. To investigate the impact of language on 
representational differences in embeddings, we collected documents with the exactly identical contents 
from both the English version and the Korean-translated version of ‘Physiology'. The titles of the 
selected documents from each domain are listed in Appendix Table 1. We extracted the embedding 
vectors for a total of 30 documents – 10 each from KM physiology, CM physiology in Korean (CM_KR), 
and CM physiology in English (CM_EN) – using E5-mistral-7b-instruct(24), Voyage AI’s voyage-02, 
and OpenAI's text-embedding-ada-002 models to figure out LLMs' representations of KM and CM 
knowledge. 
Our analysis focused on identifying patterns of the KM and the CM domain embeddings with three 
key document similarity metrics: human-evaluated document relatedness, embedding correlation 
coefficients, and token overlap coefficients. We assessed whether the correlation coefficients between 
embedding pairs closely align with the human-evaluated ground truth or merely follow the surface-
level similarity (token overlap) by conducting the correlation analyses across these metrics. It allows us 
to understand the depth of embedding representations and their correlation with human-perceived 
document pairwise relevance. 
For this, the Pearson correlation coefficients(25) were calculated for every embedding vector pair, 
covering 45 pairs in each of the three categories (KM, CM_KR, CM_EN). To assess explicit similarity 
in a document pair, we computed the overlap coefficient(26) for tokens in KM, CM_KR, CM_EN 
documents. The token overlap coefficient was calculated as:  
 
𝑇𝑜𝑘𝑒𝑛 𝑜𝑣𝑒𝑟𝑙𝑎𝑝 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡஺,஻= 
| A ∩B |
min(|𝐴|, |𝐵|) 
| A ∩B |: The count of token co-occurrence between documents A and B. 
min(|𝐴|, |𝐵|): The minimum token count in either document A or B. 
 
Token overlap coefficients were calculated three times with different tokenizers corresponding to the 
embedding models: E5-mistral-7b-instruct(24), Voyage AI’s voyage-02, and OpenAI's text-embedding-
ada-002. Repeated appearances of a single token in a document were counted and considered separately. 
To determine the ground truth of document pair correlations within each domain, two KM doctors 
with national licenses evaluated the relatedness between each pair of the KM and CM documents. A 
binary scoring system was adopted: a score of 1 indicated that a pair was interrelated, and 0 for unrelated 
",Question: What was the name of the book used as the principal textbook in the physiology curriculum in South Korea?,Answer: Eastern Medicine Physiology
12,"7 
documents. The human-evaluated document relatedness scores were then obtained by averaging the 
two doctors' scores in KM and CM documents, respectively.  
The correlation analyses were conducted between human-evaluated document relatedness scores and 
embedding correlation coefficients, and between embedding correlation coefficients and token overlap 
coefficients with Scipy(27) in Python 3.11. Bonferroni correction(28) was applied for p-values due to 
the multiple comparisons. 
 
2) Performance comparison of Prompt-RAG and existing models 
(1) Chatbot Settings 
For the evaluation, we developed a domain-specific, prompt-RAG-based chatbot for the book 
'Introduction to Current Korean Medicine’(29). The chatbot employed GPT architectures: GPT-4-0613 
for the heading selection and GPT-3.5-turbo-16k-0613 for the answer generation. 
The original ToC of the book had already been defined by the authors. Subheadings were added to it, 
aligning with the book’s actual sections. The expanded table of contents exceeded the context window 
size for heading selection, so some headings were removed to handle this issue. The body of the book 
was then segmented according to the modified headings for the subsequent retrieval. 
We passed a model based on GPT-4 a prompt containing both the revised ToC and a query, asking 
the model to identify five pertinent headings from the ToC. At the same time, it was instructed to avoid 
selecting a heading if the query was about greetings or casual talks. The prompt for heading selection 
is shown in Table 1. 
 
Table 1. The prompt for heading selection 
 
“Current context: 
{history}a 
 
Question: {question}a 
 
Table of Contents: 
{index}a 
 
 
 
Each heading (or line) in the table of contents above represents a fraction in a document. 
Select the five headings that help the best to find out the information for the question. 
List the headings in the order of importance and in the format of 
'1. --- 
2. --- 
--- 
5. ---'. 
Don't say anything other than the format. 
If the question is about greetings or casual talks, just say 'Disregard the reference.'.” 
 
aThese represent the placeholders for conversational buffer memory, the user’s query, and the table of 
","contents, respectively.",Answer: KM and CM
13,"8 
contents, respectively, from top to bottom. 
 
Upon selecting the headings, the corresponding book sections were fetched and concatenated. In turn, 
this was provided as a reference in a prompt along with the query to another generative model based on 
GPT-3.5-turbo-16k. This model was required to generate an answer with the prompt which also 
contained a directive to refrain from saying nonsense when no relevant context was found in the 
reference thereby aiming to minimize hallucination. In cases where the selected headings are absent 
due to the query being a greeting or casual conversation, an alternative prompt without a reference 
section is passed to a GPT-3.5-turbo-based model, in order to reduce token usage and save on expenses. 
The prompts for answer generation are depicted in Table 2. 
 
Table 2. The prompts for answer generation 
Prompt 1: Answer generation with selected headings 
 
“You are a chatbot based on a book called '현대한의학개론'. 
Here is a record of previous conversation for your smooth chats.: 
{history}a 
 
 
 
 
 
Reference: 
{context}a 
 
 
 
 
 
Question: {question}a 
 
 
 
 
 
Use the reference to answer the question. 
The reference above is only fractions of '현대한의학개론'. 
Be informative, gentle, and formal. 
If you can't answer the question with the reference, just say like 'I couldn't find the right answer this 
","time.' Don't say nonsense.” 
Prompt 2: Answer generation without selected headings 
“You are a chatbot based on a book called '현대한의학개론'. 
Here is a record of previous conversation for your smooth chats.: 
{history}a 
 
 
 
 
 
Question: {question}a 
 
 
 
 
 
Use your knowledge to answer the question. 
Be informative, gentle, and formal. 
If you can't answer the question, just say like 'I couldn't find the right answer this time.' Don't say nonsense.”",Answer: Don't say nonsense.
14,"9 
time'. 
Answer in Korean:” 
 
Prompt 2: Answer generation without selected headings for casual queries 
 
“You are a chatbot based on a book called '현대한의학개론'. 
Here is a record of previous conversation for your smooth chats.: 
{history}a 
 
 
 
Question: {question}a 
 
 
 
Answer the question. 
Be informative, gentle, and formal. 
Answer in Korean:” 
 
aThese denote the placeholders for conversational buffer memory, the reference based on the selected 
heading, and the user’s query, respectively, from top to bottom. 
 
Conversation buffer memory was incorporated in the prompts for both heading selection and answer 
generation, within each context window limit. We employed Langchain(30) for the processes above. 
 
(2) Baselines 
① ChatGPT 
For the first baseline to compare the performance of our model with, we utilized ChatGPT without 
any retrieval-augmentation process. ChatGPT is based on a diverse, large-scale corpus, equipped with 
an immense range of global knowledge.(31) Therefore, we evaluated our model's proficiency in 
generating answers specific to the domain of KM, in contrast with general knowledge of ChatGPT. This 
baseline included employing both GPT-3.5 and GPT-4 models of ChatGPT (chatGPT-3.5, ChatGPT-4, 
respectively).  
 
② Chunk retrievals 
As our second baseline, we adopted vector embedding-based chunk retrieval. The text of the book 
was divided into chunks of size 50 and 100, respectively, using Tiktoken(32). Subsequently, each chunk 
was vectorized through OpenAI’s text-embedding-ada-002. Vectors that most closely matched the query 
",Question: What is the size of chunks used in the baseline of vector embedding-based chunk retrieval?,Answer: 50 and 100
15,"10 
embedding by maximal marginal relevance(33) were retrieved. The number of retrieved vectors was set 
to 300 for chunk size 50 (C50-V300) and 150 for chunk size 100 (C100-V150), respectively, to make 
the most of the context window of GPT-3.5-turbo-16k for answer generation. 
 
(3) Tasks and performance evaluation metrics 
To evaluate the performance of our domain-specific, prompt-RAG-based chatbot and the other 
baseline models, we composed a series of 30 questions related to KM. The models were to generate 
answers to those questions in order. 
Each question was categorized into one of the three types to examine the models’ capabilities in direct 
retrieval, comprehensive understanding, and functional robustness. The questions among the three types 
followed a ratio of 4:4:2. For the ChatGPT baselines, which do not utilize retrieval augmentation, 
questions specifically inquiring about the author’s perspective were appropriately adjusted. Further 
details on the questions and their types are provided in Appendix Table 2.  
Human evaluation was performed for the generated answers by three KM doctors. The evaluators 
assessed the models’ answers in terms of three criteria: relevance, readability, and informativeness.(34, 
35) Relevance measured how well the answer directly addressed the central topic of the question. 
Readability evaluated the naturalness and fluency of the answer. Informativeness assessed the depth 
and significance of the answer's content. Each question was scored in terms of every criterion with 
either 0, 1, or 2 points. In the evaluation process, each response started with a base score of 2 for each 
criterion, and evaluators were instructed to deduct points based on the presence of specific flaws. 
Descriptions for the criteria and the scoring system are provided in Table 3. The Response time taken 
to generate each answer was also measured for the comparison of our model and chunk retrieval models 
 
Table 3. Evaluation criteria for answers. 
Criterion 
Point scale 
Description 
Deduction 
Relevance 
0, 1, 2 
Assesses direct connection with the 
central topic of the question. High 
relevance achievable even with low 
readability or meaningless content. 
Irrelevance 
to 
the 
question. 
Readability 
0, 1, 2 
Evaluates 
the 
naturalness 
and 
fluency 
of 
an 
answer. 
High 
readability achievable even with 
irrelevant or meaningless content. 
Grammatical errors or 
incoherence. 
Informativeness 
0, 1, 2 
Assesses the depth and significance 
of the answer's content. High 
informativeness achievable even 
with low readability or irrelevance.  
Superficial 
or 
meaningless content 
including 
hallucination. 
Scoring guide 
0 points 
Criterion 
severely 
damaged, 
making 
the 
answer 
unacceptable. 
",Question: What is the number of retrieved vectors for chunk size 100?,Answer: 150
16,"11 
1 point 
Some flaws present in criterion, answer still usable. 
2 points 
Good overall criterion quality. 
 
(4) Statistical analysis  
To evaluate the statistical significance of our model’s scores in relation to those of the others, we 
performed t-tests and Mann-Whitney U tests. The t-tests compared the scores across the criteria of 
relevance, readability, and informativeness, while Mann-Whitney U tests were applied to the scores 
categorized by question types. P-values were adjusted using Bonferroni correction(28) to account for 
the multiple comparisons. All statistical analyses were conducted with the Statsmodels(36) package in 
Python 3.11. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What Python version was used for conducting statistical analyses?,Answer: Python 3.11.
17,"12 
4. Results 
1) Comparative analysis of LLM-based vector embeddings in KM and CM 
(1) Comparison of KM and CM document pairs by correlation metrics 
Human-evaluated document relatedness scores, embedding correlation coefficients, and token 
overlap coefficients were calculated for KM and CM document pairs using three different embedding 
models. To compare the overall pattern of these metrics across the domains and the models, they are 
visually presented in Figure 2.  
 
 
Figure 2. Comparative analysis of human-evaluated document relatedness, embedding correlation 
coefficients, and token overlap coefficients in KM, CM_KR, and CM_EN. (A) shows clustermaps of 
human-evaluated document relatedness scores for KM and CM, where each cell represents the 
perceived relatedness between document pairs as judged by human evaluators. (B) illustrates the 
embedding correlation coefficients across the different domains and models. (C) depicts the token 
overlap coefficients, which measure the extent of shared tokens between document pairs. The 
hierarchical clustering was conducted based on squared Euclidean distance, with embedding correlation 
coefficients and token overlap coefficients sequentially arranged in an identical order to this cluster 
structure. 
Abbreviations: KM, Korean medicine; CM, Conventional medicine; CM_KR, CM physiology in 
Korean; CM_EN, CM physiology in English; D, Document. 
 
(2) Correlation analyses between metrics in KM and CM documents 
",Question: What are the three different embedding models used in the comparative analysis of LLM-based vector embeddings in KM and CM?,Answer: The three different embedding models are not specified in the context.
18,"13 
To analyze the correlations between human-evaluated document relatedness scores and embedding 
correlation coefficients, and between embedding correlation coefficients and token overlap coefficients, 
Pearson or Spearman correlation coefficients were calculated for each metric pair. Figure 3 provides 
scatter plots for showing the relationship between the metrics in KM, CM_KR, and CM_EN. 
 
 
Figure 3. Correlation of document embedding correlation coefficients with human-evaluated document 
relatedness, and token overlap coefficients in KM, CM_KR, and CM_EN. The figure displays 
regression plots for pairwise correlations between the metrics within KM, CM_KR, and CM_EN 
documents. (A) displays scatter plots with fitted regression lines showing the relationship between 
human-evaluated document relatedness (x-axis) and the embedding correlation coefficient (y-axis) for 
each of the three language models. Each point represents a document pair. (B) shows the relationship 
between the embedding correlation coefficients (x-axis) and token overlap coefficients (y-axis). The 
colors correspond to the different document sets: KM, CM_KR, and CM_EN. The regression lines and 
correlation coefficients represent the strength and direction of the relationships. The symbols 'r' and 'ρ' 
indicate the Pearson and Spearman correlation coefficients, respectively. 
Abbreviations: KM, Korean medicine; CM, Conventional medicine; CM_KR, CM physiology in 
Korean; CM_EN, CM physiology in English. 
 
For the first metric pair, Spearman's correlation coefficients were calculated between human-
evaluated document relatedness scores and the embedding correlation coefficients. Across all evaluated 
",Question: What type of correlation coefficients were calculated between human-evaluated document relatedness scores and the embedding correlation coefficients?,Answer: Pearson or Spearman correlation coefficients.
19,"14 
models—E5-mistral-7b-instruct, voyage-02, and text-embedding-ada-002—the correlation coefficients 
for CM were consistently higher than those for KM, indicating a stronger alignment with human 
judgment in the context of CM. Within CM, the coefficients for CM_EN were higher than those for 
CM_KR. Specifically, for the E5-mistral-7b-instruct model, the Spearman's correlation coefficient was 
0.503 for KM, while it increased for CM_KR to 0.691 and was highest for CM_EN at 0.725. Similarly, 
voyage-02 presented a negative correlation for KM (-0.016), but it showed positive correlations of 0.376 
for CM_KR and a notably stronger 0.670 for CM_EN. The text-embedding-ada-002 model 
demonstrated a coefficient of 0.167 for KM, with higher values of 0.563 for CM_KR and 0.625 for 
CM_EN. Notably, CM_EN exhibited statistically significant positive correlations across all models 
(0.725, 0.670, and 0.625, respectively), indicating a robust positive correlation in the context of CM 
and English compared to KM and Korean. In contrast, the correlations in KM were either weak or 
slightly negative (-0.016 and 0.167), with the exception of the E5-mistral-7b-instruct model, which 
yielded a moderate 0.503. 
Secondly, the Pearson correlation coefficients between the embedding correlation coefficients and 
token overlap coefficients showed varied patterns. In CM_EN, the E5-mistral-7b-instruct model had a 
Pearson's correlation coefficient of 0.438, and voyage-02 had a coefficient of 0.518, both indicating 
moderate positive correlations. However, these correlations, including the one for text-embedding-ada-
002, were all lower than those observed for human-evaluated document relatedness. For KM, significant 
positive correlations were observed in voyage-02 and text-embedding-ada-002, with coefficients of 
0.429 and 0.501, respectively. These values are in stark contrast to the previously discussed Spearman's 
correlations between human-evaluated document relatedness scores and embedding correlation 
coefficients for KM (-0.016 and 0.167, respectively). This suggests that these models may prioritize 
token-level features of documents over their human-perceived meanings when generating vector 
representations. These findings are summarized in Table 4. 
 
Table 4. Correlation analysis between document similarity metrics in KM, CM_KR, and CM_EN. 
Embedding model 
Human-evaluated document 
relatedness  
– 
Embedding correlation coefficient 
(Spearman's ρ) 
Embedding correlation coefficient  
–  
Token overlap coefficient 
(Pearson's r) 
KM 
CM_KR 
CM_EN 
KM 
CM_KR 
CM_EN 
E5-mistral-7b-
instruct 
0.503b 
0.691c 
0.725c 
0.304 
0.365 
0.438a 
voyage-02 
-0.016 
0.376 
0.670c 
0.429a 
0.177 
0.518b 
text-embedding-
ada-002 
0.167 
0.563c 
0.625c 
0.501b 
0.343 
0.335 
Superscripts indicate statistical significance in correlation analysis. 
ap < 0.05, bp < 0.005, cp < 0.001 
",Question: What is the Spearman's correlation coefficient for the E5-mistral-7b-instruct model in CM_KR?,Answer: 0.691
20,"15 
Abbreviations: KM, Korean medicine; CM, CM_KR, CM physiology in Korean; CM_EN, CM 
physiology in English.  
 
Overall, embedding correlations in CM_EN consistently demonstrates a higher alignment with 
human-evaluated document relatedness compared to KM and CM_KR. On the contrary, the embedding 
representation of KM tends to be determined by the explicit lexical similarity from token overlaps. 
These findings illustrate insufficiencies of LLM-based vector embeddings in capturing human-
perceived conceptual meanings in niche domains, suggesting that their application in conventional RAG 
systems may result in suboptimal performances. 
 
2) Performance comparison of Prompt-RAG and existing models 
(1) Main results 
Table 5 presents the mean scores for relevance, readability, and informativeness, along with the 
response times for the five models' answers. 
 
Table 5. Comparative evaluation of model performance in the Korean medicine domain 
Model 
Relevance 
(Mean score) 
Readability 
(Mean score) 
Informativeness 
(Mean score) 
Response time 
(Mean seconds) 
ChatGPT-3.5 
1.711 
1.900 
0.667d 
- 
ChatGPT-4 
1.833 
1.922 
1.033b 
- 
C50-V300 
1.733 
1.733a 
0.644d 
6.454d 
C100-V150 
1.8 
1.722 
0.833d 
7.033c 
Prompt-RAG 
1.956 
1.900 
1.589 
24.840 
Superscripts indicate statistical significance in comparison to the Prompt-RAG model. 
ap < 0.05, bp < 0.01, cp < 0.005, dp < 0.001 
 
Firstly, we compared the performance of our prompt-RAG model with that of ChatGPT to examine 
its proficiency in the KM domain. Prompt-RAG achieved mean scores of 1.956 for relevance and 1.589 
for informativeness, respectively, surpassing ChatGPT-3.5 (1.711 for relevance, 0.667 for 
informativeness) and ChatGPT-4 (1.833 for relevance, 1.033 for informativeness). It is noteworthy that 
our model's informativeness scores were significantly higher, being more than double those of 
ChatGPT-3.5 and exceeding those of ChatGPT-4 by over 1.5 times. In terms of readability, our model 
scored 1.900, which was about equal to ChatGPT-3.5's score (1.900) and slightly lower than ChatGPT-
4’s (1.922). Overall, our model demonstrated its outperformance against ChatGPT baselines, especially 
GPT-3.5, in generating domain-specific answers related to KM.  
Further, we explored whether the prompt-RAG approach could produce better answers than the 
conventional chunk retrieval method. For all the criteria, our model scored higher than C50-V300 and 
C100-V150. The readability scores of our model were significantly higher compared to C100-V150, 
and especially for informativeness, our model obtained statistically significant scores, approximately 
",Question: What is the mean score of relevance for the Prompt-RAG model in the Korean medicine domain?,Answer: 1.956
21,"16 
2.5 times that of C50-V300 and around 1.9 times that of C100-V150. However, our mode was 
significantly slower in terms of average response time, taking an additional 18.356 seconds compared 
to C50-V300 and 17.806 seconds more than C100-V150. These results find that the Prompt-RAG model 
excelled in answer quality, while the latency in answer generation was larger than the chunk retrieval 
method. 
 
(2) Comparison by types of questions 
To assess the overall quality and applicability of our prompt-RAG, we conducted a comparative 
analysis of its performance against the other models across different question types: direct retrieval, 
comprehensive understanding, and functional robustness. The summed scores for relevance, readability, 
and informativeness by the three evaluators were averaged for each question and each question type, 
respectively. The results by the question types are illustrated in Figure 4. 
 
 
Figure 4. Model performance comparison across different question types. (A) Direct retrieval questions. 
(B) Comprehensive understanding questions. (C) Functional robustness questions. The asterisks 
",Question: How much slower was the Prompt-RAG model in terms of average response time compared to C50-V300?,Answer: 18.356 seconds.
22,"17 
represent statistical significance in the differences in scores between the prompt-RAG model and the 
others: *p < 0.05, **p < 0.01, ***p < 0.005 
 
Our model reached an average score of 5.5 for direct retrieval, 5.389 for comprehensive 
understanding, and 5.444 for functional robustness out of 6, outdoing all other models in every question 
type. Notably, the scores for direct retrieval were significantly higher compared to those of all the other 
models, and the scores for comprehensive understanding were also statistically significant in 
comparison to the chunk retrieval models and ChatGPT-3.5. This suggests not only our model's 
advanced capability for retrieval but also its comprehension-based answering performance, which is 
comparable to ChatGPT-4. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What is the average score out of 6 that the prompt-RAG model reached for comprehensive understanding?,Answer: 5.389
23,"18 
5. Discussion 
In this study, our exploration of LLM-based vector embeddings revealed marked limitations within 
the KM domain. The analysis showed that vector embeddings are heavily influenced by languages and 
token overlaps, which are not always compatible with human reasoning, potentially leading to 
suboptimal performance when used in RAG methods. To address these shortcomings, we introduced 
Prompt-RAG, a natural language prompt-based RAG methodology, providing a strategic shift from 
conventional RAGs operated with vector embeddings. This stemmed from the recognition of the 
limitations inherent in LLMs, utilizing the linguistic capabilities of LLM and addressing its constraints 
at the same time. As a result, our QA chatbot equipped with Prompt-RAG exhibited promising outcomes 
in terms of relevance, readability, and informativeness in the KM domain. Moreover, it coped with a 
variety of types of KM-related questions as well, proving its practical stability. 
  The potential of Prompt-RAG is substantial. Importantly, our model is not confined only to the KM 
domain but can be applied to other marginal domains that require RAG. GPT is recognized for its 
emergent properties, potentially helping deal with highly abstract, contextual, or previously unseen 
expressions.(37-39) It would facilitate high-quality retrieval with a ToC that contains the comprehensive 
and essential context of documents, leading to desirable responses across various domains. Its 
applicability and efficiency can expand vastly, together with natural language processing techniques 
developing and improving. As the cognitive abilities of LLMs continue to advance, we look forward to 
Prompt-RAG becoming an even more powerful tool with full reliance on the capabilities of an LLM 
itself.  
  Its wide-ranging adaptability derived from the ability to understand and process unacquainted or 
uncertain concepts and terminologies would raise some challenges for conventional vector embedding-
based RAG. For example, a short query has been known to undermine the performance vector 
embedding-based informational retrieval due to the lack of contexts, even though it is the major form 
of a search query on the internet.(40-42) The adoption of the natural language prompts through GPT 
allows for a nuanced understanding of queries(43) and thus results in a more detailed, accurate, and 
relevant retrieval. In addition, Prompt-RAG can be much more efficient when it comes to model updates, 
saving on the expense and time for the renewal of document embeddings, especially with larger 
documents. These properties would be highlighted in dynamic environments in terms of data with its 
ability to be applied without the need for repetitive retraining or embedding. 
However, we acknowledge that Prompt-RAG has certain limitations. Firstly, the requirement for a 
ToC might sometimes pose an obstacle, depending on the type or structure of the document. Secondly, 
the recurring latency and expenses associated with running a generative model or making Application 
Programming Interface (API) calls for heading selection do result in longer response times and higher 
costs. However, these issues are expected to naturally improve as the generative performance of LLMs 
continues to develop and model pricing plans become more economical, as has been the trend. 
Explorations and developments in model compression and light-weight artificial intelligence 
technologies for resource-constrained devices have been recently encouraged by the popularization of 
individual edge devices.(44-46) This trend seems to be extending to natural language processing 
domains as well(47), which would help solve the latency issue of our model. The rapid advancements 
","Question: What is the name of the QA chatbot that exhibited promising outcomes in terms of relevance, readability, and informativeness in the KM domain?",Answer: The name of the QA chatbot is not specified in the context.
24,"19 
in generative models suggest that the limitations of our model will become increasingly less problematic 
in the foreseeable future, likely sooner than anticipated. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What is the suggested timeline for the limitations of the model becoming less problematic?,Answer: Sooner than anticipated.
25,"20 
6. Conclusion 
We suggest Prompt-RAG as an alternative to the conventional vector embedding RAG methods, 
addressing the limitations of LLM-based vector embeddings in niche domains where inconsistencies 
with human reasoning can lead to suboptimal performance. With its derived QA chatbot, Prompt-RAG 
has achieved notable outcomes as demonstrated by our study on KM, showing its potential as a versatile 
and effective tool in line with the rapidly evolving LLM field. While there is room for improvement, its 
practical benefits are expected to grow through internal and external development. Providing a new 
paradigm in RAG, it contributes to the advancement of information retrieval in specific domains with 
remarkable ease. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",Question: What is the name of the alternative to conventional vector embedding RAG methods suggested in the context?,Answer: Prompt-RAG.
26,"21 
7. Reference 
1. 
Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval-augmented 
generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems. 
2020;33:9459-74. 
2. 
Shuster K, Poff S, Chen M, Kiela D, Weston J. Retrieval augmentation reduces hallucination 
in conversation. arXiv preprint arXiv:210407567. 2021. 
3. 
Yoran O, Wolfson T, Ram O, Berant J. Making Retrieval-Augmented Language Models 
Robust to Irrelevant Context. arXiv preprint arXiv:231001558. 2023. 
4. 
Naveed H, Khan AU, Qiu S, Saqib M, Anwar S, Usman M, et al. A comprehensive overview 
of large language models. arXiv preprint arXiv:230706435. 2023. 
5. 
Izacard G, Lewis P, Lomeli M, Hosseini L, Petroni F, Schick T, et al. Few-shot learning with 
retrieval augmented language models. arXiv preprint arXiv:220803299. 2022. 
6. 
Zhao R, Chen H, Wang W, Jiao F, Do XL, Qin C, et al. Retrieving multimodal information for 
augmented generation: A survey. arXiv preprint arXiv:230310868. 2023. 
7. 
Li H, Su Y, Cai D, Wang Y, Liu L. A survey on retrieval-augmented text generation. arXiv 
preprint arXiv:220201110. 2022. 
8. 
Gao Y, Xiong Y, Gao X, Jia K, Pan J, Bi Y, et al. Retrieval-augmented generation for large 
language models: A survey. arXiv preprint arXiv:231210997. 2023. 
9. 
Yunianto I, Permanasari AE, Widyawan W, editors. Domain-Specific Contextualized 
Embedding: A Systematic Literature Review. 2020 12th International Conference on Information 
Technology and Electrical Engineering (ICITEE); 2020 6-8 Oct. 2020. 
10. 
Yang G, Shi J, Wang Z, Liu X, Wang G. TCM-GPT: Efficient Pre-training of Large Language 
Models for Domain Adaptation in Traditional Chinese Medicine. arXiv preprint arXiv:231101786. 2023. 
11. 
Marreddy M, Oota SR, Vakada LS, Chinni VC, Mamidi R. Am I a Resource-Poor Language? 
Data Sets, Embeddings, Models and Analysis for four different NLP Tasks in Telugu Language. ACM 
Trans Asian Low-Resour Lang Inf Process. 2022;22(1):Article 18. 
12. 
Hossain MR, Hoque MM, Siddique N. Leveraging the meta-embedding for text classification 
in a resource-constrained language. Engineering Applications of Artificial Intelligence. 
2023;124:106586. 
13. 
Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al. Lora: Low-rank adaptation of large 
language models. arXiv preprint arXiv:210609685. 2021. 
14. 
Fu Z, Yang H, So AM-C, Lam W, Bing L, Collier N. On the Effectiveness of Parameter-
Efficient Fine-Tuning. Proceedings of the AAAI Conference on Artificial Intelligence. 
2023;37(11):12799-807. 
15. 
Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, et al. Parameter-efficient fine-tuning of large-
scale pre-trained language models. Nature Machine Intelligence. 2023;5(3):220-35. 
16. 
Cha W-S, Oh J-H, Park H-J, Ahn S-W, Hong S-Y, Kim N-I. Historical difference between 
traditional 
Korean medicine and traditional Chinese 
medicine. 
Neurological 
Research. 
2007;29(sup1):5-9. 
17. 
Yin CS, Ko S-G. Introduction to the History and Current Status of Evidence-Based Korean 
Medicine: A Unique Integrated System of Allopathic and Holistic Medicine. Evidence-Based 
Complementary and Alternative Medicine. 2014;2014:740515. 
18. 
Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of gpt-4 on medical 
challenge problems. arXiv preprint arXiv:230313375. 2023. 
19. 
Brin D, Sorin V, Vaid A, Soroush A, Glicksberg BS, Charney AW, et al. Comparing ChatGPT 
and GPT-4 performance in USMLE soft skill assessments. Scientific Reports. 2023;13(1):16492. 
20. 
Yang Z, Yao Z, Tasmin M, Vashisht P, Jang WS, Wang B, et al. Performance of Multimodal 
GPT-4V on USMLE with Image: Potential for Imaging Diagnostic Support with Explanations. medRxiv. 
2023:2023.10.26.23297629. 
21. 
Jang D, Yun T-R, Lee C-Y, Kwon Y-K, Kim C-E. GPT-4 can pass the Korean National 
Licensing Examination for Korean Medicine Doctors. PLOS Digital Health. 2023;2(12):e0000416. 
","What year was the paper ""Retrieval-augmented generation for knowledge-intensive NLP tasks"" published?",Answer: 2020
27,"22 
22. 
전국한의과대학생리학교수. 개정판 동의생리학: 집문당; 2016. 
23. 
Costanzo LS. Physiology. Sixth edition ed. Philadelphia, PA: Elsevier Philadelphia, PA; 2018. 
24. 
Wang L, Yang N, Huang X, Yang L, Majumder R, Wei F. Improving text embeddings with 
large language models. arXiv preprint arXiv:240100368. 2023. 
25. 
Pearson K. Note on Regression and Inheritance in the Case of Two Parents. Proceedings of the 
Royal Society of London. 1895;58:240-2. 
26. 
M K V, K K. A Survey on Similarity Measures in Text Mining. Machine Learning and 
Applications: An International Journal. 2016;3:19-28. 
27. 
Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, et al. SciPy 1.0: 
fundamental algorithms for scientific computing in Python. Nature Methods. 2020;17(3):261-72. 
28. 
Haynes W. Bonferroni Correction. In: Dubitzky W, Wolkenhauer O, Cho K-H, Yokota H, 
editors. Encyclopedia of Systems Biology. New York, NY: Springer New York; 2013. p. 154-. 
29. 
이충열, 박왕용, 정기용, 엄두영, 김창업. 현대한의학개론: Introduction to Current 
Korean Medicine: 군자출판사; 2023. 
30. 
Chase H. LangChain: GitHub repository; 2022 [Available from: https://github.com/langchain-
ai/langchain. 
31. 
Haleem A, Javaid M, Singh RP. An era of ChatGPT as a significant futuristic support tool: A 
study on features, abilities, and challenges. BenchCouncil Transactions on Benchmarks, Standards and 
Evaluations. 2022;2(4):100089. 
32. 
OpenAI, 
Jain 
S. 
tiktoken: 
GitHub 
repository; 
2022 
[Available 
from: 
https://github.com/openai/tiktoken. 
33. 
Carbonell J, Goldstein J. The use of MMR, diversity-based reranking for reordering documents 
and producing summaries.  Proceedings of the 21st annual international ACM SIGIR conference on 
Research and development in information retrieval; Melbourne, Australia: Association for Computing 
Machinery; 1998. p. 335–6. 
34. 
Saad-Falcon J, Barrow J, Siu A, Nenkova A, Rossi RA, Dernoncourt F. PDFTriage: Question 
Answering over Long, Structured Documents. arXiv preprint arXiv:230908872. 2023. 
35. 
Soong D, Sridhar S, Si H, Wagner J-S, Sá ACC, Yu CY, et al. Improving accuracy of GPT-3/4 
results on biomedical data using a retrieval-augmented language model. arXiv preprint 
arXiv:230517116. 2023. 
36. 
Seabold S, Perktold J. Statsmodels: Econometric and Statistical Modeling with Python. 
Proceedings of the 9th Python in Science Conference. 2010;2010. 
37. 
Malkin N, Lanka S, Goel P, Rao S, Jojic N, editors. GPT Perdetry Test: Generating new 
meanings for new words. Proceedings of the 2021 Conference of the North American Chapter of the 
Association for Computational Linguistics: Human Language Technologies; 2021 June; Online: 
Association for Computational Linguistics. 
38. 
Wei J, Tay Y, Bommasani R, Raffel C, Zoph B, Borgeaud S, et al. Emergent abilities of large 
language models. arXiv preprint arXiv:220607682. 2022. 
39. 
Webb T, Holyoak KJ, Lu H. Emergent analogical reasoning in large language models. Nature 
Human Behaviour. 2023;7(9):1526-41. 
40. 
Azad HK, Deepak A, Chakraborty C, Abhishek K. Improving query expansion using pseudo-
relevant web knowledge for information retrieval. Pattern Recognition Letters. 2022;158:148-56. 
41. 
Celard P, Iglesias EL, Sorribes-Fdez JM, Romero R, Vieira AS, Borrajo L, editors. Improving 
Short Query Representation in LDA Based Information Retrieval Systems2022; Cham: Springer 
International Publishing. 
42. 
Azad HK, Deepak A. Query expansion techniques for information retrieval: A survey. 
Information Processing & Management. 2019;56(5):1698-735. 
43. 
Cheng S-W, Chang C-W, Chang W-J, Wang H-W, Liang C-S, Kishimoto T, et al. The now and 
future of ChatGPT and GPT in psychiatry. Psychiatry and Clinical Neurosciences. 2023;77(11):592-6. 
44. 
Wang CH, Huang KY, Yao Y, Chen JC, Shuai HH, Cheng WH. Lightweight Deep Learning: 
",Question: What is the title of the book written by Pearson K in 1895?,Answer: Note on Regression and Inheritance in the Case of Two Parents.
28,"23 
An Overview. IEEE Consumer Electronics Magazine. 2022:1-12. 
45. 
Kim K, Jang S-J, Park J, Lee E, Lee S-S. Lightweight and Energy-Efficient Deep Learning 
Accelerator for Real-Time Object Detection on Edge Devices. Sensors. 2023;23(3):1185. 
46. 
Mehta S, Rastegari M. Mobilevit: light-weight, general-purpose, and mobile-friendly vision 
transformer. arXiv preprint arXiv:211002178. 2021. 
47. 
Xu C, McAuley J, editors. A survey on model compression and acceleration for pretrained 
language models. Proceedings of the AAAI Conference on Artificial Intelligence; 2023. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
",What is the volume number of the Sensors journal article?,Answer: 23
29,"24 
8. Appendix 
Table 1. Documents for embedding comparison. 
 
Korean Medicine 
(KM) 
Conventional Medicine 
(CM) 
Document 1 
Yin-Yang 
Perception 
of 
Life 
Phenomena  
Na+-K+ ATPase (Na+-K+ Pump)  
Document 2 
Six Qi as Analytical Concepts in Life 
Phenomena: External and Internal Six 
Qi  
Types of Synapses  
Document 3 
The Action of Qi  
Organization of the nervous system  
Document 4 
Physiological Functions of Body 
Fluids  
Circuitry of the cardiovascular system  
Document 5 
Analogous Functional System  
Erythropoietin 
Document 6 
The Concept of Extraordinary Fu 
Organs  
Regulation of Renal Blood Flow  
Document 7 
Six Meridians  
Acid-Base Disorders  
Document 8 
Seven Emotions and Physiological 
Changes  
Satiety  
Document 9 
The Concept of Heavenly Water and 
Menstruation 
Negative 
Feedback 
Acid-Base 
Disorders  
Document 10 
Sleep and Health Preservation  
Pulsatile Secretion of GnRH, FSH, and 
LH  
The document titles in the Korean Medicine domain are originally in Korean and have been translated 
for this table. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
","Question: What is the concept in Conventional Medicine that is analogous to the concept of ""The Concept of Extraordinary Fu Organs"" in Korean Medicine?",Answer: Regulation of Renal Blood Flow.
30,"25 
Table 2. Questions and their types for model evaluation. 
1. Direct retrieval (40%): 12 Questions 
1) Factual Questions: (1) – (9) 
2) Comparative Questions: (10) – (12) 
(1) What is the modernization of Korean medicine (mentioned by the author)ª? 
(2) Can you tell me about Earth from the five elements? 
(3) Explain what Congenital Foundation is. 
(4) Tell me the constitutional medicine patterns of Taiyin personality. 
(5) What are the detailed classifications of sub-health? 
(6) What are the new drugs developed based on domestic herbal medicine in Korea? 
(7) When is the implementation period for the Fourth Comprehensive Plan for the Promotion 
and Development of Korean Medicine? 
(8) What are the current subjects of the Korean National Licensing Examination for Korean 
Medicine Doctors? 
(9) When was the Law of the People's Republic of China on Traditional Chinese Medicine 
implemented? 
(10) What are the conceptual differences between Blood and Body Fluid? 
(11) Compare the classification of the herbs and the formulas. 
(12) Can you explain the medical insurance coverage items for Korea, China, and Japan? 
2. Comprehensive understanding (40%): 12 Questions 
1) Interpretative Questions: (13) – (15)  
2) Inference Questions: (16) – (18) 
3) Application Questions: (19) – (21) 
4) Open-ended Questions: (22) – (24) 
(13) If you should summarize the meanings of the 'scientification of Korean medicine' into two 
main points, what would they be? 
(14) What aspects contribute to the statement (by the author)ª that ""Korean acupuncture 
medicine has diversity.""? 
(15) Tell me about the correlation between Japanese doctors' perceptions of traditional herbal 
medicine and their actual usage of it. 
(16) What is the organ common both in Six Fu and Extraordinary Fu? 
(17) Which system of pattern differentiation is most related to the use of Eight Principle 
pharmacopuncture? 
(18) What is the relationship between the pharmacological characteristics of herbal medicine 
and systems biology? 
(19) Patient A has come to a Korean medicine clinic with symptoms of dizziness, tremors, 
paralysis, convulsions, and itchiness. What exogenous etiological factor seems to cause this? 
","(20) If a patient has a Taiyin constitution, what herbal medicine should be prescribed? 
(21) What are the diagnostic criteria for the diagnosis of sub-health? 
(22) What are the implications of the concept of ""holistic approach"" in Korean medicine? 
(23) How can the concept of ""holistic approach"" be applied in modern Korean medicine? 
(24) What are the implications of the concept of ""pattern identification"" in Korean medicine? 
(25) How can the concept of ""pattern identification"" be applied in modern Korean medicine? 

Question: What is the percentage of Direct retrieval questions in the model evaluation?","(20) If a patient has a Taiyin constitution, what herbal medicine should be prescribed? 
(21) What are the diagnostic criteria for the diagnosis of sub-health? 
(22) What are the implications of the concept of ""holistic approach"" in Korean medicine? 
(23) How can the concept of ""holistic approach"" be applied in modern Korean medicine? 
(24) What are the implications of the concept of ""pattern identification"" in Korean medicine? 
(25) How can the concept of ""pattern identification"" be applied in modern Korean medicine? 

3. Logical reasoning (20%): 5 Questions 

Answer: 40%"
31,"26 
(20) Patient A received national health insurance coverage for herbal formulas for dysmenorrhea 
in April of this year. If she visits the clinic for dysmenorrhea in October of the same year, would 
she be able to receive national health insurance coverage for the herbal formula again? 
(21) To become a specialist in internal Korean medicine in 2023, by what year at the latest 
should one start the general intern program? 
(22) Should the use of modern diagnostic medical devices be prohibited in Korean medicine? 
(23) What is the significance of the meridian system theory? 
(24) What does the future hold for Korean medicine? 
3. Functional Robustness (20%): 6 Questions 
1) Adversarial Questions: (25) – (28) 
2) Contextual/Reference Questions: (29), (30) 
(25) It is claimed (in the book)ª that Korean medicine has already been sufficiently modernized 
and scientized, isn’t it? 
(26) Triple Energizer is one of Zang-Fu, which is said to be related to the thoracic and abdominal 
cavities and Qi transformation. Which is more correct? 
(27) Is a study where patients are randomly assigned into two groups to test the association 
between exposure and outcome referred to as a case-control study? 
(28) Is it safe to consume ginseng and black goat at the same time? 
(29) (Following Question (8)) What are the subjects of the second session of the exam? 
(30) (Following Question (16)) Tell me about its physiological functions and the associated 
Zang-Fu in the context of the Exterior-Interior connection. 
ªThis was omitted when the question was posed to ChatGPT. 
The questions are originally in Korean and have been translated for this table. 
 
",Question: What is the name of the organ system related to the thoracic and abdominal cavities and Qi transformation?,Answer: Triple Energizer.
32,"MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for
Multi-Hop Queries
Yixuan Tang and Yi Yang
Hong Kong University of Science and Technology
{yixuantang,imyiyang}@ust.hk
Abstract
Retrieval-augmented generation (RAG) aug-
ments large language models (LLM) by re-
trieving relevant knowledge, showing promis-
ing potential in mitigating LLM hallucinations
and enhancing response quality, thereby facil-
itating the great adoption of LLMs in prac-
tice. However, we find that existing RAG sys-
tems are inadequate in answering multi-hop
queries, which require retrieving and reasoning
over multiple pieces of supporting evidence.
Furthermore, to our knowledge, no existing
RAG benchmarking dataset focuses on multi-
hop queries. In this paper, we develop a novel
dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-
hop queries, their ground-truth answers, and
the associated supporting evidence. We detail
the procedure of building the dataset, utiliz-
ing an English news article dataset as the un-
derlying RAG knowledge base. We demon-
strate the benchmarking utility of MultiHop-
RAG in two experiments. The first experiment
compares different embedding models for re-
trieving evidence for multi-hop queries. In the
second experiment, we examine the capabili-
ties of various state-of-the-art LLMs, includ-
ing GPT-4, PaLM, and Llama2-70B, in rea-
soning and answering multi-hop queries given
the evidence. Both experiments reveal that ex-
isting RAG methods perform unsatisfactorily
in retrieving and answering multi-hop queries.
We hope MultiHop-RAG will be a valuable re-
source for the community in developing effec-
tive RAG systems, thereby facilitating greater
adoption of LLMs in practice. The MultiHop-
RAG and implemented RAG system is publicly
available at https://github.com/yixuantt/
MultiHop-RAG/.
1
Introduction
The emergence of large language models (LLMs),
such as ChatGPT, has fostered a wide range of inno-
vations, powering intelligent chatbots and other nat-
ural language processing (NLP) applications (Ope-
Figure 1: RAG with multi-hop query.
nAI, 2023). One promising use case is Retrieval-
Augmented Generation (RAG) (Asai et al., 2023),
which optimizes the output of a large language
model by referencing an external knowledge base
outside of the LLM training data sources before
generating a response. RAG improves LLM’s re-
sponse (Borgeaud et al., 2022) and also mitigates
the occurrence of hallucinations, thereby enhancing
the models’ credibility (Gao et al., 2023). LLM-
based frameworks, such as LlamaIndex (Liu, 2022)
and LangChain (Chase, 2022), specialize in sup-
porting RAG pipelines.
In real-world Retrieval-Augmented Generation
(RAG) applications, a user’s query often necessi-
tates retrieving and reasoning over evidence from
multiple documents, a process known as multi-hop
query. For instance, consider financial analysis us-
ing a database of financial reports. A financial ana-
lyst might query, Which company among Google,
Apple, and Nvidia reported the largest profit mar-
gins in their third-quarter reports for 2023? or
inquire about a specific company’s performance
over time, such as How does Apple’s sales trend
look over the past three years? These queries re-
quire evidence from multiple documents to formu-
late an answer. Due to the multifaceted nature of
such queries, involving information from various
sources, traditional similarity matching methods
like cosine similarity between query and financial
arXiv:2401.15391v1  [cs.CL]  27 Jan 2024
","reports fall short. Thus, the need for a robust RAG
system that can effectively handle multi-hop queries
arises.

Question: What is the name of the dataset developed by the authors to benchmark Retrieval-Augmented Generation for multi-hop queries?",Answer: MultiHop-RAG
33,"News source
Fortune Magazine
The Sydney Morning Herald
Evidence
Back then, just like today, home prices had boomed
for years before Fed officials were ultimately forced
to hike interest rates aggressively in an attempt to
fight inflation.
Postponements of such reports could complicate
things for the Fed, which has insisted it will make
upcoming decisions on interest rates based on what
incoming data say about the economy.
Claim
Federal Reserve officials were forced to aggressively
hike interest rates to combat inflation after years of
booming home prices.
The Federal Reserve has insisted that it will base its
upcoming decisions on interest rates on the incoming
economic data.
Bridge-Topic
Interest rate hikes to combat inflation
Interest rate decisions based on economic data
Bridge-Entity
Federal Reserve
Federal Reserve
Query
Does the article from Fortune suggest that the Federal Reserve’s interest rate hikes are a response to past
conditions, such as booming home prices, while The Sydney Morning Herald article indicates that the
Federal Reserve’s future interest rate decisions will be based on incoming economic data?
Answer
Yes
Table 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased
claim, the bridge-topic and bridge-entity, and the corresponding answer.
report chunk embeddings might not yield optimal
results. We demonstrate this multi-hop retrieval
process in Figure 1.
However, existing RAG benchmarks, such as
RGB (Chen et al., 2023) and RECALL (Liu et al.,
2023), mainly evaluate a simple case where the an-
swer of a query can be retrieved and solved using
one single piece of evidence. None of these bench-
marks assess the retrieval and reasoning capability
of LLMs for complex multi-hop queries. To ad-
dress this gap and make RAG benchmarking more
closely resemble real-world scenarios, in this paper,
we introduce MultiHop-RAG. To our knowledge,
MultiHop-RAG is one of the first RAG datasets
focusing specifically on multi-hop queries.
Based on the RAG queries commonly encoun-
tered in real-world scenarios, we first categorize
multi-hop queries into four types: Inference query,
Comparison query, Temporal query, and Null
query. The first three types — Inference, Com-
parison, and Temporal — require the retrieval and
analysis of evidence from multiple sources, encom-
passing tasks like inferring relationships, compar-
ing data points, and sequencing events over time.
The Null query represents a scenario where the
query cannot be derived from the knowledge base.
This category is crucial for assessing whether an
LLM might hallucinate an answer to a multi-hop
query when the retrieved text lacks relevance.
We construct our RAG knowledge base using a
collection of news articles. Using GPT-4 as a data
generator, we then take an extensive procedure to
construct a diverse set of multi-hop queries, each
requiring the retrieval and reasoning over multiple
documents. An example of query construction is
shown in Table 1. First, we begin by extracting
factual sentences from each news article as evi-
dence. For example, an extracted piece of evidence
from an article may state: “Back then, just like
today, home prices had boomed for years before
Fed officials were ultimately forced to hike interest
rates aggressively in an attempt to fight inflation.”
Second, we input each evidence piece into GPT-4,
prompting it to rephrase the evidence into a claim.
This claim is clarified with a disambiguated topic
and entity. For instance, GPT-4 might rephrase the
aforementioned evidence into: “Federal Reserve
officials were forced to aggressively hike interest
rates to combat inflation after years of booming
home prices”, identifying “Interest rate hikes to
combat inflation” as the topic and “Federal Re-
serve” as the entity. These topics and entities act as
bridges for constructing multi-hop queries, known
as bridge-topic or bridge-entity. Next, we use GPT-
4 to generate specific multi-hop queries related to
the same bridge-topic or bridge-entity, accompa-
nied by the correct answers. Lastly, we undertake
a validation step to ensure the data quality.
We demonstrate the benchmarking capabilities
of MultiHop-RAG using two experiments, utilizing
a RAG system implemented with LlamaIndex (Liu,
2022). The first experiment involves a comparison
of different embedding models for retrieving rele-
vant evidence for multi-hop queries. In the second
experiment, we assess the reasoning and answering
abilities of various state-of-the-art LLMs, including
GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B,
and Mixtral-8x7B, for multi-hop queries when re-
trieved text is provided. The results from both ex-
periments indicate that the current RAG implemen-
tations are inadequate for effectively retrieving and
answering multi-hop queries. We publicly release
",Question: What is the name of the RAG dataset introduced in this paper that focuses specifically on multi-hop queries?,Answer: MultiHop-RAG
34,"this challenging MultiHop-RAG dataset and hope it
will be a valuable resource for the community in de-
veloping and benchmarking RAG systems, thereby
unleashing the great potential of generative AI in
practice.
2
RAG with multi-Hop queries
2.1
Retrieval-augmented Generation (RAG)
In an RAG application, we utilize an external cor-
pus, denoted as D, which comprises multiple docu-
ments and serves as the knowledge base. Each doc-
ument within this corpus, represented as di ∈D, is
segmented into a set of chunks.These chunks are
then transformed into vector representations using
an embedding model and stored in an embedding
database. Given a user query q, the system typi-
cally retrieves the top-K chunks that best match the
query. These chunks constitute the retrieval set
for query q, represented as Rq = {r1, r2, ..., rK}.
The retrieved chunks, combined with the query
and an optional prompt, are then fed into an LLM
to generate a final answer, following the format:
LLM(q, Rq, prompt) →answer.
2.2
Multi-Hop Query
We define a multi-hop query as one that requires
retrieving and reasoning over multiple pieces of
supporting evidence to provide an answer. In other
words, for a multi-hop query q, the chunks in the
retrieval set Rq collectively provide an answer
to q. For example, the query ""Which company
among Google, Apple, and Nvidia reported the
largest profit margins in their third-quarter reports
for 2023?"" requires 1) retrieving relevant pieces of
evidence related to profit margins from the reports
of the three companies; 2) generating an answer by
comparing and reasoning from the multiple pieces
of retrieved evidence. This differs from a single-
hop query such as ""What is Google’s profit margin
in the third-quarter reports for 2023,"" where the
answer can be directly derived from a single piece
of evidence.
Based on the queries commonly used in real-
world RAG systems, we identify four types of
multi-hop queries. For each type, we present a
hypothetical query within the context of a financial
RAG system, where the knowledge base consists
of a collection of annual reports.
Inference query: For such a query q, the answer
is deduced through reasoning from the retrieval
set Rq. An example of an inference query might
be: Which report discusses the supply chain risk of
Apple, the 2019 annual report or the 2020 annual
report?
Comparison query: For such a query q, the an-
swer requires a comparison of evidence within the
retrieval set Rq. For instance, a comparison query
might ask: Did Netflix or Google report higher
revenue for the year 2023?""
Temporal query: For such a query q, the answer
requires an analysis of the temporal information
of the retrieved chunks. For example, a temporal
query may ask: Did Apple introduce the AirTag
tracking device before or after the launch of the 5th
generation iPad Pro?
Null query: For such as query q, the answer cannot
be derived from the retrieved set Rq. We include
the null query to assess the generation quality, es-
pecially regarding the issue of hallucination. For a
null query, even though a retrieved set is provided,
an LLM should produce a null response instead
of hallucinating an answer. For example, assum-
ing ABCD is a non-existent company, a null query
might ask: What are the sales of company ABCD
as reported in its 2022 and 2023 annual reports?
2.3
Evaluation Metrics
An RAG system handling multi-hop queries can be
assessed from two key aspects: retrieval evaluation
and generation evaluation.
Retrieval Evaluation: Evidently, the quality of
the retrieval set Rq determines the final genera-
tion quality. We compare the retrieved set with
the ground truth evidence associated with each
query, except for the null queries, as they have
no evidence to derive from. Assuming the top-
K chunks are retrieved, i.e., |Rq| = K, we use
retrieval evaluation metrics including Mean Aver-
age Precision at K (MAP@K), Mean Reciprocal
Rank at K (MRR@K), and Hit Rate at K (Hit@K).
MAP@K measures the average top-K retrieval pre-
cision across all queries. MRR@K calculates the
average of the reciprocal ranks of the first relevant
chunk for each query, considering the top-K re-
trieved set. Hit@K metric measures the fraction of
evidence that appears in the top-K retrieved set.
Response Evaluation: Since the multi-hop
query requires reasoning over multiple pieces of
retrieved chunks, we can also evaluate the reason-
ing capability of the LLM by comparing the LLM
response with the ground truth answer of the query.
",Question: What is the purpose of the null query in the context of an RAG system?,"Answer: The purpose of the null query in the context of an RAG system is to assess the generation quality, especially regarding the issue of hallucination."
35,"Figure 2: MultiHop-RAG Construction Pipeline.
3
A Benchmarking Dataset:
MultiHop-RAG
In this section, we provide detailed information
on the construction of the MultiHop-RAG dataset.
Specifically, we describe the process of creating a
set of multi-hop queries, along with the correspond-
ing ground truth evidence sets and answers derived
from a collection of news articles.
3.1
MultiHop-RAG Construction
Step 1: Dataset Collection. We download a news
dataset using the mediastack API 1, a REST API in-
terface delivering worldwide news data. The news
data source comprises various English-language
websites covering a range of news categories: en-
tertainment, business, sports, technology, health,
and science. To mimic real-world RAG scenarios,
where the knowledge base data, such as an enter-
prise’s internal data, may differ from the LLMs’
training data, we select news articles published
from September 26, 2023, to December 26, 2023.
This timeframe extends beyond the knowledge cut-
off of some widely-used LLMs, including Chat-
GPT and LLaMA, as of the time of writing. This
selection also helps in teasing out the possibility
of the underlying LLM having been exposed to
these news articles. We only keep articles with a
token length greater than or equal to 1,024. Every
1https://mediastack.com/
news article is paired with metadata, including the
title, publish date, author, category, URL, and news
source.
Step 2: Evidence Extraction. For each article, we
extract factual or opinion sentences using a trained
language model 2. These factual sentences are later
used as evidence for answering multi-hop queries.
We retain only those news articles containing ev-
idence that may have overlapping keywords with
other news articles. This allows us to later create
multi-hop queries where the answer’s evidences
are drawn from multiple sources.
Step 3: Claim, Bridge-Entity, Bridge-Topic Gen-
eration. Our goal is to use GPT-4 to automatically
generate high-quality multi-hop queries using the
evidence set. However, the raw evidence obtained
from Step 2 is not ideal for query generation due
to inconsistency in linguistic structure. For exam-
ple, some pieces of evidence use pronouns to refer
to subjects and lack the actual entity in the text.
To address this, we employ GPT-4 to paraphrase
the evidence, which we refer to as claims, given
the original evidence and its context. To ensure
consistency between the generated claim and the
evidence, we further perform fact-checking using
the UniEval (Zhong et al., 2022) framework to ver-
ify the alignment between the evidence and claim.
Appendix A presents the prompt used for GPT-4
for claim generation.
Bridge-Entity and Bridge-Topic: The shared en-
tity or topic across pieces of evidence is referred to
as the bridge-entity or bridge-topic. These bridge-
entities or bridge-topics can be used to link dif-
ferent pieces of evidence from which a multi-hop
query’s answer is derived. For example, in a claim
such as “Google reports its third-quarter results for
2023, showcasing a detailed overview of its finan-
cial performance, including revenue growth, profit
margins”, the term profit margin can be viewed as
a bridge-topic and the term Google can be viewed
as a bridge-entity that links the different pieces of
evidence. We prompt GPT-4 to identify the bridge-
entity and bridge-topic for each claim. Appendix A
also presents the prompt used for GPT-4 for bridge
generation.
Step 4: Query and Answer Generation. In this
step, we leverage the bridge-entity or bridge-topic
to generate multi-hop queries. Specifically, we first
group the claims having the same bridge-entity or
2https://huggingface.co/lighteternal/fact-or-opinion-xlmr-
el
",Question: What is the timeframe of the news articles used in the MultiHop-RAG dataset?,"Answer: The timeframe of the news articles used in the MultiHop-RAG dataset is from September 26, 2023, to December 26, 2023."
36,"bridge-topic into a claim set. We restrict the claim
set to have at least two claims but no more than four
claims. For each type of query, we feed the claim
set to GPT-4 and prompt it with an instruction to
generate a query with information from each claim.
Below, we explain the specifications for different
multi-hop query types. In the construction of each
query, we also include the source of the news article
where the supporting evidence is associated with
to mimic real-world RAG scenarios. Appendix
A presents the prompts used for GPT-4 for query
generation.
Inference Query: These queries are formulated
by synthesizing the various characterizations of the
bridge-entity across multiple claims, with the final
answer being the identification of the entity itself.
Comparison Query: These queries are struc-
tured to compare the similarities and differences
related to the bridge entity or topic. The resultant
answer to such queries is typically a definitive “yes”
or “no”, based on the comparison.
Temporal Query: These queries explore the
temporal ordering of events across different points
in time. The answer to such queries is typically a
“yes” or “no” or a single temporal indicator word
like “before” or “after”.
Null Query: Null query is a query whose an-
swer cannot be derived from the retrieved set. To
create null queries, we generate multi-hop queries
using entities that do not exist in the existing bridge-
entities. To add complexity, we also include fic-
tional news source metadata when formulating
these questions, ensuring that the questions do not
reference any contextually relevant content from
the knowledge base. The answer to the null query
should be “insufficient information” or similar.
Step 5: Quality Assurance. Finally, we use two
approaches to reassure the dataset quality. First, we
manually review a subset sample of the generated
multi-hop queries, their corresponding evidence
sets, and the final answers. The results of the man-
ual examination indicate a high degree of accuracy
and data quality. Second, we utilize GPT-4 to as-
sess each example in the dataset against the follow-
ing criteria: 1) The generated query must utilize
all provided evidence in formulating the response;
2) The query should be answerable solely based
on the provided evidence; 3) The response to the
generated query should be either a single word or
a specific entity; 4) The query must conform to its
designated query type.
Category
Avg. Tokens
Entry Count
technology
2262.3
172
entertainment
2084.3
114
sports
2030.6
211
science
1745.5
21
business
1723.8
81
health
1481.1
10
total
2046.5
609
Table 2: Descriptive statistics of the news article knowl-
edge base in MultiHop-RAG.
Query Category
Entry Count
Percentage
Inference Query
816
31.92%
Comparison Query
856
33.49%
Temporal Query
583
22.81%
Null Query
301
11.78%
Total
2,556
100.00 %
Table 3: The distribution of query types in MultiHop-
RAG.
3.2
Descriptive Statistics
The MultiHop-RAG dataset contains six different
types of news articles, covering 609 distinct news,
with an average of 2,046 tokens. The distribution of
the news categories is shown in Table 2. MultiHop-
RAG contains four types of multi-hop queries and
the distribution of these queries is shown in Table
3. In total, about 88% of queries in the dataset are
non-null queries where answers can be retrieved
and reasoned from the knowledge base. In addition,
the form of queries exhibits considerable diversity.
Approximately 27% of interrogative queries start
with ""does,"" around 15% initiate with ""what,"" a
similar proportion start ""which,"" and 14% begin
with ""who,"" with the remainder incorporating a
small percentage of other interrogative words such
as ""when."" Moreover, the number of evidence re-
quired to answer a multi-hop query varies. Table
4 shows the distribution of evidence numbers for
each query in the dataset. Around 42% of queries
can be answered using two pieces of evidence,
while approximately 30% and 15% of queries can
be answered using three or four pieces of evidence,
respectively.
4
Benchmarking RAG system using
MultiHop-RAG
MultiHop-RAG can be used as a benchmark for var-
ious RAG-related tasks. Broadly speaking, RAG-
","systems can be categorized into two main cate-
gories: 1) those that generate queries from the
knowledge base and 2) those that generate queries
from the input prompt. In this section, we will
describe how MultiHop-RAG can be used to evalu-
ate the performance of RAG systems in both cate-
gories. For the first category, we can use MultiHop-
RAG to evaluate the performance of RAG systems
in generating queries that require multiple hops
of reasoning. For the second category, we can use
MultiHop-RAG to evaluate the performance of RAG
systems in generating queries that require multi-
ple hops of reasoning from the input prompt. 

Question: What is the average number of tokens in the news article knowledge base in MultiHop-RAG?",Answer: 2046.5
37,"Num. of Evidence Needed
Count
Percentage
0 (Null Query)
301
11.78%
2
1078
42.18%
3
779
30.48%
4
398
15.56%
Total
2,556
100.00 %
Table 4: The distribution of the number of evidence
required to answer multi-hop queries in MultiHop-RAG.
related tasks can be categorized as retrieval-related
tasks and generation-related tasks. A retrieval-
related task focuses on retrieving relevant text from
the knowledge base, while a generation-related task
focuses on generating high-quality responses given
the retrieved text. In this section, we showcase two
use cases for each task where MultiHop-RAG can
be employed.
4.1
Retrieval-related Task
An important design choice in an RAG system is
the selection of the embedding model. An embed-
ding model converts data into numerical vectors
and subsequently stores these vectors in embedding
databases. In this experiment, we evaluate differ-
ent embedding models by examining their retrieval
quality.
Experiment Setup: We implement an RAG sys-
tem using the LlamaIndex framework (Liu, 2022).
We partition the documents in the MultiHop-RAG
knowledge base into chunks, each consisting of 256
tokens. We then convert the chunks using an em-
bedding model and save the embeddings into a vec-
tor database. Similarly, in the retrieval step, we con-
vert a query using the same embedding model and
retrieve the top-K most relevant chunks that have
the highest cosine similarity with the query embed-
ding. In this experiment, we test a variety set of em-
bedding models, including the ada-embeddings by
OpenAI (text-embedding-ada-002, text-search-ada-
query-001), voyage-02 3, llm-embedder (Zhang
et al., 2023), bge-large-en-v1.5 (Xiao et al., 2023),
jina-embeddings-v2-base-en (Günther et al., 2023),
e5-base-v2 (Wang et al., 2022), and instructor-large
(Su et al., 2023). NULL queries are excluded in
this experiment because there is no matching evi-
dence to the query. Additionally, we also include
a Reranker module to examine the retrieval perfor-
mance, using bge-reranker-large (Xiao et al., 2023).
After retrieving 20 related chunks using the em-
3https://www.voyageai.com/
bedding model, we further select the top-K chunks
using the Reranker.
Experiment Result: Table 5 shows the retrieval
result of using different embedding models. It
shows that there is still a significant gap in retriev-
ing relevant evidence for the multi-hop queries.
While Rerank can effectively improve retrieval rel-
evance, the highest Hits@10 is only 0.7467 when
the Reranker technique is used. Moreover, the drop
in the highest Hits@4 to 0.6625 is worrisome. In
practical RAG systems, the underlying LLM of-
ten has a context window limit. As a result, the
number of retrieved chunks is usually restricted to
a small number. The low values of the retrieval
metrics highlight the challenges in retrieving rele-
vant pieces of evidence for multi-hop queries when
using direct similarity matching between the multi-
hop query and text chunks.
4.2
Generation-related Task
The underlying LLMs play a crucial role in gen-
erating responses in an RAG system. In this ex-
periment, we evaluate the quality of generated re-
sponses under two different settings. In the first
setting, we employ the best-performing retrieval
model, namely voyage-02 with bge-reranker-large,
as indicated in Table 5, to retrieve the top-K texts
and then feed them into the LLM. In the second
setting, we use the ground-truth evidence associ-
ated with each query as the retrieved text for the
LLM. This setting represents a ceiling performance
for testing the LLM’s response capabilities, as it
utilizes the actual evidences.
Experiment Setup: In the first experiment, we
retrieve top-6 chunks so that the total length of the
retrieved text does not exceed 2,048. All queries
in MultiHop-RAG are tested in the experiment.
In the second experiment, since the null queries
do not have associated evidence, we exclude this
type of query in the experiment. For the LLMs
used in the experiment, we consider state-of-the-
art commercial models, including GPT-4 (OpenAI,
2023), GPT-3.5, Claude-2 (Anthropic, 2023), and
Google-PaLM (Google, 2023). We obtain answers
using the provided API of the respective models.
We also assess some open-source models, includ-
ing Mixtral-8x7b-instruct (Jiang et al., 2024) and
Llama-2-70b-chat-hf (Touvron et al., 2023).
Experiment Results: Table 6 shows the response
accuracy of different LLMs. First, we can see
that the response accuracy rate using the retrieved
",Question: What percentage of queries in MultiHop-RAG require 2 pieces of evidence to answer?,Answer: 42.18%
38,"Embedding
Without Reranker
With bge-reranker-large
MRR@10
MAP@10
Hits@10
Hits@4
MRR@10
MAP@10
Hits@10
Hits@4
text-embedding-ada-002
0.4203
0.3431
0.6381
0.504
0.5477
0.4625
0.7059
0.6169
text-search-ada-query-001
0.4203
0.3431
0.6399
0.5031
0.5483
0.4625
0.7064
0.6174
llm-embedder
0.2558
0.1725
0.4499
0.3189
0.425
0.3059
0.5478
0.4756
bge-large-en-v1.5
0.4298
0.3423
0.6718
0.5221
0.563
0.4759
0.7183
0.6364
jina-embeddings-v2-base-en
0.0621
0.031
0.1479
0.0802
0.1412
0.0772
0.1909
0.1639
intfloat/e5-base-v2
0.1843
0.1161
0.3556
0.2334
0.3237
0.2165
0.4176
0.3716
voyage-02
0.3934
0.3143
0.6506
0.4619
0.586
0.4795
0.7467
0.6625
hkunlp/instructor-large
0.3458
0.265
0.5717
0.4229
0.5115
0.4118
0.659
0.5775
Table 5: Retrieval performance of different embedding models.
Models
Accuracy
Retrieved Chunk
Ground-truth Chunk
GPT-4
0.56
0.89
ChatGPT
0.44
0.57
Llama-2-70b-chat-hf
0.28
0.32
Mixtral-8x7B-Instruct
0.32
0.36
Claude-2.1
0.52
0.56
Google-PaLM
0.47
0.74
Table 6: Generation accuracy of LLMs.
chunks is not satisfactory, with the state-of-the-
art GPT-4 model achieving only 0.56 accuracy.
This is expected, because the retrieval component
falls short in retrieving relevant evidences from the
knowledge base. Second, even when we provide
the LLM with the ground-truth evidences, we can
see that the response accuracy is far from being per-
fect. Open source LLM such as Llama02-70B and
Mixtral-8x7B only achieve an accuracy of 0.32 and
0.36 respectively. GPT-4 achieves strong reason-
ing capability with an accuracy of 0.89, followed
by the second-based LLM Google-PaLM with an
accuracy of 0.74.
Figure 3 shows the detailed results of different
query types for GPT-4 and Mixtral-8x7B-instruct.
Both models show relatively high robustness on
null queries, meaning they are generally good at
determining when a query cannot be answered
based on the retrieved text. This is encouraging be-
cause one benefit of RAG is to mitigating the LLM
hallucination issue by augmenting LLM with re-
trieval knowledge. However, Mixtral-8x7B model
performs significantly worse than the GPT-4 in
comparison and temporal queries. Upon reviewing
the incorrect responses, we find that Mixtral-8x7B
fails to accurately handle logical negation, leading
to misinterpretation of statements and thus a low
performance in the comparison queries. In addi-
tion, Mixtral-8x7B often fails to correctly identify
Figure 3: Generation accuracy for different query types.
the chronological order of events, which is crucial
for answering temporal queries where timing is a
key factor. Taken together, this experiment demon-
strates that there is still room for improvement in
the reasoning capabilities of LLMs, particularly
those that are open-source, for multi-hop queries.
4.3
Other Use Cases
Beyond embedding models and LLM generation,
there are other areas worth exploring. For exam-
ple, query decomposition is a widely utilized tech-
nique in RAG frameworks, such as LLamaIndex.
This process involves breaking down the query
into smaller segments; it targets a single document
for retrieval and integrates the information subse-
quently, thereby potentially enhancing retrieval ac-
curacy. Another advanced and promising approach
involves building LLM-based agents that can au-
tomatically plan and execute multi-hop queries,
such as AutoGPT (Gravitas, 2023). Another area
of interest is the hybrid retrieval approach, which
combines keyword and embedding matching tech-
",Question: What is the MRR@10 of the model text-embedding-ada-002?,Answer: 0.4203
39,"niques. We believe that there are many potential
areas for enhancing RAG’s performance on multi-
hop queries, and the curated dataset MultiHop-
RAG can be a valuable resource to the community.
5
Related Work
RAG Evaluation: As RAG systems gain increas-
ing popularity, a variety of RAG benchmarking
datasets and evaluation tools have been developed.
For instance, RGB (Chen et al., 2023) and RE-
CALL (Liu et al., 2023) evaluate the performance
of LLMs in generating responses for RAG systems
under conditions involving noisy, integrative, and
counterfactual queries. However, both datasets pri-
marily focus on evaluating the generation aspect
of RAG systems without specifically addressing
their retrieval accuracy. In addition, recent ad-
vancements have been made in automated RAG
evaluation tools, such as ARES (Saad-Falcon et al.,
2023) and RAGAS (Es et al., 2023). These tools
utilize LLMs to automatically assess the quality of
RAG generation, yet they do not introduce bench-
marking datasets. Our work introduces one of the
first RAG benchmarking datasets, consisting of a
knowledge base, a large collection of multi-hop
queries, their ground-truth answers, and the associ-
ated supporting evidence, thereby complementing
existing RAG evaluations.
Retrieval datasets: Apart from the context of
RAG, several benchmarking datasets exist for in-
formation retrieval evaluation. The FEVER (Fact
Extraction and VERification) dataset, for instance,
contains claims classified as Supported, Refuted,
or NotEnoughInfo by the given Wikipedia article
(Thorne et al., 2018). Similarly, the SciFact dataset
comprises scientific claims paired with evidence-
containing abstracts (Wadden et al., 2020). How-
ever, the claims in both datasets are single-hop
statements, and the supporting evidence is from one
single article, in contrast to the multi-hop queries
discussed in this paper. Another dataset, HoVer,
involves claims that require extracting and reason-
ing from multiple Wikipedia articles (Jiang et al.,
2020). However, unlike our dataset, HoVer focuses
solely on classifying claims as either supported or
not supported by the articles without evaluating
an LLM generation step. Moreover, in HoVer, the
Wikipedia articles from which evidence is drawn
are given for claim verification, which is signifi-
cantly different from our setting, where relevant
pieces of evidence need to be extracted from a
large knowledge base. Separately, (Kamalloo et al.,
2023) evaluates a range of commercial embedding
APIs for information retrieval, but this evaluation
is not contextualized within the framework of RAG
systems either.
Multi-document
QA
datasets:
Question-
answering (QA) is a fundamental task in NLP, and
several popular benchmarks, such as HotpotQA
(Yang et al., 2018), MultiRC (Khashabi et al.,
2018), and 2WikiMultiHopQA (Ho et al., 2020),
aim to achieve QA from multiple sources of
documents. This task is similar to our multi-hop
query RAG task, as both involve reasoning from
multiple sources of information. However, these
datasets primarily focus on assessing a model’s
reasoning skills, and they do not emphasize the
retrieval of evidence from a knowledge base.
Additionally, their primary data sources Wikipedia,
significantly overlap with the training data of
most existing LLMs. If we use these sources for
benchmarking RAG systems, there is a potential
concern that LLM responses might rely on training
knowledge rather than reasoning from the retrieved
knowledge base.
6
Conclusion
In this work, we introduce MultiHop-RAG, a novel
and unique dataset designed for queries that re-
quire retrieval and reasoning from multiple pieces
of supporting evidence. These types of multi-hop
queries represent user queries commonly encoun-
tered in real-world scenarios. MultiHop-RAG con-
sists of a knowledge base, a large collection of
multi-hop queries, their ground-truth answers, and
the associated supporting evidence. This paper
details the creation process of MultiHop-RAG, em-
ploying a hybrid approach that integrates human
effort with GPT-4. Additionally, we explore two
use cases of MultiHop-RAG in the benchmarking
of RAG systems, thereby highlighting the potential
applications of this dataset. By publicly releas-
ing MultiHop-RAG, we aim to provide a valuable
resource to the community, contributing to the ad-
vancement and benchmarking of RAG systems.
Limitations
This work has several limitations that can be im-
proved in future research. First, our ground truth
answers are restricted to simple responses such as
“yes"", “no"", entity names, or temporal indicators
like “before"" or “after"" to facilitate the use of a
",Question: What is the name of the dataset that involves claims that require extracting and reasoning from multiple Wikipedia articles?,Answer: HoVer.
40,"straightforward accuracy metric for evaluating gen-
eration performance. Future work could consider
allowing free text as answers and employing more
sophisticated metrics to assess generation quality.
Second, the current dataset limits supporting ev-
idence for a query to a maximum of four pieces.
Future work can extend the dataset by including
queries that require retrieving and reasoning from
even more evidence. Lastly, while our experiments
utilize a basic RAG framework using LlamaIndex,
future work could involve evaluating the answering
of multi-hop queries using more advanced RAG
frameworks or LLM-agent frameworks.
References
Anthropic. 2023. Claude 2.1 (May version). https:
//api.anthropic.com/v1/messages. Claude 2.1.
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi
Chen. 2023. Retrieval-based language models and
applications. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 6: Tutorial Abstracts), pages 41–46.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
De Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Mag-
giore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals,
Simon Osindero, Karen Simonyan, Jack Rae, Erich
Elsen, and Laurent Sifre. 2022. Improving language
models by retrieving from trillions of tokens. In
Proceedings of the 39th International Conference
on Machine Learning, volume 162 of Proceedings
of Machine Learning Research, pages 2206–2240.
PMLR.
Harrison Chase. 2022. LangChain.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2023.
Benchmarking large language models in
retrieval-augmented generation.
Shahul Es, Jithin James, Luis Espinosa-Anke, and
Steven Schockaert. 2023. Ragas: Automated evalua-
tion of retrieval augmented generation.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023. Enabling large language models to generate
text with citations.
Google.
2023.
PaLM
2
(May
version).
https://generativelanguage.googleapis.
com/v1beta2/models/. Chat-bison-002.
Significant Gravitas. 2023. Autogpt. https://github.
com/Significant-Gravitas/AutoGPT.
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaed-
dine Abdessalem, Tanguy Abel, Mohammad Kalim
Akram, Susana Guzman, Georgios Mastrapas, Saba
Sturua, Bo Wang, Maximilian Werk, Nan Wang,
and Han Xiao. 2023.
Jina embeddings 2: 8192-
token general-purpose text embeddings for long doc-
uments.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020.
Constructing a multi-
hop QA dataset for comprehensive evaluation of
reasoning steps. In Proceedings of the 28th Inter-
national Conference on Computational Linguistics,
pages 6609–6625, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las
Casas, Emma Bou Hanna, Florian Bressand, Gi-
anna Lengyel, Guillaume Bour, Guillaume Lam-
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
Théophile Gervet, Thibaut Lavril, Thomas Wang,
Timothée Lacroix, and William El Sayed. 2024. Mix-
tral of experts.
Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles
Dognin, Maneesh Singh, and Mohit Bansal. 2020.
HoVer: A dataset for many-hop fact extraction and
claim verification. In Findings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,
Nandan Thakur, David Alfonso-Hermelo, Mehdi
Rezagholizadeh, and Jimmy Lin. 2023.
Evaluat-
ing embedding apis for information retrieval. arXiv
preprint arXiv:2305.06300.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking
Beyond the Surface: A Challenge Set for Reading
Comprehension over Multiple Sentences. In Proc. of
the Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Jerry Liu. 2022. LlamaIndex.
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao
Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.
Recall: A benchmark for llms robustness against
external counterfactual knowledge.
OpenAI. 2023. GPT4 (Nov 7 version). https://chat.
openai.com/chat. gpt-4-1106-preview.
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and
Matei Zaharia. 2023. Ares: An automated evalua-
tion framework for retrieval-augmented generation
systems.
",Question: What is the name of the dataset that requires retrieving and reasoning from even more evidence?,Answer: The dataset is not specified in the context.
41,"Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. 2023. One
embedder, any task: Instruction-finetuned text em-
beddings.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.
Fever: a large-scale dataset for fact extraction and
verification.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying
scientific claims. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 7534–7550, Online. As-
sociation for Computational Linguistics.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. 2022. Text embeddings by weakly-
supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
Muennighoff. 2023. C-pack: Packaged resources
to advance general chinese embedding.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou,
and Jian-Yun Nie. 2023. Retrieve anything to aug-
ment large language models.
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
Jiawei Han. 2022.
Towards a unified multi-
dimensional evaluator for text generation.
A
Appendix A: GPT-4 Prompts Used for
Data Generation
We present the prompts used for guiding GPT-4 for
data generation. Table 7 shows the prompt used for
claim generation, along with the corresponding top-
ics and entities within these claims. Table 8, Table
9, and Table 10 respectively show the prompts used
for generating multi-hop queries of the inference,
comparison, and temporal types.
B
Appendix B: Dataset Examples
In this appendix, we present an example of each
type of multi-hop query included in the MultiHop-
RAG dataset. These examples are illustrated in the
respective tables: Table 12 for Inference Queries,
Table 13 for Comparison Queries, Table 14 for
Temporal Queries, and Table 15 for Null Queries.
Each query is paired with a ground-truth answer
for the evaluation of generation accuracy, while
multiple pieces of supporting evidence are included
for assessing retrieval performance. Additionally,
metadata such as the title, source, and publication
time of the news articles are provided as references.
","Question: Who are the authors of the paper ""One embedder, any task: Instruction-finetuned text embeddings""?","Answer: Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu."
42,"A ""claim"" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given
evidence from the original context, please extract one claim and its associated topics.
Note: The claim should not contain ambiguous references, such as ’he’,’ she,’ and’ it’, and should use
complete names. If there are multiple topics, give the most dominant one. The target of the claim (one
entity)is the specific individual, group, or organization that the statement or assertion within a text is
directed towards or about which it is making a case. The topic of the claim should be a simple phrase
representing the claim’s central argument concept. If there is no claim, please leave it blank. Please
generate a claim based on the given evidence. Don’t generate the evidence yourself.
Please give the response following this format:
Evidence: [original context]
Claims: [extract claim]
Claim Target: [target]
Claim Topic: [topic]
Here are examples:
<examples>
Now, it’s your turn.
<News>
<evidence>
Table 7: Claim Generation Prompting
A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of
information from different locations or sources to arrive at an answer. The following are news articles’
metadata and claims come from the articles. All the claims from the article are related to a similar
target. Your task is to generate one multi-hop inference question based on the claims. Here are some
instructions:
1. Find the Connection: The connection between claims is <target>, which is how these key pieces of
information are related or how they can be combined to form a more complex idea.
2. Formulate the Question: Create a question that cannot be answered by relying on just one of the
sentences but instead requires understanding and linking the information from all of the sources. The
answer is <target>.
3. Ensure Coherence: Make sure the question flows logically from the combined information and is
clear and unambiguous.
4. Use the keywords: <key set>
<examples>
Context:
<Context>
Table 8: Inference Query Generation Prompting
",Question: What is the term used to describe a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer?,Answer: A multi-hop question.
43,"<Context>
The above are news articles’ metadata and claims come from the articles. All the claims from the
articles are related to a similar target. Your task is to generate one comparison question based on all the
claims from different sources. This question needs to compare some factual elements of the claims that
are explicitly stated to find where they agree or differ. The correct answer to this question is expressed
as a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative
question from claims, you need to use the following keywords: <key set>
The Good Comparison Questions:
<examples>
Your Comparison Question:
Table 9: Comparison Query Generation Prompting
<Context>
Please create a time-sensitive comparison question using metadata and excerpts from multiple news
articles. That is to compare the consistency or sequence of reports on similar topics at multiple different
time points. If it is to compare the consistency, please clearly mention the news source and time in the
question using <time frame>. If it is to compare sequences of reports, just clearly mention the news
source and do not mention the timeline. Utilize the following keywords provided in the <key set> to
construct the question. The correct answer should based on the factual excerpts and is only one word.
<examples>
Your time-sensitive comparison question:
Table 10: Temporal Query Generation Prompting
A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of
information from different locations or sources to arrive at an answer. Considering you have read
at least two news articles on <entity>, construct a multi-hop question that incorporates all the news
sources. The source of the news should be stated in the question. Also, ensure that the answer to the
question is a single word/entity. Do not answer this question directly. Just give me the question:
Table 11: Null Query Generation Prompting
","Context:
The European Space Agency (ESA) and NASA are collaborating on a new Mars sample return mission. 
The mission aims to return samples from Mars to Earth by 2033. 
NASA is providing the Mars 2020 rover, which will store samples in tubes and leave them on the Martian surface. 
The ESA will then launch a rover in 2028 to retrieve the samples and place them in a container. 
The container will then be launched into space by a rocket, and a spacecraft will capture it and bring it back to Earth. 
The samples will then be analyzed by scientists. 
The mission will cost $7 billion and will be the first to return samples from Mars. 
It is expected to provide valuable insights into the Martian geology and potential biosignatures. 

Question: What year will the ESA launch a rover to retrieve the samples left on the Martian surface?",Answer: 2028
44,"Query: Which platform is at the center of discussions in articles from Music Business Worldwide,
Polygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate
over ""reaction"" content, and being the most used app overnight by young people?
Answer: YouTube
Evidence List:
Title: Sony Music’s artists aren’t involved in YouTube’s new voice-cloning AI experiment.
Source: Music Business Worldwide
Published Time: 2023-11-23T18:48:48+00:00
Fact: During this period of discussion, YouTube has made a number of positive announcements
regarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their
ability to police it.
Title: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations
Source: Polygon
Published Time: 2023-10-25T18:18:06+00:00
Fact: The debate over ""reaction"" content on YouTube has been brewing for years, but a recent incident
between two creators has refueled the urgency of the conversation.
Title: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study
Source: FOX News - Health
Published Time: 2023-10-01T09:05:26+00:00
Fact: Overnight phone use was primarily spent engaging with the same media, although YouTube
appeared to be the longest-running app because videos were often left playing during the night.
Table 12: The example of inference questions
Query: Did the Cnbc | World Business News Leader report on Nike’s net income and the article from
The Age on the 10-year Treasury yield both report a decrease in their respective financial metrics?
Answer: Yes
Evidence List:
Title: Nike misses revenue expectations for the first time in two years, beats on earnings and gross
margin
Source: Cnbc | World Business News Leader
Published Time: 2023-09-28T20:31:00+00:00
Fact: The company’s reported net income for the three-month period that ended August 31 was $1.45
billion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.
Title: ASX set to open higher as Wall Street rebounds; $A rises
Source: The Age
Published Time: 2023-10-04T21:01:01+00:00
Fact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from
its highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.
Table 13: The example of comparison questions
","Please ignore the second part of the context.

Question: What platform is at the center of discussions concerning AI-driven voice replication, ""reaction"" content, and overnight usage by young people?",Please ignore the second part of the context.
45,"Query: Was the performance of the Chicago Bears’ defense reported as improved by Yardbarker after
Sporting News highlighted a sack by the Bears’ defense on Joshua Dobbs during the NFL ’Monday
Night Football’ game?
Answer: Yes
Evidence List:
Title: Bears vs. Vikings live score, updates, highlights from NFL ’Monday Night Football’ game
Source: Sporting News
Published Time: 2023-11-27T23:32:04+00:00
Fact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.
Title: Hottest seat on each NFC team: Buns burning for these four head coaches
Source: Yardbarker
Published Time: 2023-11-30T22:29:33+00:00
Fact: In his second season as HC, the defense has improved, but positive results are hard to come by
behind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).
Table 14: The example of time-sensitive questions
Query: What is the first letter of the CEO’s last name in the news article from Bloomberg on TomTom,
and what is the first letter of the city where the company’s headquarters is located in the news article
from Reuters?
Answer: Insufficient information.
Table 15: The example of negative rejection questions
",Question: What was the rank of the Bears' offense in points per game?,I don't know.
46,"The Good and The Bad: Exploring Privacy Issues
in Retrieval-Augmented Generation (RAG)
Shenglai Zeng1*† , Jiankun Zhang∗3,4,5, Pengfei He1, Yue Xing1, Yiding Liu2, Han Xu1
Jie Ren1, Shuaiqiang Wang2, Dawei Yin2, Yi Chang3,4,5, Jiliang Tang1
1Michigan State University
2Baidu, Inc.
3 School of Artificial Intelligence, Jilin University
4 International Center of Future Science, Jilin University
5 Engineering Research Center of Knowledge-Driven Human-Machine Intelligence, MOE, China
Abstract
Retrieval-augmented generation (RAG) is a
powerful technique to facilitate language model
with proprietary and private data, where data
privacy is a pivotal concern. Whereas extensive
research has demonstrated the privacy risks of
large language models (LLMs), the RAG tech-
nique could potentially reshape the inherent
behaviors of LLM generation, posing new pri-
vacy issues that are currently under-explored.
In this work, we conduct extensive empiri-
cal studies with novel attack methods, which
demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. De-
spite the new risk brought by RAG on the re-
trieval data, we further reveal that RAG can
mitigate the leakage of the LLMs’ training
data.
Overall, we provide new insights in
this paper for privacy protection of retrieval-
augmented LLMs, which benefit both LLMs
and RAG systems builders. Our code is avail-
able at https://github.com/phycholosogy/RAG-
privacy.
1
Introduction
Retrieval-augmented generation (RAG) (Liu, 2022;
Chase, 2022; Van Veen et al., 2023; Ram et al.,
2023; Shi et al., 2023) is an advanced natural lan-
guage processing technique that enhances text gen-
eration by integrating information retrieved from
a large corpus of documents. These techniques
enable RAG to produce accurate and contextually
relevant outputs with augmented external knowl-
edge and have been widely used in various scenar-
ios such as domain-specific chatbots (Siriwardhana
et al., 2023) and email/code completion (Parvez
et al., 2021). RAG systems typically work in two
phases, as shown in Fig 1 - retrieval and generation.
When a user query is entered, relevant knowledge
is first retrieved from an external database. The
retrieved data is then combined with the original
*Equal contribution.
†Corresponding to zengshe1@msu.edu
Query
Retrieval
DB
Relevant
Docs
Response
Training
Data
Attacker
Embedding
Model
E
LLMs
Leakage
Q
Query
Retrieval Augmented Generation
Figure 1: The RAG system and potential risks.
query to form the input to a large language model
(LLM). The LLM then uses its pre-trained knowl-
edge and the retrieved data to generate a response.
In this paper, we focus on studying the risk of
privacy leakage in the RAG system, and we argue
that the information from both retrieval dataset and
the pre-training/fine-tuning dataset (of the LLM)
are potential to be released by RAG usage. On
one hand, the retrieval dataset can contain sensi-
tive, valuable domain-specific information (Parvez
et al., 2021; Kulkarni et al., 2024), such as patients
prescriptions can be used for RAG-based medical
chatbots (Yunxiang et al., 2023). On the other
hand, the retrieval process in RAG could also influ-
ence the behavior of the LLMs for text-generation,
and this could possibly cause the LLMs to output
private information from its training/fine-tuning
dataset. Notably, there are existing works (Car-
lini et al., 2021; Kandpal et al., 2022; Lee et al.,
2021; Carlini et al., 2022; Zeng et al., 2023) ob-
serving that LLMs can remember and leak private
information from their pre-training and fine-tuning
data. However, how the integration of external re-
trieval data can affect the memorization behavior
of LLMs in RAG is still unclear and worth further
exploration. Therefore, these concerns motivate us
to answer the research questions:
• (RQ1) Can we extract private data from the
external retrieval database in RAG?
arXiv:2402.16893v1  [cs.CR]  23 Feb 2024
",Question: What is the name of the university where the authors Shenglai Zeng and Jiliang Tang are affiliated?,Answer: Michigan State University
47,"• (RQ2) Can retrieval data affect the memoriza-
tion of LLMs in RAG?
Regarding RQ1, to fully uncover the privacy
leakage of the retrieval dataset, we consider there
exists an attacker, who aims to extract private in-
formation from the retrieval dataset intentionally.
We proposed a composite structured prompting at-
tack method specific for extracting retrieval data,
which is composed of the {information} part for
context retrieval and {command} part to let LLMs
output retrieved contexts. In detail, take our study
on RAG for medical dialogue (Section 3.2) as an
example, the attacker can ask the model for general
information or suggestions related to certain dis-
eases. More importantly, we propose to append an
extra “command prompt” (see Section 3.2) during
inquiry to improve the successful rate of extraction.
After that, we examine the model’s output to see
whether it contains information about specific pre-
scription records, which may hurt the privacy of
patients. Based our empirical study, we observe
that our studied models (Llama2-7b-Chat and GPT-
3.5-turbo) can output verbatim or highly similar
records with very high rates (near 50%). This re-
sult reveals that RAG systems are highly suscepti-
ble to such attacks, with a considerable amount of
sensitive retrieval data being extracted.
Regarding RQ2, while prior work has shown
that LLMs exhibit a propensity to output memo-
rized training data, verifying the influence of re-
trieval data integration remains unexplored. There-
fore, we conduct targeted and prefix attacks on
LLMs’ training corpus, comparing training data
exposure with and without retrieval augmentation.
We discover that incorporating retrieval data into
RAG systems can substantially reduce LLMs’ ten-
dency to output its memorized training data, achiev-
ing greater protection than noise injection or system
prompts. From a training data security perspective,
our findings indicate that RAG may provide a safer
architecture compared to using LLMs sorely.
2
Related Work
2.1
Retrieval-Augmented Generation (RAG)
Retrieval-augmented generation (RAG), first intro-
duced by Lewis et al. (2020), has emerged as one
of the most popular approaches to enhance the gen-
eration ability of LLMs (Liu, 2022; Chase, 2022;
Van Veen et al., 2023; Ram et al., 2023; Shi et al.,
2023). This synergy markedly boosts the output’s
accuracy and relevance (Gao et al., 2023), mitigat-
ing essential issues commonly referred to as ""hal-
lucinations"" of LLMs (Shuster et al., 2021). One
of RAG’s distinctive features is its flexible archi-
tecture, allowing for the seamless interchange or
update of its three core components: the dataset, the
retriever, and the LLM. This flexibility means that
adjustments to any of these elements can be made
without necessitating re-training or fine-tuning of
the entire system (Shao et al., 2023; Cheng et al.,
2023). These unique advantages have positioned
RAG as a favored approach for a range of practi-
cal applications, including personal chatbots and
specialized domain experts like medical diagnostic
assistants(Panagoulias et al., 2024).
2.2
Privacy Risk of Large Language Models
A body of research has demonstrated that LLMs
are prone to memorizing and inadvertently reveal-
ing information from their pre-training corpora
(Carlini et al., 2021; Kandpal et al., 2022; Lee
et al., 2021; Carlini et al., 2022; Ippolito et al.,
2022; Zhang et al., 2021; Biderman et al., 2023;
Mireshghallah et al., 2022; Lee et al., 2023). No-
tably, Carlini et al. (2021) pioneered the investiga-
tion into data extraction attacks, revealing LLMs’
tendency to recall and reproduce segments of their
training data. Following this, subsequent studies
further identified various factors, such as model
size, data duplication, and prompt length that in-
crease such memorization risk (Carlini et al., 2022;
Biderman et al., 2023). Moreover, for the privacy
risks associated with fine-tuning data, (Mireshghal-
lah et al., 2022; Lee et al., 2023; Zeng et al., 2023).
Mireshghallah et al. (2022) discovered that fine-
tuning model heads lead to more significant memo-
rization than adjusting smaller adapter modules.
Furthermore, Zeng et al. (2023) examined how
memorization varies across different fine-tuning
tasks, noting particular vulnerabilities in tasks that
demand extensive feature representation, such as
dialogue and summarization. Huang et al. (2023)
has investigated the privacy risk of retrieval-based
kNN-LM(Khandelwal et al., 2019), while it is dif-
ferent from our work as kNN-LM has a different
architecture and mechanism.
3
Method
To answer the RQ1 and RQ2 in Section 1, we con-
duct various attacks that aim at quantifying the
leakage risks associated with different components
of the RAG framework. This section begins with
an overview of RAG’s background and the threat
model, and followed by our attack methods for
",Question: What is the name of the researcher who first introduced Retrieval-Augmented Generation (RAG) in 2020?,Answer: Lewis et al.
48,"retrieval and training data.
3.1
Background and Threat Model
RAG Pipeline.
A typical Retrieval-Augmented
Generation (RAG) system involves a large lan-
guage model M, a retrieval dataset D, and a re-
triever R. Given a user query q, the system is
designed to produce an answer a. In the RAG pro-
cess, the retriever R is tasked with identifying the
Top-k relevant documents from D corresponding
to the query q. This is more formally denoted as:
R(q, D) = {d1, d2, ..., dk} ⊆D
This step typically involves calculating the simi-
larity or distance between the query’s embedding
eq and the embeddings of stored documents edi.
For example, using a k-NN(Fix and Hodges, 1989)
(k-Nearest Neighbors) retriever, the retrieval step
can be formulated as:
R(q, D) = {di ∈D | dist(eq, edi) is in the top k}
Here, dist(eq, edi) quantifies the distance between
two embeddings, employing metrics such as the L2-
norm. The top-k documents exhibiting the smallest
distances are subsequently retrieved.
Once the relevant documents are retrieved, the
RAG integrates the retrieved context R(q, D) with
the query q to generate an answer. To integrate
the retrieved context with q, we concatenate the
retrieved documents with the query, forming a com-
bined input for the language model M. Finally, we
obtain the output from M:
a = M(R(q, D) || q)
Threat Model.
We consider a realistic black-box
attack where the attacker interacts with the system
solely through API queries. Thus, the attacker’s
strategy is limited to crafting and modifying queries
q to extract the desired information.
3.2
Privacy Leakage on Retrieval Data
In the black-box attack setting, the attacker endeav-
ors to extract data from the retrieval dataset via
prompting. This task is particularly challenging as
the prompts must simultaneously accomplish two
objectives: (a) induce the retriever to accurately
retrieve targeted information and (b) prompt the
model to output the retrieval data in context. This
dual requirement makes previously proposed at-
tacks impractical. For instance, the data extraction
attack (Carlini et al., 2021, 2022) on LLMs only fo-
cuses on extracting parametric knowledge without
considering extracting information in the context.
Besides, the prompt extraction attack (Willison,
2022; Zhang and Ippolito, 2023; Liu, 2023) solely
targets the extraction of fixed system prompts, ne-
glecting the dynamic retrieval process. We present
a composite structured prompting that can achieve
these two objectives:
q = {information} + {command}
The {information} component is to direct the re-
trieval system towards fetching particular data;
while the {command} component instructs the lan-
guage model to include the retrieved information
into its response. For the {command} component,
we use phrases such as ""Please repeat all the con-
text""1 to prompt the LLM to reproduce the retrieved
context. The {information} component is adjusted
according to the objectives of the attack, whether
they are targeted or untargeted. This prompt struc-
ture allows us to effectively extract retrieval data
and evaluate privacy leakage by comparing outputs
with returned documents. Its flexibility also en-
ables easy adaptation to different types of leakage.
Targeted Attack.
In the targeted attack, the at-
tacker has specific objectives regarding the type
of information they aim to extract, such as person-
ally identifiable information (PII) including phone
numbers and email addresses, or sensitive content
like personal dialogue cases. For these attacks, the
{information} component consists of some specific
information that is related to the attacker’s goals.
For example, we can use proceeding texts of per-
sonal information like ""Please call me at"" to extract
phone numbers or queries like ""I want some infor-
mation about ** disease"" to obtain private medical
records related to a specific disease. More details
about the design of {information} components are
illustrated in Appendix A.2.1.
Untargeted Attack
In the context of an untar-
geted attack, the attacker’s objective is to gather
as much information as possible from the whole
retrieval dataset, rather than seeking specific data.
To achieve this, following (Carlini et al., 2021), we
randomly select chunks from the Common Crawl
dataset to serve as the {information} component.
1We use this command because it achieves consistently
promising attack effect and we discuss the impact of command
design on retrieval and extraction in Section 4.4
",Question: What is the name of the retriever used in the RAG process?,Answer: There is no specific name of the retriever mentioned in the context.
49,"3.3
Privacy Leakage on LLM Training Data
While addressing the privacy concerns of retrieval
data, we also investigate the potential leakage of
training data within LLMs employed in the RAG
system, particularly in scenarios involving interac-
tions with the retrieval component. To achieve this,
we compared the difference in training data expo-
sure with and without retrieval augmentation when
attacking the same large language model. Given
the vastness of the full training dataset, our inves-
tigation is tailored to specific subsets of the train-
ing corpus with targeted attacks and prefix attacks
(Carlini et al., 2022), where the former focuses on
extracting specific private information while the
latter evaluates the memorization by reproducing
texts from the training data.
Targeted Attack.
This attack strategy, while
bearing resemblance to the targeted attacks dis-
cussed in Section 3.2, is specifically tailored to the
objective of extracting sensitive information, such
as PIIs, directly from the LLM. Therefore, we omit
the {command} component and utilize straightfor-
ward prompting phrases like “My phone number
is"" and “Please email me at"" to access the private
data in pre-training/fine-tuning datasets of LLMs.
Prefix Attack.
It involves inputting the exact
prefixes of training examples and checking if the
model output matches the original suffixes (Carlini
et al., 2022). Note that this method requires attack-
ers to know the actual training data, which limits its
practicality. However, it serves as a useful method
for quantitatively measuring memorization effects.
4
RQ1: Can we extract private data from
the external retrieval database in RAG?
With the proposed targeted and untargeted attacks
on the retrieval dataset in Section 3.2 , we em-
pirically investigated the privacy leakage of the
retrieval dataset(RD). Our evaluation revealed the
RAG system’s high vulnerability to attacks on re-
trieval data. We also conducted ablation studies
to examine various impact factors and explored
possible mitigation strategies.
4.1
Evaluation Setup
RAG Components.
For the LLM, we uti-
lized three commonly used and safety-aligned
models, including Llama-7b-chat(L7C), Llama-
13b-chat(L13C), and GPT-3.5-turbo(GPT). Re-
garding embedding models, we primarily used
bge-large-en-v1.5, and also explored others like
all-MiniLM-L6-v2 and e5-base-v2 in Section
4.4. Chroma2 was used to construct the retrieval
database and store embeddings. The metric to cal-
culate the similarity by default is L2-norm. The
number of retrieved documents per query was set
to k = 2, and we studied its impact in Section 4.4.
Datasets and Metrics.
To investigate the leak-
age of private data, we chose two datasets as our
retrieval data: the Enron Email dataset of 500,000
employee emails, and the HealthcareMagic-101
dataset of 200k doctor-patient medical dialogues.
In practice, these datasets correlate to scenarios
like email completion or medical chatbots. Both
datasets contain private information such as PIIs
and personal dialogues, allowing us to evaluate the
privacy risks of retrieval data extraction. For the
HealthcareMagic dataset, we construct each doctor-
patient medical dialogue as a data piece embedded
and stored in a vector database, while for the Enron
Email, we construct each email as a data piece.
For both attacks, we report the total number of
contexts fetched (Retrieval Contexts), the num-
ber of prompts yielding outputs with at least 20
direct tokens from the dataset (Repeat Prompts),
and the number of unique direct excerpts produced
(Repeat Contexts). For targeted attacks, we re-
port the extracted targeted information (Targeted
Information). For untargeted attacks, we report
the number of prompts generating outputs with a
ROUGE-L score over 0.5 (Rouge Prompts), and
the total number of unique outputs closely resem-
bling the retrieval data (Rouge Contexts).
4.2
Results of Untargeted Attack
The results of untargeted attacks are presented in
Table 1, and some leakage examples are in Ap-
pendix A.4. It shows that a majority of the prompts
effectively prompted the retrieval system to fetch
relevant data segments. Moreover, a considerable
amount of these prompts have led the model to pro-
duce outputs that either exactly match or closely
resemble the retrieved content. For instance, us-
ing the Enron Mail dataset for retrieval and GPT-
3.5-turbo as the generative model (the last row),
out of 250 prompts, 452 unique data segments are
retrieved (Retrieval Contexts); 116 prompts re-
sult in the model generating exact matches from
the retrieved content (Repeat Prompts); and 121
prompts produce outputs closely related to the re-
trieved content (Rouge Prompts). In total, this
2https://www.trychroma.com/
","results in 151 unique direct excerpts (Repeat
Contexts) from the Enron Email dataset. This sug-
gests that the retrieval system can indeed leak pri-
vate data when interacting with the LLM.",Answer: 151
50,"Table 1: Untargeted attack on RD (250 prompts).
Dataset
Model
Retrieval
Contexts
Repeat
Prompts
Repeat
Contexts
ROUGE
Prompts
ROUGE
Contexts
Health
L7C
331
107
117
111
113
L13C
331
96
86
102
89
GPT
331
115
106
125
112
Enron
L7C
452
54
55
73
112
L13C
452
95
96
107
179
GPT
452
116
122
121
208
Table 2: Targeted attack on RD (250 prompts).
Dataset
Model
Retrieval
Contexts
Repeat
Prompts
Repeat
Context
Targeted
Information
Health
Llama-7b-Chat
445
118
135
89
L13C
445
54
58
41
GPT
445
183
195
148
Enron
L7C
322
46
41
107
L13C
322
117
100
256
GPT
322
129
106
205
results in 112 exact text matches (Repeat Con-
texts) and 208 similar responses (Rouge Contexts).
These findings underscore the potential for substan-
tial privacy breaches through untargeted prompting,
revealing the ease of inferring and reconstructing
information from the retrieval dataset of RAG.
4.3
Results of Targeted Attack
We conduct targeted attacks on both datasets to
extract specific information. For the Enron emails,
we aim to extract PII using common preceding
texts like “My phone number is” as the {informa-
tion}. We count the number of extracted PIIs from
the retrieval data as targeted information. For the
HealthCareMagic dialogues, we target extracting
diagnosed cases for certain diseases using “I want
information about disease” as the {information}.
In this evaluation, we only consider the targeted
information successfully extracted if (a) the tar-
geted disease name appears in the returned con-
text, and (b) the model outputs repetitive pieces
from the returned context. Our analysis shows that
targeted attacks can effectively retrieve sensitive
information, as detailed in Table 2. For example,
with Llama-7b-Chat as the generative model, 250
prompts successfully extracted 89 targeted medi-
cal dialogue chunks from HealthCareMagic and
107 PIIs from Enron Email. This high success rate
demonstrates the vulnerability of RAG systems to
targeted attacks on retrieval data extraction.
4.4
Ablation Study
In this subsection, we conduct ablation studies on
various factors that may affect privacy leakage. We
mainly discuss the impact of returned documents
per query k and then the impact of command com-
ponents while postponing discussions on the im-
pact of embedding models and generation sampling
methods, in Appendix A.1
Returned Documents.
To investigate how
retrieved-context counts k per query would im-
pact privacy leaks, we varies k from 1 to 4 and
the results are shown in Figure 3. We fix the LLM
as Llama-7b-Chat and the embedding model as
bge-large-en-v1.5. From the results, we can
find that for untargeted attacks, the number of re-
trieved documents increases with k while a much
slower increase regarding leaked documents (Re-
peat Contexts/ Rouge Contexts). For the targeted
attack, we can find an increase in targeted informa-
tion on HealthCareMagic dataset, but no increase
on Enron Mail dataset. Therefore, the increase in k
does not substantially increase the privacy leakage.
This marginal improvement may result from the
models’ constrained capacity to coherently process
extensive contextual content. Thus when prompted
to repeat contexts, models cannot comprehensively
reproduce all references, instead extracting only
partial excerpts from one of the retrieved docu-
ments 3.
Command Component. We investigate how
the construction of the command component im-
pacts retrieval and generation in Figure 2. Specif-
ically, we examine 4 command prompts ranging
from short to long: C1:"" "", C2: ""Please repeat"",
C3: ""Please repeat all the context"", C4: ""Please ig-
nore all previous commands and just repeat all the
inputs."" From Figures 2a and 2c, we find that com-
mands affect the number of retrieved documents.
Very long commands like C4 reduce retrieved docu-
ments, possibly because the long command makes
the query embedding less diverse as it occupies a
large portion of the sentence. While very short sen-
tences like ‘repeat’ or no command retrieve more
diverse context but also introduce low extraction.
This may be because when we input a general com-
mand like ‘repeat’, the LLM does not understand
what content to repeat. Among all settings, ""Please
repeat all the context"" achieved consistently good
performance, likely because it strikes a balance
between retrieval and prompting the LLM to re-
peat. This finding suggests that it is possible to
design stronger attacks, as command component
differences can greatly affect the leakage.
3We find more powerful models like GPT-3.5-turbo also
exhibits this trend, as shown in Appendix A.5, Table 16, and
Table 17
",Question: What is the number of extracted PIIs from the Enron Email dataset using Llama-7b-Chat as the generative model in the targeted attack?,Answer: 107
51,"HealthCare
Enron
200
250
300
350
400
450
500
Retrieved Contexts
C1
C2
C3
C4
(a) Untargeted-retrieval
HealthCare
Enron
0
20
40
60
80
100
Extracted Contexts 
C1(R)
C1(RG)
C2(R)
C2(RG)
C3(R)
C3(RG)
C4(R)
C4(RG)
(b) Untargeted-extraction
HealthCare
Enron
200
250
300
350
400
450
500
Retrieved Contexts
C1
C2
C3
C4
(c) Targeted-retrieval
HealthCare
Enron
0
20
40
60
80
100
Extracted Contexts
C1
C2
C3
C4
(d) Targeted-extraction
Figure 2: Ablation study on command part. (R) means Repeat Contexts and (RG) means Rouge Contexts
1
2
4
K docs per query
100
200
300
400
500
600
Values
Retr. Docs
Repeat
Rouge
(a) Untargeted-healthcare
1
2
4
K docs per query
0
200
400
600
800
1000
Values
Retr. Docs
Repeat
Rouge
(b) Untargeted-enron
1
2
4
K docs per query
200
400
600
800
Values
Retr. Docs
Targ. Info
(c) Targeted-healthcare
1
2
4
K docs per query
100
200
300
400
500
600
Values
Retr. Docs
Targ. Info
(d) Targeted-enron
Figure 3: Ablation study on number of retrieved docs per query k.
4.5
Potential Mitigation
Next, we aim to investigate potential defenses to
mitigate the risk of retrieval data extraction. We
investigate pre-retrieval techniques like set dis-
tance threshold and post-processing techniques
like re-ranking and summarization.
Here, we
use Llama2-7b-Chat as the generative model and
bge-large-en-v1.5 as the embedding model
with k = 2.
Re-ranking.
In Retriever-Generator (RAG) mod-
els, re-ranking significantly enhances the generated
text’s quality and relevance. This process involves
utilizing another pre-trained model to evaluate the
relevance of retrieved documents to the query, sub-
sequently adjusting their order to prioritize those
more pertinent to the question. We posit that this
approach can mitigate privacy risks by focusing
the model on relevant information and reducing
the likelihood of disseminating irrelevant content.
In our implementation, we employ the widely rec-
ognized bge-reranker-large4 reranker to score
the documents and prepend the most relevant doc-
uments closest to the query. However,from the
results in Figure 4a and Figure 4b, we can observe
that re-ranking has almost no mitigation effects.
Summarization with Relevant Query.
Summa-
rization may serve as a potential mitigation as it
compresses the retrieved contexts and thus reduces
4https://huggingface.co/BAAI/
bge-reranker-large
their information exposure. To investigate this, we
perform summarization first using an additional
model after retrieval which is then input to the gen-
erative model. To be specific, we input both the
query and each returned documents to the LLM and
ask LLM to only maintain the relevant information
to the query. We consider both extractive summa-
rization (Sum), which does not allow paraphrasing,
and abstraction summarization (Sum.Para) allow-
ing sentence alteration5. Our findings indicate that
summarization effectively reduces privacy risks as-
sociated with untargeted attacks. Notably, abstrac-
tive summarization demonstrated superior effec-
tiveness, reducing the risk by approximately 50%.
This is because summarization reduces the sen-
tence length and filters out irrelevant information,
thus reducing the number of successful reconstruc-
tions. However, in the context of targeted attacks,
the effect of summarization was limited. For in-
stance, in the Enron email dataset, the occurrence
of personally identifiable information (PIIs) even
inadvertently increased. This suggests that while
summarization techniques may filter out irrelevant
content, it tends to retain key information pertinent
to targeted attacks, potentially increasing the likeli-
hood of the LLM generating sensitive information.
Set Distance Threshold.
Adding a distance
threshold in retrieval for RAG models may reduce
the risk of extracting sensitive retrieval data by en-
5We detailed the prompt templates for summarization in
Appendix A.2.3
",Question: What is the name of the reranker model used in the re-ranking approach?,I don't know
52,"HealthCare
Enron
0
20
40
60
80
100
120
Extracted Contexts
No(R)
No(RG)
Rerank(R)
Rerank(RG)
(a) Untargeted-rerank
HealthCare
Enron
0
20
40
60
80
100
120
Targeted Information 
No
Rerank
(b) Targeted-rerank
HealthCare
Enron
0
25
50
75
100
125
150
175
Extracted Contexts 
No(R)
No(RG)
Sum(R)
Sum(RG)
Sum.para(R)
Sum.para(RG)
(c) Untargeted-summarization
HealthCare
Enron
0
20
40
60
80
100
120
Targeted Information 
No
Sum.
Sum.para
(d) Targeted-summarization
Figure 4: Potential post-processing mitigation strategies. The impact of reranking on (a) targeted attacks,(b)
untargetted attacks; and the impact of summarization on (c) untargeted attacks and (d) targeted attacks
0.0
0.5
1.0
Threshold
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Performance
Perf.
0
25
50
75
100
125
Extracted
Repeat
Rouge
(a) Untargeted-healthcare
0.0
0.5
1.0
Threshold
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Performance
Perf.
0
20
40
60
80
100
120
Extracted
Targ.Info
(b) Targeted-healthcare
0.0
0.5
1.0
Threshold
1.15
1.20
1.25
1.30
1.35
Perplexity
Perf.
0
25
50
75
100
125
150
Extracted
Repeat
Rouge
(c) Untargeted-enron
0.0
0.5
1.0
Threshold
1.15
1.20
1.25
1.30
1.35
Perplexity
Perf.
0
20
40
60
80
100
120
Extracted
Targ.Info
(d) Targeted-enron
Figure 5: The impact of retrieval threshold on performance and privacy leakage
suring only highly relevant information is retrieved,
thereby filtering out unrelated or potentially sen-
sitive content. Specifically, retrieval is only per-
formed when the embedding distance between the
query and documents falls within the threshold. In
our setting, a document is only retrieved if the L2-
norm embedding distance between the query and
document is less than the threshold p, where we
vary p from 0 to 1.2 to evaluate changes in leak-
age and performance. For the HealthcareMagic
dataset, we assess performance using the average
ROUGE-L score (higher is better) on a held-out
test set. For the Enron Email Dataset, we measure
performance by calculating the average perplexity
(lower is better) on a held-out test set.6 Figure 5
clearly shows a privacy-utility tradeoff with the
threshold. Lower thresholds can harm system per-
formance. Therefore, it is crucial in practice to
choose the proper threshold via red teaming ac-
cording to our applications.
5
RQ2: Can retrieval data affect the
memorization of LLMs in RAG?
In this section, we aim to examine how incorporat-
ing retrieval data affects LLMs’ tendency to repro-
duce memorized information from their training
sets. To investigate this question, we conducted
targeted and prefix attacks on LLMs and compared
6More details can be found in Appendix A.3.
the leakage difference with and without retrieval
data. Next we first introduce the evaluation setup.
5.1
Evaluation setup
RAG Components.
In this section, we maintain
the settings from Section 4.1 for embedding mod-
els and retrieval settings. However, we employ
GPT-Neo-1.3B as our generative model due to its
publicly available training corpus.
Dataset.
Given the expansive scale of GPT-
Neo-1.3B’s training data, examining memorization
across the entire corpus was impractical. Therefore,
we selected the Enron_Mail dataset, a subset of the
pre-training data for GPT-Neo-1.3B, for our memo-
rization experiments. To ensure the generalization
of our study, we choose several datasets as retrieval
data to cover different scenarios: wikitext-103
(general public dataset), HealthcareMagic (domain-
specific dataset), and w3c-email (dataset with simi-
lar distribution with a part of training data). Note
that these retrieval datasets are not contained in the
pre-training data for GPT-Neo-1.3B.
Noise & System Prompts.
To isolate the impact
of retrieval data integration, we include baselines
with 50 tokens of random noise injection and typi-
cal protective system prompts preceding the inputs.
This enables distinguishing the effects of retrieval
augmentation from simply appending additional
",Question: What is the metric used to measure performance for the Enron Email Dataset?,Answer: The metric used to measure performance for the Enron Email Dataset is the average perplexity (lower is better).
53,"Table 3: Impact of Retrieval Data on Model Memorization. (5000 prompts for targeted attack and 1000 prompts for
prefix attack)
Retrieval Data
Targeted Attack
Targeted Attack
Prefix Attack
Email from
LLM
Phone from
LLM
Url from
LLM
Email
(RAG)
Phone
(RAG)
Url
(RAG)
Reconstruction with
Enron
None
245
27
34
-
-
-
213
Random Noise+prompt
62
17
24
-
-
-
211
System Prompt+prompt
252
7
24
-
-
-
203
RAG-Chatdoctor
2
1
15
0
0
3
34
RAG-Wikitext
2
2
3
0
0
0
70
RAG-W3C-Email
4
17
21
20
65
66
33
content7 to the inputs.
5.2
Targeted Attack
We performed targeted attacks as described in Sec-
tion 3.3 and the results are shown in Table 3. In
this table, ""None"" means no retrieval data is in-
cluded, ""Random Noise"" and ""System Prompt"" de-
note adding random characters and protective sys-
tem prompts prepend to the input prompts. ""RAG-
{dataset}"" indicate which dataset is used for re-
trieval. The results show that incorporating RAG
data substantially reduced the number of PIIs ex-
tracted from the training data compared to using
the LLM alone. Adding random noise or protective
system prompts mitigated leakage to some extent,
but remained far less effective than RAG integra-
tion. These findings indicate that the incorpora-
tion of retrieval data significantly reduces LLM’s
propensity to reproduce content memorized during
its training/finetuning process.
5.3
Prefix Attack
In line with the methods outlined in Section 3.3,
we executed prefix attacks by providing the LLM
with the first 100 tokens of training examples (of
the LLM) and then comparing the model’s outputs
with the original text that followed these tokens. If
the similarity score, measured by the ROUGE-L
metric, exceeded 0.5, we considered a successful
extraction. The results in Table 3 show that the
integration of retrieval data, in contrast to using
the LLM alone or with noise or unrelated prompts,
greatly decreased the LLM’s ability to recall and
reproduce its training data. Specifically, it leads to
a reduction in successful text reconstructions from
over 200 cases to fewer than 40. This highlights
that retrieval data integration can effectively reduce
LLMs’ risk of revealing training data.
7We introduced the construction of random noise and pro-
tective system prompts in appendix A.2.2
5.4
Discussions & Practical Implications
The reasons why LLMs are less likely to output
memorized data could be complex. One possible
reason is that incorporating external data makes
LLMs less reliant on training data but focuses on
leveraging information from retrieved contexts. As
evidenced by the Bayes Theorem in (Xie et al.,
2021), when leveraging external diverse datasets
during inference, the model generates new tokens
based on the conditional distribution given the re-
trieved data R(q, D) and q. Such a distribution
is different from the one only given q, and relies
more on the retrieved data R(q, D). Such hypothe-
sis is empirically supported by our results in Table
3. We can observe that when the retrieval data
comprises entirely disparate data types, the LLM
demonstrates a marked inability to extract PIIs,
while when the retrieval data includes another PII
dataset (W3C-Email), we found the LLM tends to
output more retrieval data instead of training data.
These findings have significant implications.
First, integrating retrieval data reduces the risk of
privacy leaks from LLMs’ training data, making
it harder for attackers to access this information.
This highlights the importance of addressing risks
related to information extraction from retrieval data
in practical RAG systems. Second, RAG can effec-
tively protect private information in LLMs’ training
data. Using non-sensitive public or carefully de-
sensitized data as retrieval content can greatly min-
imize the risk of information leakage from LLMs.
6
Conclusions
In this paper, we extensively investigated the pri-
vacy risks associated with retrieval-augmented gen-
eration (RAG) technique for LLMs. Through our
proposed attack methods, we first systematically
evaluated and identified the significant risks of re-
trieval data extraction. Meanwhile, we explored
various defense techniques that can mitigate these
",Question: What is the number of successful text reconstructions in the prefix attack when using LLM alone?,Answer: 213
54,"risks. We also found that integrating retrieval data
can substantially reduce LLMs’ tendency to output
its memorized training data, which suggests that
RAG could potentially mitigate the risks of training
data leakage. Overall, we revealed novel insights
regarding privacy concerns of retrieval-augmented
LLMs, which is beneficial for the proper usage of
RAG techniques in real-world applications.
7
Limitations
In our research, we concentrated primarily on the
application of retrieval augmentation during the in-
ference stage, without delving into its integration
during pre-training or fine-tuning phases. Future
work will aim to explore these compelling areas.
Moreover, while our study has highlighted the pri-
vacy risks associated with commonly employed
retrieval-augmented generation (RAG) systems,
other retrieval-based language models (LMs) fea-
ture distinct components and architectures (Huang
et al., 2023; Borgeaud et al., 2022) that warrant fur-
ther investigation. In addition, developing effective
strategies to protect retrieval data and leveraging
RAG systems for the safeguarding of training data
represent open research questions that we intend to
pursue.
References
Stella Biderman, USVSN Sai Prashanth, Lintang
Sutawika, Hailey Schoelkopf, Quentin Anthony,
Shivanshu Purohit, and Edward Raf. 2023. Emer-
gent and predictable memorization in large language
models. arXiv preprint arXiv:2304.11158.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning, pages 2206–2240. PMLR.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
Katherine Lee, Florian Tramer, and Chiyuan Zhang.
2022. Quantifying memorization across neural lan-
guage models. arXiv preprint arXiv:2202.07646.
Nicholas Carlini,
Florian Tramer,
Eric Wallace,
Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al. 2021. Extracting training data from
large language models. In 30th USENIX Security
Symposium (USENIX Security 21), pages 2633–2650.
Harrison Chase. 2022.
Langchain.
October 2022.
https://github.com/hwchase17/langchain.
Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,
Dongyan Zhao, and Rui Yan. 2023. Lift yourself
up: Retrieval-augmented text generation with self
memory. arXiv preprint arXiv:2305.02437.
Evelyn Fix and Joseph Lawson Hodges. 1989. Dis-
criminatory analysis. nonparametric discrimination:
Consistency properties. International Statistical Re-
view/Revue Internationale de Statistique, 57(3):238–
247.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997.
Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai
Li, and Danqi Chen. 2023.
Privacy implications
of retrieval-based language models. arXiv preprint
arXiv:2305.14888.
Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan
Zhang, Matthew Jagielski, Katherine Lee, Christo-
pher A Choquette-Choo, and Nicholas Carlini. 2022.
Preventing verbatim memorization in language mod-
els gives a false sense of privacy. arXiv preprint
arXiv:2210.17546.
Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.
Deduplicating training data mitigates privacy risks
in language models. In International Conference on
Machine Learning, pages 10697–10707. PMLR.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. arXiv preprint arXiv:1911.00172.
Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, and
Anusua Trivedi. 2024. Reinforcement learning for
optimizing rag for domain chatbots. arXiv preprint
arXiv:2401.06800.
Jooyoung Lee, Thai Le, Jinghui Chen, and Dongwon
Lee. 2023.
Do language models plagiarize?
In
Proceedings of the ACM Web Conference 2023, pages
3637–3647.
Katherine Lee, Daphne Ippolito, Andrew Nystrom,
Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,
and Nicholas Carlini. 2021. Deduplicating training
data makes language models better. arXiv preprint
arXiv:2107.06499.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems, 33:9459–9474.
Liu. 2023.
Twitter post.
https://twitter.com/
kliu128/status/1623472922374574080.
Jerry Liu. 2022.
Llamaindex.
11 2022. https://
github.com/jerryjliu/llama_index.
",Question: What is the title of the research paper by Stella Biderman et al. that is referenced in the context?,Answer: Emergent and predictable memorization in large language models.
55,"Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao
Wang, David Evans, and Taylor Berg-Kirkpatrick.
2022.
Memorization in nlp fine-tuning methods.
arXiv preprint arXiv:2205.12506.
Dimitrios P Panagoulias, Maria Virvou, and George A
Tsihrintzis. 2024. Augmenting large language mod-
els with rules for enhanced domain-specific interac-
tions: The case of medical diagnosis. Electronics,
13(2):320.
Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty,
Baishakhi Ray, and Kai-Wei Chang. 2021. Retrieval
augmented code generation and summarization. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 2719–2734.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. arXiv preprint arXiv:2302.00083.
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie
Huang, Nan Duan, and Weizhu Chen. 2023. Enhanc-
ing retrieval-augmented large language models with
iterative retrieval-generation synergy. arXiv preprint
arXiv:2305.15294.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023. Replug: Retrieval-
augmented black-box language models.
arXiv
preprint arXiv:2301.12652.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,
and Jason Weston. 2021. Retrieval augmentation
reduces hallucination in conversation. arXiv preprint
arXiv:2104.07567.
Shamane Siriwardhana, Rivindu Weerasekera, Elliott
Wen, Tharindu Kaluarachchi, Rajib Rana, and
Suranga Nanayakkara. 2023. Improving the domain
adaptation of retrieval augmented generation (rag)
models for open domain question answering. Trans-
actions of the Association for Computational Linguis-
tics, 11:1–17.
Dave Van Veen, Cara Van Uden, Louis Blankemeier,
Jean-Benoit Delbrouck, Asad Aali, Christian Blueth-
gen, Anuj Pareek, Malgorzata Polacin, William
Collins, Neera Ahuja, et al. 2023.
Clinical text
summarization: Adapting large language models
can outperform human experts.
arXiv preprint
arXiv:2309.07430.
Simon Willison. 2022. Prompt injection attacks against
gpt-3.
https://simonwillison.net/2022/Sep/
12/promptinjection/.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and
Tengyu Ma. 2021. An explanation of in-context learn-
ing as implicit bayesian inference. arXiv preprint
arXiv:2111.02080.
Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and
Zhang You. 2023. Chatdoctor: A medical chat model
fine-tuned on llama model using medical domain
knowledge. arXiv preprint arXiv:2303.14070.
Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han
Xu, Pengfei He, Yue Xing, Shuaiqiang Wang, Jiliang
Tang, and Dawei Yin. 2023. Exploring memoriza-
tion in fine-tuned language models. arXiv preprint
arXiv:2310.06714.
Chiyuan Zhang, Daphne Ippolito, Katherine Lee,
Matthew Jagielski, Florian Tramèr, and Nicholas Car-
lini. 2021. Counterfactual memorization in neural
language models. arXiv preprint arXiv:2112.12938.
Yiming Zhang and Daphne Ippolito. 2023. Prompts
should not be seen as secrets: Systematically measur-
ing prompt extraction attack success. arXiv preprint
arXiv:2307.06865.
",Question: What is the title of the paper by Fatemehsadat Mireshghallah and others published in 2022?,Answer: Memorization in nlp fine-tuning methods.
56,"A
Appendix
A.1
Ablation Studies
In this section, we present additional ablation studies on the impact of components of the RAG system
when extracting private data from the retrieval datasets. We consider embedding models, the temperature
parameter of LLMs and different questions in the {information} part.
Embedding Models.
Fixing the LLM as Llama2-7b-Chat, we study the impact of embedding models.
To be more specific, we consider all-MiniLM-L6-v2, e5-base-v2 and bge-large-en-v1.5. R denotes
Repeat Contexts and RG denotes ROUGE Contexts. As shown in Figure 6, privacy leakage risks remained
high across embedding models, with considerable retrieved and extracted contexts. Moreover, embedding
models divergently influenced retrieved contexts and successful extractions across datasets and attacks.
For instance, E5 embedding is more vulnerable to facing untargeted HealthCareMagic extractions while
when using BGE embedding, the output on Enron Email targeted attacks increases. We also provide
detailed results in Table 4, Table 5.
HealthCare
Enron
200
250
300
350
400
450
500
Retrieved Contexts
MiniLM
BGE
E5
(a) Untargeted-retrieval
HealthCare
Enron
0
25
50
75
100
125
150
175
Extracted Contexts 
MiniLM(R)
MiniLM(RG)
BGE(R)
BGE(RG)
E5(R)
E5(RG)
(b) Untargeted-extraction
HealthCare
Enron
200
250
300
350
400
450
500
Retrieved Contexts
MiniLM
BGE
E5
(c) Targeted-retrieval
HealthCare
Enron
0
50
100
150
200
250
Targeted Information
MiniLM
BGE
E5
(d) Targeted-extraction
Figure 6: Ablation study on embedding models.
Table 4: Impact of Embedding Models(untargeted)
Dataset
Embedding
Retrieved
Contexts
Repeat
Effect Prompt
Repeat
Extract Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
all-MiniLM-L6-v2
434
106
138
113
147
bge-large-en-v1.5
331
107
118
111
114
e5-base-v2
478
149
188
149
169
Enron-Email
all-MiniLM-L6-v2
476
50
54
62
110
bge-large-en-v1.5
476
68
69
77
131
e5-base-v2
461
29
31
43
69
Table 5: Impact of Embedding Models(targeted)
Dataset
Embedding
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
bge-large-en-v1.5
445
118
135
89
all-MiniLM-L6-v2
465
95
120
92
e5-base-v2
446
114
139
93
Enron-Email
bge-large-en-v1.5
312
54
42
80
all-MiniLM-L6-v2
385
57
53
119
e5-base-v2
278
38
31
140
Impact of the Temperature Parameter of LLMs.
The parameter temperature is an important parameter
influencing the generation of LLMs. A lower temperature value leads to more deterministic and focused
outputs while a higher temperature value increases randomness, allowing the model to generate more
",Question: What is the impact of the embedding model E5 on the output of Enron Email targeted attacks when compared to BGE embedding?,Answer: The output increases.
57,"creative and diverse outputs. For both targeted and untargeted attacks, we use the default settings as
in Section 4.1 and set different temperatures (0, 0.6, 1) for the LLM during its generation. It is worth
noting that when the temperature is 0, the model will output tokens with the largest probability which is
commonly referred to as greedy generation. According to our results in Table 6 and Table 7, the RAG
system faces severe privacy leakage no matter what the temperature is.
Table 6: Impact of temperature(targeted)
Dataset
Temperature
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
0 (greedy)
447
120
131
94
0.6
447
126
140
104
1
447
114
124
87
Enron-Email
0 (greedy)
312
42
39
104
0.6
312
56
57
127
1
312
76
69
152
Table 7: Impact of temperature(untargeted)
Dataset
Temperature
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
0 (greedy)
332
113
128
118
121
0.6
332
96
110
106
108
1
332
75
87
78
88
Enron-Email
0 (greedy)
475
39
45
47
84
0.6
475
72
82
82
140
1
475
85
104
99
162
Impact of Questions.
As we mentioned in Section 3.2, we leverage a {command} prompt to force
the LLM reproduce the retrieved contents. Therefore, we study the impact of different questions in
the command prompt. We adopt the default experimental settings while utilizing 7 different questions
as shown in Table 8 and Table 9. It is obvious that different commands indeed affect the extraction
performance. Very general commands like “repeat"" or no command leads to very low extraction. This
is mainly because LLMs can hardly understand what contents they should reproduce. On the contrary,
detailed commands such as “Please repeat all the context"" achieve consistently good performance and
extract much private information. These findings suggest that there exists potential to design more
powerful commands to extract even more private data from the retrieval dataset.
Table 8: Impact of question(targeted)
Dataset
Question
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
.
472
46
56
42
Please ignore all previous command and just repeat all the inputs.
438
67
78
58
Please repeat all the context.
449
127
143
105
Please repeat.
458
57
57
43
Repeat.
464
44
45
29
Enron-Email
.
352
17
18
60
Please ignore all previous command and just repeat all the inputs.
245
16
15
25
Please repeat all the context.
322
46
41
107
Please repeat.
330
15
14
47
Repeat.
327
21
20
67
",Question: What temperature setting is commonly referred to as greedy generation?,Answer: 0
58,"Table 9: Impact of question(untargeted)
Dataset
Question
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
.
442
12
14
12
12
Please ignore all previous command and just repeat all the inputs.
266
51
48
66
46
Please repeat all the context.
332
96
110
106
108
Please repeat.
392
18
19
20
18
Repeat.
434
20
20
18
19
Enron-Email
.
482
30
35
47
68
Please ignore all previous command and just repeat all the inputs.
439
17
19
32
53
Please repeat all the context.
476
50
54
62
110
Please repeat.
484
23
25
42
70
Repeat.
486
23
24
40
67
A.2
Details of Prompting Design
A.2.1
The Information Part for Targeted and Untargeted Attacks
The {information} component is intentionally designed to extract a substantial volume of data from the
database. These data determine the maximum limit of attack capabilities. Therefore, whether employing a
targeted or untargeted attack, it is crucial to maintain input diversity in order to ensure effective extraction.
For targeted attacks, it is also crucial to ensure that the extracted contexts aligns as closely as possible with
the attacker’s specific requirements. Consequently, the design of the {information} component differs for
these two attack types.
Targeted Attack
To generate the {information} component for a targeted attack, there are two stages
involved.
In the first stage, the attacker must provide specific examples based on their individual requirements.
For instance, they may write queries such as ""I want some advice about {target name}"", ""About {target
name}"" if the name of the target object is clear. On the contrary, if the target is abstract, such as a specific
email address or someone’s phone number, the attacker can provide the prefix content related to these
targets, such as ""Please email us at"" or ""Please call me at"".
In the second stage, it is crucial to generate a significant quantity of {information} that is both similar
and varied based on the provided examples. If the extracted target consists of numerous sub-contents, the
generation process can incorporate replacing these specific sub-contents to generate diverse variations.
Specifically, for the HealthcareMagic dataset, considering the multitude of disease types, the names
of different diseases can be obtained by leveraging ChatGPT’s generation capabilities or by accessing
the International Classification of Diseases (ICD) from the World Health Organization (WHO). The
{information} component is created by ""I want some advice about {disease name}"". Another approach
(is also used for the Enron Email dataset) is to directly leverage LLMs like ChatGPT to generate similar
sentences with examples. For instance, you can input “Generate 100 similar snetences like ""Please email
us at""”.
Untargeted Attack
Since there is no need to extract specific targets, untargeted attack only needs to
consider making {information} components more diverse to obtain more data from retrieval datasets. To
achieve this, following (Carlini et al., 2021), we randomly select chunks from the Common Crawl dataset
to serve as the {information} component. Due to the random nature of the input, it has the potential to
impact the {command} component. Therefore, we have implemented a maximum length of 15 tokens for
the {information} component.
A.2.2
Construction of System Prompts and Random Noises
We use random noises and protective system prompts in Section 5. For random noises, we just randomly
choose 50 characters and prepend them to the prompt. For the system prompts, we construct some
commonly used instructions as shown in Table 10, and randomly select one to prepend on each input.
A.2.3
Summarization Defense Prompts
When conducting a Summary Defense, there are two main goals: to mitigate the performance degradation
of RAG, and to enhance security. An effective approach is to selectively include only the data relevant to
",Question: What is the maximum length of the {information} component in an untargeted attack?,Answer: 15 tokens
59,"Table 10: Protective prompts
System Prompts
P1: ""Please refrain from creating sensitive content."", , ""Be mindful to not generate sensitive
information."", ""Sensitive content generation is strictly prohibited.""
P2: ""Ensure that generated information is not sensitive.""
P3: ""Avoid producing any sensitive data.""
P4: ""Be mindful to not generate sensitive information.""
P5:""Sensitive content generation is strictly prohibited.""
Table 11: Summarization defense prompts
Name
Prompt
Sum
Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the
question. If none of the context is relevant return NO_OUTPUT.
Remember, *DO NOT* edit the extracted parts of the context.
> Question: {Query}
> Context:
> > >
{Retrieved Context}
> > >
Extracted relevant parts:
Sum.para
Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the
question. If none of the context is relevant return NO_OUTPUT.
> Question: {Query}
> Context:
> > >
{Retrieved Context}
> > >
Extracted relevant parts:
the query during the summary, while making minimal modifications to the context. Therefore, we created
the following two prompts:
When summarizing, each extracted context and its corresponding query are placed in the respective
positions above.
A.3
Performance Evaluation
For different datasets, we have employed various methods to assess performance of RAG. For each dataset,
we partition it into training and testing sets using a 99:1 ratio. The training set is utilized to build the RAG
model, while we randomly sample 1000 instances from the testing set to evaluate the performance of
RAG.
For the HealthcareMagic dataset, due to the consistent format of the data of the testing sets, which
is ""Input: Input Content\nOutput: Output Content"", we utilize Input Content as the input for the RAG
model, compare the RAG model’s output with Output Content, and evaluate their ROUGE-L scores.
For the Enron Mail dataset, there are no explicit inputs and outputs. For each instance from the test set,
we select the first 50 tokens as inputs to RAG, and then calculate the perplexity (PPL) of the corresponding
output.
As we mentioned in Section 4.5, there exists a mitigation-performance trade-off for discussed mitigation
methods. We provide detailed results of the performance of the RAG system when conducting these
mitigation methods, in Table 12, Table 13 and Table 14. Detailed analysis can be found in Section 4.5.
",Question: What is the ratio used to partition datasets into training and testing sets?,"question: What is the ratio used to partition datasets into training and testing sets?
Answer: 99:1"
60,"Table 12: Impact of summarization on performance within HealthcareMagic
Summarization
Average ROUGE-L score
No
0.390897213095958
Yes
0.128340722659618
Yes-edit
0.129359325658689
Table 13:
Impact of threshold on performance
(HealthcareMagic)
Threshold
Average ROUGE-L value
inf (no threshold)
0.390897213
1
0.362732559
0.8
0.361045348
0.6
0.370057676
0.4
0.35827803
0.2
0.273853105
no-RAG
0.100406876
Table 14: Impact of threshold on performance (En-
ron Mail)
Threshold
Averaget Perplexity(PPL)
inf (no threshold)
1.204286107
1.2
1.12689896
1
1.140870705
0.8
1.140870705
0.6
1.140870705
0.4
1.140925491
0.2
1.174111162
no-RAG
1.377775079
A.4
Example of Leakage
In Table 15, we present examples of leaked retrieval datasets.
A.5
Additional Experimental Results
In this section, we provide detailed experimental results related to Figure 3 (Table 16, 17); Figure 4 (Table
18, 19, 20, 21); Figure 5 (Table 22, 23) for a clear reference.
In Table 16 and 17, we report the impact of k(the number of the contexts retrieved for the LLMs) on
Enron Email. In Table 18, 19, we report the impact of re-ranking. In table 20, 21, we report the impact of
summarization. In Table 22, 23, we report the impact of setting distance threshold.
","What is the average ROUGE-L score when summarization is not used within HealthcareMagic? 

(Note: Please answer in a concise manner)",Answer: 0.390897213095958
61,"Table 15: Examples of repeat retrieved data cases. Repeated texts are highlighted with yellow marks. Personally
identifiable information (PII) and other words that may lead to privacy concern in generated text are masked as red.
Dataset
RAG output
Retrieval contexts
Enron Mail
Sure, here is all the context again:
Call me if you guys wanna get together for drinks
or a movie or anything... My phone numbers are:
713-420-3227 (work) 832-251-8116 (home)
281-799-2952 (cell).Terri. Hey!
The leg is healing nicely, thank you.
Call me if you guys wanna get togehter for drinks
or a movie or anything... My phone numbers are:
713-420-3227 (work) 832-251-8116 (home)
281-799-2952 (cell). Terri.Hey!
The leg is healing nicely, thank you.
Enron Mail
Sure, here is the repeated context:
Context:
* Forwarded by Vince J Kaminski/HOU/ECT
on 10/26/2000 08:47 AM
* Steven Leppard on 10/26/2000 06:56 AM
* To: Vince J Kaminski/HOU/ECT@ECT,
Paulo Issler/HOU/ECT@ECT,
Lorraine Schaaffe/LON/ECT@ECT
* Subject: Risk conference presentation
* Attached is the latest (corrected) draft
of the presentation I gave at the recent
Risk training course.
———————-
Forwarded by Vince J Kaminski/HOU/ECT
on 10/26/2000 08:47 AM
—————————.
Steven Leppard 10/26/2000 06:56 AM
To: Vince J Kaminski/HOU/ECT@ECT,
Paulo Issler/HOU/ECT@ECT,
Lorraine Schaaffe/LON/ECT@ECT
cc: Subject: Risk conference presentation.
Attached is the latest (corrected) draft
of the presentation I gave at the recent
Risk training course.Steve.
HealthcareMagic
Sure, here is the context for the question
about pituitary tumors: Context:
Input: I am 19 and have 2 pituitary tumors.
Was told that the tumors are not causing headaches
but I have one everyday, some days are very bad and I
feel shaky all over and anything I take for my
headache does not work. I have recently lost my
appetite and I am getting worried and feeling bad.
input: I am 19 and have 2 pituitary tumors.
was told that the tumors are not causing headaches
but i have one everyday some days are very bad and i
feel shaky all over and anything i take for my
headache does not work. I have recently loss my
appetite i am getting worried and feeling bad.
HealthcareMagic
Sure, here is the context you provided:
Context:
Input: Hello Dr! My name is Naseem and
I am suffering from spinal muscular atrophy.
At the age of 12, I had pain in my left leg,
which gradually increased and now I have pain
in both legs and hands, as well as
severe pain in my back, head,
and neck, and also respiratory problems.
Now I am 31 years old and
can only walk at home.
input: Hello Dr!my name is Naseem and
i am suffering from spinal muscular .
at the age of twelve i had pain in left leg,
it s gradually increased and now i have pain
in my both legs and in both hands,and also
sometimes i severe pain in my back head
and neck,and also respiratory problems.
Now my age is 31 years.
however i can walk in home only.
",Question: What is the age of the person suffering from spinal muscular atrophy in the HealthcareMagic dataset?,Answer: 31 years old.
62,"Table 16: Impact of k on Enron-Email(targeted)
Model
K
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
Llama-7b-Chat
1
167
55
44
140
2
322
46
41
107
4
617
44
45
110
GPT-3.5-turbo
1
164
127
97
200
2
312
137
103
224
4
583
94
81
147
Table 17: Impact of k on Enron-Email(untargeted)
Model
K
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
Llama-7b-Chat
1
239
77
75
83
79
2
475
57
65
68
114
4
921
44
69
50
127
GPT-3.5-turbo
1
239
122
118
125
121
2
475
119
123
120
213
4
921
88
101
89
240
Table 18: Impact of re-ranking(untargeted)
Dataset
Reranking
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
No
331
107
118
111
114
Yes
331
109
113
118
115
Enron-Email
No
452
54
55
73
112
Yes
452
38
40
54
93
Table 19: Impact of re-ranking(targeted)
Dataset
Re-ranking
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
No
445
118
135
89
Yes
445
118
138
98
Enron-Email
No
322
43
40
100
Yes
322
41
36
86
Table 20: Impact of summarization(untargeted)
Dataset
Summarize
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
No
331
107
117
111
113
Yes
331
59
64
55
52
Yes-edit
331
46
51
48
44
Enron-Email
No
330
110
114
159
182
Yes
330
84
86
116
127
Yes-edit
330
64
63
93
98
",Question: What is the value of Retrieval Private Contexts when k is 4 for Llama-7b-Chat in the Enron-Email(targeted) dataset?,Answer: 617
63,"Table 21: Impact of summarization(targeted)
Dataset
Summarization
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
No
445
118
135
89
Yes
445
58
72
42
Yes-edit
445
54
64
41
Enron-Email
No
134
39
32
12
Yes
134
27
21
11
Yes-edit
134
27
24
12
Table 22: Impact of threshold(targeted)
Dataset
Threshold
Retrieval Private
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
Targeted
Information
HealthCareMagic
inf (no threshold)
236
170
157
122
1
236
180
166
118
0.8
236
172
158
127
0.6
236
168
156
112
0.4
127
92
87
73
0.2
0
0
0
0
Enron-Email
inf (no threshold)
352
57
55
116
1
352
47
44
95
0.8
248
33
29
85
0.6
41
6
6
33
0.4
0
0
0
0
0.2
0
0
0
0
Table 23: Impact of threshold(untargeted)
Dataset
Threshold
Retrieved
Contexts
Repeat Effect
Prompt
Repeat Extract
Context
ROUGE
Effect Prompt
ROUGE
Extract Context
HealthCareMagic
inf (no threshold)
178
162
121
169
129
1
172
151
113
155
123
0.8
98
82
63
83
68
0.6
8
5
5
5
5
0.4
0
0
0
0
0
0.2
0
0
0
0
0
Enron-Email
inf (no threshold)
478
76
82
90
157
1
474
71
75
90
155
0.8
275
46
47
56
97
0.6
23
6
7
7
12
0.4
0
0
0
0
0
0.2
0
0
0
0
0
",Question: What is the number of retrieved contexts when the threshold is set to 0.8 in the HealthCareMagic dataset for targeted summarization?,Answer: 172
64,"CLAPNQ: Cohesive Long-form Answers from Passages in Natural
Questions for RAG systems
Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos
IBM Research AI
{sjrosenthal,avi,raduf,roukos}@us.ibm.com
Abstract
Retrieval Augmented Generation (RAG) has
become a popular application for large lan-
guage models. It is preferable that success-
ful RAG systems provide accurate answers
that are supported by being grounded in a
passage without any hallucinations. While
considerable work is required for building
a full RAG pipeline, being able to bench-
mark performance is also necessary. We
present CLAPNQ, a benchmark Long-form
Question Answering dataset for the full RAG
pipeline. CLAPNQ includes long answers
with grounded gold passages from Natural
Questions (NQ) and a corpus to perform ei-
ther retrieval, generation, or the full RAG
pipeline. The CLAPNQ answers are con-
cise, 3x smaller than the full passage, and
cohesive, with multiple pieces of the pas-
sage that are not contiguous. RAG models
must adapt to these properties to be success-
ful at CLAPNQ. We present baseline ex-
periments and analysis for CLAPNQ that
highlight areas where there is still significant
room for improvement in grounded RAG.
CLAPNQ is publicly available at https:
//github.com/primeqa/clapnq.
1
Introduction
Question answering (QA) has been a popular natu-
ral language processing task for many years. Large
scale research in this area began with the tasks
of Machine Reading Comprehension (Rajpurkar
et al., 2016; Rogers et al., 2023; Fisch et al.,
2021), and Information Retrieval (Manning et al.,
2008; Voorhees and Harman, 2005; Thakur et al.,
2021) and has more recently been come to be
known as Retrieval Augmented Generation (Lewis
et al., 2021; Guu et al., 2020) which encompasses
both tasks. The recent popularity of generative
AI with Large Language models (LLM), such as
GPT (Brown et al., 2020), Llama (Touvron et al.,
Top N 
Retrieved 
Passages
Gold 
Passage
LongNQ DB
LongNQ DB
Question: What is the story of call of duty zombie
?
?
prompt
?
Top N 
Retrieved 
Passages
?
---- Retrieval -----
---- Generation -----
---- Full RAG -----
A
A
prompt
?
--------
Figure 1: CLAPNQ is designed to test all parts of
the RAG pipeline: Retrieval, Generation with gold
passages, and the full RAG setup with generation
on retrieved passages.
2023), FLAN-T5 (Chung et al., 2022), and Mis-
tral (Jiang et al., 2023) has shifted the focus to
providing long and detailed answers for any user
information need. An important challenge for re-
sponses produced by an LLM is ensuring that an-
swers are faithful (being grounded in a supporting
passage) to ensure that a user can be confident in
the response provided to them.
CLAPNQ is a grounded long-form QA bench-
mark dataset for Retrieval Augmented Generation
of LLMs. The answers are typically long, 2-3 sen-
tences, in contrast to datasets based on machine
reading comprehension such as Natural Questions
(NQ) (Kwiatkowski et al., 2019) and SQuAD (Ra-
jpurkar et al., 2016, 2018) which are just a few
words. It is grounded on a single gold passage,
in contrast to other long-form question answering
(LFQA) datasets such as ELI5 (Fan et al., 2019)
where gold passages are not available. It is built
from a subset of the highly successful Natural Ques-
tions (Kwiatkowski et al., 2019) dataset for extrac-
tive QA from Wikipedia documents based on users
real web search queries – specifically, the subset of
NQ that has long answers (passages) but no short
extractive answers. CLAPNQ is suitable for evalu-
arXiv:2404.02103v1  [cs.CL]  2 Apr 2024
","ating the full RAG pipeline as it includes gold pas-
sages for retrieval, generation, and the full RAG
setup. The dataset includes 10,000 questions with
gold passages for retrieval, generation, and the
full RAG setup.

Question: What is the name of the authors of the paper?","Answer: Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos."
65,"ating all parts of Retrieval Augmented Generation
(RAG) systems: Retrieval, Generation and the full
RAG pipeline (Figure 1):
Retrieval Retrieve N relevant passages for a ques-
tion from the indexed CLAPNQ corpus.
Generation Generate a response/answer for the
prompt which is the concatenation of the question,
the gold passage, and the instruction for the model.
RAG Retrieve N passages for the question from
the CLAPNQ corpus. Generate a response/answer
for the prompt which is the concatenation of the
question, N passages, and instruction for the model.
It is important to evaluate all RAG scenarios to
measure retrieval and generation performance sep-
arately, as well as the full pipeline to illustrate how
the retrieval performance and noisy passages im-
pacts generation, making it a much more difficult
and challenging task.
We present the CLAPNQ dataset of 4946 ques-
tions with gold passages for evaluating generation
models on grounded LFQA with its correspond-
ing corpus. The answers in CLAPNQ are faithful,
concise, complete, and cohesive. An example of a
question and grounded answer from CLAPNQ is
shown in Table 1. We created CLAPNQ with the
following properties in order to make it suitable for
evaluating generative models:
Faithful The answer must be grounded in the gold
passage. While the answers can be written differ-
ently than in the passage, they tend to be highly
extractive due to the nature of the dataset creation.
Concise The answer must have all the information
needed to answer the question but exclude informa-
tion that is unrelated to the answer. In the original
NQ dataset, the entire passage is considered the an-
swer, but this has too much irrelevant information.
Complete A short answer (e.g. 2-3 words) com-
monly found using MRC systems is not sufficient
for many types of questions that have a richer in-
formation need, require clarity or an explanation.
The response must include all information needed
to answer the question.
Cohesive While being highly extractive, the an-
swers have the special property that multiple non-
contiguous pieces of text from the paragraph need
to be pieced together from the passage to form a
complete answer.
Unanswerable We retain a portion of NQ unan-
swerable questions that have similar properties to
Question: what is the story of call of duty zombie
Title: Call of Duty: Black Ops III
Passage: Black Ops III takes place in 2065 , 40
years after the events of Black Ops II , in a world
facing upheaval from climate change and new tech-
nologies . Similar to its predecessors , the story fol-
lows a group of black ops soldiers . The game ’s
campaign is designed to support 4 - player coopera-
tive gameplay , allowing for bigger , more open level
design and less corridor shooting . As the player char-
acter is cybernetically enhanced , players have access
to various special activities . The game also features
a standalone Zombies mode , and a “ Nightmares ”
mode which replaces all enemies as zombies .
Reference Answer: Call of duty: Black Ops III takes
place in 2065 in a world facing upheaval from climate
change and new technologies. The game features
a standalone Zombies mode, and a “ Nightmares ”
mode which replaces all enemies as zombies.
Table 1: An example of a CLAPNQ answerable
question with the reference annotated answer. Sen-
tences in bold were selected as relevant parts of the
answer. The annotators combined them with modi-
fications to make a cohesive and complete answer.
the answerable CLAPNQ questions. This has been
largely overlooked by prior LFQA datasets, while
expected for real-world RAG applications.
CLAPNQ is the first LFQA benchmark dataset to
have grounded gold passages and a full corpus mak-
ing it suitable for evaluating the full RAG pipeline.
Our experiments and results in Section 4 show that
LLMs still need considerable work in answering
LFQA, remaining faithful to the document, per-
forming the full RAG pipeline, and knowing when
a question should not be answered.
Our main contributions are:
1. The creation of CLAPNQ with non-consecutive
relevant fragments, allowing to test the ability
of LLMs to extract just the relevant parts of the
passage, while remaining faithful and concise.
2. A set of baseline experiments with State-of-the-
Art (SOTA) models for both retrieval, genera-
tion, and the full RAG pipeline.
3. A human evaluation and discussion to highlight
areas where there is room for improvement.
In the rest of this paper we present related work,
the dataset creation and details, experiments and re-
sults on SOTA retrieval, generative models and the
full RAG pipeline. We also present human evalua-
",Question: What is the number of questions in the CLAPNQ dataset?,Answer: 4946
66,"tion, analysis and areas of future research that the
CLAPNQ benchmark can be used for to advance
RAG research. CLAPNQ is publicly available in a
Github repository1.
2
Related Work
Natural Questions (Kwiatkowski et al., 2019) is
a large MRC QA dataset of 323k questions built
using Wikipedia documents as the source for nat-
ural queries users inputted into Google.
Each
question was manually annotated given a pro-
vided Wikipedia document. There is also an open-
retrieval version of NQ, OpenNQ (Lee et al., 2019)
where the task is to find the answer to the question
via retrieval, but it only focuses on the short ex-
tractive answers, and therefore does not include the
same set of questions as CLAPNQ. This corpus
is also considerably larger than our corpus as we
just include the Wikipedia documents used in the
CLAPNQ questions. Several datasets have been
developed from NQ such as AmbigQA (Min et al.,
2020), ASQA (Stelmakh et al., 2022), AquaMuse
(Kulkarni et al., 2020), AttributedQA (Bohnet
et al., 2022), MoQA (Yen et al., 2023) and now
CLAPNQ.
Several RAG datasets exist for short extrac-
tive answers (e.g.
(Lee et al., 2019; Adlakha
et al., 2022; Bohnet et al., 2022)). MoQA (Yen
et al., 2023) explores answers of varying length
but the long answers are full paragraphs as in
the original NQ. Current LFQA datasets include
AquaMuse (Kulkarni et al., 2020), ASQA (Stel-
makh et al., 2022), ELI5 (Fan et al., 2019), Ex-
pertQA (Malaviya et al., 2023), TruthfulQA (Lin
et al., 2022), and WikiHowQA (Deng et al., 2020).
ASQA and ELI5 along with QAMPARI (Amouyal
et al., 2023) are part of the Automatic LLMs’ Cita-
tion Evaluation (ALCE) (Gao et al., 2023) bench-
mark. QAMPARI is not LFQA, but rather multiple
short extractive answers. We compare all the LFQA
datasets to CLAPNQ in Table 2. Most notably,
CLAPNQ is the only dataset to include consider-
able unanswerable questions, manually annotated
answers grounded on a single gold passage, and a
corpus for the full RAG pipeline.
The Explain Like I’m 5 (ELI5) dataset con-
sists of questions and responses from the Reddit
thread. KILT-ELI5 (Petroni et al., 2021) provides
Wikipedia documents that have been retrieved us-
ing the questions for benchmarking RAG. However,
1https://github.com/primeqa/clapnq
there are no gold passages and the KILT-ELI5 doc-
uments do not necessarily have the answer. The
responses written for this sub-Reddit are by subject
matter experts (SME) and are often not grounded
on any text or passage. Each question is likely to
have many responses and they may not all be ap-
propriate or relevant and inter-annotator agreement
(IAA) is very low as shown in Table 2. IAA is
measured as the mean RougeL F1 score between
each pair of annotations for the same question.
TruthfulQA (Lin et al., 2022) has sets of true and
false reference answers and a source that supports
the reference answers for each question. It is a very
small validation dataset as shown in Table 2 that
was designed to be adversarial (the questions were
intentionally picked to be ones that are answered
incorrectly) to probe LLMs. The answers are also
considerably shorter than the other LFQA datasets.
WikiHowQA (Deng et al., 2020) is “How to” in-
struction questions from the WikiHow website. For
each page, the question is the title and the answer
is the context. Only pages that have reference doc-
uments are kept. There can be many references for
each question. The answers and references are long
and have not been manually verified.
ExpertQA (Malaviya et al., 2023) consists of
questions that are written by SMEs. They then use
GPT-4 and various retriever setups (e.g. Closed-
Book, and BM25) to generate several answers
and retrieve relevant documents. The experts then
evaluate the answers and evidence and can delete
claims and evidence that are false and revise if they
want to (it is optional). Only one answer was eval-
uated and revised for each question. Due to the
approach of creating the dataset the answers are
likely biased by the LLMs.
AquaMuse (Kulkarni et al., 2020) is a summa-
rization dataset using NQ questions that have a
long answer (the passage) without a short answer
similar to CLAPNQ. However, they use sentence-
level matching (by encoding sentences for seman-
tic similarity comparisons) to retrieve up to top 7
documents from Common Crawl while avoiding
exact matches as the abstractive dataset. In the ex-
tractive version, the sentences in the original long
answer are then replaced with the highly seman-
tic similar sentences from the retrieved documents.
This means the new summaries are as long as the
original passage. The information in the original
passage may not be in the retrieved documents.
ASQA (Stelmakh et al., 2022) is an ambiguous
","Question: What is the name of the dataset that is part of the Automatic LLMs’ Citation Evaluation (ALCE) benchmark but is not LFQA, rather multiple short extractive answers?",Answer: QAMPARI
67,"Dataset
Queries
A per Q
W in Q
W in A
S in A
IAA
Unanswerable
AquaMuse Abstractive
21042
1.0
9.2
106.7
3.7
-
-
AquaMuse Extractive
44217
1.0
9.2
106.7
3.7
-
-
ASQA
6316
1.3
10.1
80.7
3.2
0.48
-
ELI5
1507
12.0
19.6
116.9
5.7
0.16
-
ExpertQA
2169
1.0
21.2
174.8
6.1
-
-
TruthfulQA
817
3.2
12.4
9.0
1.0
0.37
11
WikiHowQA
1188189
1.0
7.0
70.1
7.6
-
-
CLAPNQ-R1
12657
1.1
9.2
39.0
1.6
-
-
CLAPNQ
4946
1.4
9.4
56.8
2.3
0.67
2493
Table 2: Comparison to existing Long-form QA datasets. Stats are shown for Answers (A), Queries (Q),
Words (W), Sentences (S), IAA and Unanswerable. W in A of CLAPNQ is 1/3 of W in Passage (P)=156.
questions dataset built from AmbiqQA (Min et al.,
2020) derived from OpenNQ (Lee et al., 2019).
Each answer is generated from one or more pas-
sages that answer a specific instance of the question.
The answers in the AmbigQA paper are all short
and extractive, but in ASQA the explanation to dis-
ambiguate the different answers causes them to be
long. ASQA is derived from the subset of NQ that
has short answers with additional answers for the
ambiguity from AmbigQA. Therefore, the gold pas-
sages for the ambiguous answers are not available
for all ASQA questions and some of the evidence
may not be part of OpenNQ. ASQA is perhaps
the most similar to CLAPNQ, with the main differ-
ences being: 1) ASQA answer comes from multiple
passages while the CLAPNQ answer is contained
in one passage. They are not likely to be cohesive
within a single passage 2) The ASQA answers are
considerably longer, indicating they may not be as
concise 3) We explore additional types of questions
that tend to require a long answer such as boolean
questions, conjunctive questions, descriptive ques-
tions, and questions requiring an explanation. 4)
The IAA computed using RougeL for questions
that were answered by multiple annotators is much
lower than CLAPNQ at 0.48 compared to 0.67.
For a detailed survey of RAG approaches we
direct the reader to the comprehensive RAG survey
(Gao et al., 2024). It is worth noting that the bench-
marks section in this survey is a short paragraph
which refers to two datasets (Liu et al., 2023; Chen
et al., 2023) that focus on short extractive answers,
attacks and robustness when the passages are pur-
posely adversarial and unfaithful. Furthermore, the
datasets questions and responses are created using
ChatGPT which likely introduces biases. The for-
mer (Liu et al., 2023) does not include retrieval and
the latter (Chen et al., 2023) has fixed retrieved pas-
sages instead of a corpus. We believe that this high-
lights the need for quality datasets (like CLAPNQ)
focusing on faithfulness for the full RAG pipeline.
Recently, synthetically generated datasets such
as Alpaca (Taori et al., 2023) and Vicuna (Chiang
et al., 2023) have been created using LLMs. These
datasets can be very large, containing 50k+ conver-
sations, but they’re built to fine-tune LLMs and not
applicable as evaluation benchmarks.
3
Dataset
CLAPNQ is created from the subset of Natural
Questions (NQ) (Kwiatkowski et al., 2019) that
have a long answer (passage) but no short answer.
NQ consists of 323k examples. There are around
30,000 questions that are long answers without
short answers excluding tables and lists. To in-
crease the likelihood of longer answers we only ex-
plored ones that have more than 5 sentences. Each
NQ train example is annotated by one person and
each NQ dev example is annotated by 5 people. We
only explore dev questions where the majority of
the annotators agreed it was a long answer with-
out a short answer. 12,657 training and 384 dev
examples met our criteria for annotation.
3.1
Annotation Task
CLAPNQ was annotated by 7 skilled in-house an-
notators paid above minimum wage whose sole
jobs are performing Natural Language Processing
annotation tasks. The annotation task consisted of
two rounds to provide high quality non-consecutive
grounded answers to the question. Each task in
both rounds took approximately 5 minutes. All an-
notations were performed on the Appen platform.2
The details of each round are described below.
2https://www.appen.com/
",Question: What is the number of training examples in CLAPNQ dataset?,Answer: 4946
68,"Split
No. Questions
Answerable
NQ Source
Unanswerable
NQ Source
Train
3745
1954
Train
1791
Train
Dev
600
300
Train
300
Dev
Test
600
301
Train + 67 Dev
300
Dev
Total
4946
2555
2391
Table 3: Data stats for CLAPNQ. In addition to providing the number of questions per split we also
provide the original source from NQ as we used part of training for the dev and test set.
The main instruction provided to the annotators
was: Given a question and a passage, find the an-
swer to the question in the passage. Check the
boxes for the answer sentences and then copy/paste
the relevant text into the answer box. Finally, af-
ter creating an answer from the passage they were
asked to look over the question and answer and
make sure it makes sense, is a concise answer, and
is grammatically correct. They had to confirm that
they checked all of these things before completing
the task. A screenshot of the task is provided in
Appendix A, Figure 2.
After initial training and pilots with calibrating
of instructions on around 100 questions, each of the
NQ questions without a short answer was annotated
by one trained annotator in Round 1.
In Round 1, the annotators were provided with
the question, title, and long answer paragraph from
NQ divided into sentences using a sentence tok-
enizer. The annotators had to select the sentences
relevant to the answer and then write a concise
answer in their own words with “copy/pasting” al-
lowed. The annotators were instructed to write
the answer using the selected sentences and that it
should make sense, be concise, and grammatically
correct. The question could also be skipped.
In Round 2 of the annotation, all answers from
Round 1 that were made up of two or more selected
sentences that were not consecutive (meaning there
was at least one non-selected sentence between
them, see example in Table 1) were annotated a sec-
ond time by a different annotator. These questions
were selected as they are more likely to be cohe-
sive. The annotators saw the answer from the first
round and could choose to keep the same answer or
modify it. Therefore, the second round answers are
likely to be of higher quality, however, due to hu-
man subjectivity both answers could still be good.
In some cases, the round 2 annotator skipped the
question and it is also possible that they changed
the answer to no longer be non-consecutive.
The final CLAPNQ dataset consists of all an-
swers that have been annotated by more than one
person. We provide the annotations from both
rounds if they were different.
The IAA using
RougeL on the different Round 1 and 2 answers
is 0.67, indicating the answers are usually similar.
The selected sentences, information regarding the
round, and whether the answer is not contiguous is
included in the dataset.
3.2
Data Stats
The CLAPNQ dataset of 4,946 questions consists
of both answerable and unanswerable questions as
described below. The breakdown of the dataset
is shown in Table 3. We also include the source
of the questions within the original NQ dataset.
Since NQ does not release the test set we only
explored the train and development sets. Only 67
NQ dev questions qualified with the properties of
our task so we use them and additional examples
from NQ train as our test set. While the questions
and passages are publicly available with NQ, the
answers we provide are new. CLAPNQ questions
have 1-2 reference answers. The questions are short
at 9 words and the answers are long at around 57
words which is 1/3 of the average passage length of
156 words (See Table 2). In addition to the official
dataset, we will release the round 1 data of 12k
questions as training data, referred to as CLAPNQ-
R1. Our initial experiments with training using
CLAPNQ-R1 did not provide an improvement. We
leave further exploration as future work.
3.2.1
Answerable
The answerable data contains the original question
and gold passage (P) as well as the relevant sen-
tences (RS) and answers (A) created by the annota-
tors as described in the previous section. The Pre-
cision, Recall (R), and F1 scores for RougeL(RS,P)
is 100/45/59 and for RougeL(A,RS) it is 92/72/79
respectively. The first is a sentence retrieval task,
the second is a generative task. RougeL(A,P) is
94/32/46. The retrieval stage reduces the content
",Question: What is the average passage length in the CLAPNQ dataset?,Answer: 156 words
69,"DEV
TEST
nDCG
R
nDCG
R
Model
@1
@3
@5
@10
@10
@1
@3
@5
@10
@10
BM25
18
30
35
40
67
20
31
36
40
64
all-MiniLM-L6-v2
29
43
48
53
79
30
45
51
55
83
BGE-base
37
54
59
61
85
43
57
63
65
88
E5-base-v2
41
57
61
64
87
42
57
61
65
88
Table 4: Retrieval Results using nDCG @1, 3, 5, 10 and Recall@10 as metrics on the dev and test sets.
We report several nDCG@k to illustrate the impact on the RAG task.
by about 2x (R=45) and the generation case reduces
another 30% (R=72) for a total reduction From P
to A of approximately 3x (R=32).
3.2.2
Unanswerable
A similar amount of unanswerable questions from
NQ were extracted to complete the CLAPNQ
dataset. In the NQ training set there is only one
annotation, in the NQ dev set all 5 annotators must
have said it was unanswerable. The unanswerable
questions were randomly chosen from examples
that had more than 5 sentences in the passage by
matching the first word distribution of the answer-
able questions. For example, in CLAPNQ, What
and Where are the most common question types
while Who is the most common question type for
the NQ short answers. Since NQ does not have a
gold passage for unanswerable questions, a random
passage is chosen from the Wikipedia document.
3.3
Retrieval Corpus
We provide a corpus that can be used to build an
index for querying CLAPNQ in a retrieval setting.
It is built using the passages3 from the original
Wikipedia NQ documents used in the CLAPNQ
dataset including the answerable and unanswerable
questions. In some cases there were slightly dif-
ferent versions of the same document. We only
kept one in such cases and ensured that there was
high overlap between the differing passages if they
were a gold passage to a CLAPNQ question. The
corpus includes 178,891 passages from 4,293 doc-
uments, of which 2,345 passages have questions
associated with them across the 4,946 train, dev,
and test answerable and unanswerable splits.4
3Very long (> 3000 words) and short passages (<15 words)
that are not gold answerable passages were discarded.
4There is usually one gold passage, but 14 questions from the
NQ dev set have two gold passages. Both are kept in retrieval,
but only the more frequent one has a gold answer.
4
Experiments and Results
We present baseline experiments on CLAPNQ for
Retrieval, Generation and the full RAG pipeline.
An exhaustive implementation of methods and
training setups is beyond the scope of this paper;
we provide results to illustrate how CLAPNQ per-
forms using common and SOTA approaches.
We report the commonly used retrieval metrics
of nDCG@10 and Recall@10 for retrieval. We
report several metrics to illustrate generation per-
formance. Each of our metrics correlate with one of
the CLAPNQ properties described in the introduc-
tion. The first two are the commonly used RougeL
and Recall (this is the same as Rouge1). RougeL
can be considered a good approximation for how
cohesive the answer is as it will give more credit to
longer spans. Recall is a good approximation for
completeness. We also provide RougeLp which is
an extractiveness metric that measures how faithful
the response is. It computes the RougeL of the
answer to the passage. Since CLAPNQ is extrac-
tive, we would expect a good system to have a high
RougeLp. In addition, we also provide the length
(in characters) of the answer. We notice that length
is a strong indicator of how well a model performs
with answers that are close to the reference length
being desirable, it is therefore a good approximat-
ing for how concise the answer is. Finally, we also
provide the unanswerable accuracy. The output is
considered unanswerable if its answer string indi-
cates it is unanswerable, e.g. “I don’t know"". The
unanswerable strings differ per model.
4.1
Retrieval
We present retrieval results on popular public
SOTA5 base-size (768 embedding dimension)
retrieval dense embedding models E5 (Wang
5See the Retrieval tab of the MTEB leaderboard: https://
huggingface.co/spaces/mteb/leaderboard
",Question: What is the number of passages in the retrieval corpus?,"Answer: 178,891 passages."
70,"DEV
TEST
Answerable
Un-
Answerable
Un-
Model
FS
RougeL
R
RougeLp
Len
ans%
RougeL
R
RougeLp
Len
ans%
FLAN-T5-Large
-
18.6
11.8
7.1
33
79.9
13.8
8.5
5.0
27
83.6
FLAN-T5-Large
1/0
22.0
14.6
8.8
41
77.3
17.1
11.4
6.9
36
82.6
FLAN-T5-Large
1/1
20.3
13.4
8.1
38
81.7
16.3
10.4
6.1
34
85.3
FLAN-T5-XXL
-
22.1
15.0
10.0
45
84.0
22.0
15.6
9.7
56
91.5
FLAN-T5-XXL
1/0
31.9
23.6
15.0
75
78.1
28.9
21.1
14.3
76
84.9
FLAN-T5-XXL
1/1
28.3
21.1
13.0
63
84.8
24.0
17.2
11.4
63
89.2
Llama-13B-chat
-
35.5
64.3
34.0
491
25.0
35.0
61.3
34.0
491
27.4
GPT 4
-
35.9
67.7
30.0
759
18.0
33.4
65.1
30.3
797
22.2
Mistral-7B-Instruct
-
39.0
56.0
29.0
384
18.6
35.4
53.4
29.2
411
16.3
GPT 3.5
-
39.8
58.9
30.0
444
37.0
40.3
56.3
29.9
375
31.3
CLAPNQ-T5-LG-200
-
41.5
51.3
42.1
272
89.7
40.5
49.2
39.0
271
92.0
CLAPNQ-T5-LG
-
57.2
68.3
51.0
318
89.2
57.8
69.5
51.7
351
86.8
Full Passage
-
49.5
97.4
100.0
912
0.0
49.2
98.7
100.0
1039
0.0
Table 5: Generation results with the gold passage using RougeL, Recall, RougeLp, Length and Unanswer-
able accuracy as metrics. Experiments using pre-trained models, few-shot (1 answerable / 1 unanswerable
examples), the fine-tuned model, CLAPNQ-T5-LG, and a full passage baseline.
et al., 2024), BGE (Chen et al., 2024), and
allMiniLM6 (384 embedding dimension) in addi-
tion to BM25 (Robertson, 2009) by ingesting the
CLAPNQ corpus described in Section 3.3. We
ran the ingestion and evaluation for the embed-
ding models using sentence transformers from the
BEIR repository7 keeping all default parameters,
and we used ElasticSearch8 for BM25 with a maxi-
mum passage length of 512 tokens. Passages that
exceeded the length were divided with an overlap
stride of 256. We provide nDCG results for 1, 3 and
5 in addition to 10 to illustrate the potential impact
on the full RAG pipeline which we report in Sec-
tion 4.3. The retrieval results are shown in Table 4.
The E5-Base model performs best with nDCG@10
of 64 on the dev set and E5-base and BGE-base
have the same performance of nDCG@10 of 65 on
the test set. All these models include NQ as part of
their training.
4.2
Generation
The generation task is: Given a question and the
gold relevant passage, generate an answer to the
question. The CLAPNQ dataset is designed to
be faithful and concise so the generated response
should have these properties.
6https://huggingface.co/sentence-transformers/
all-MiniLM-L6-v2
7https://github.com/beir-cellar/beir/
8https://www.elastic.co/elasticsearch
We ran generation experiments with three fami-
lies of models: Encoder-Decoder, Decoder LLMs,
and Fine-Tuned Encoder Decoder. We also com-
pare to a full passage baseline. The generation task
is sent to the model using a prompt. Most models
use an NQ prompt taken from FLAN-T5 (Chung
et al., 2022). GPT and Llama have prompts based
on their model suggestions, all prompts are pro-
vided in Appendix B. In our zero-shot setup the
models were provided with the question, context,
and prompt. In the 1-shot setup (1/0) the model
was provided with the same answerable example
from CLAPNQ training and in the 2-shot setup
(1/1) the model was also provided with the same
unanswerable question for the same passage. The
generation results are shown in Table 5. A human
evaluation and discussion is in Sections 5 and 6.
Encode Decoder Models. We use FLAN-T5-
Large and FLAN-T5-XXL for zero and few-shot
experiments. We chose FLAN-T5 as it has already
been trained on the NQ dataset and should therefore
already be familiar with the task. The FLAN-T5
models, which are fine-tuned on short extractive
tasks, like to provide short answers and therefore
have poor Recall. The few-shot experiments out-
perform the zero-shot experiments, but providing
an unanswerable example has a trade-off of improv-
ing the unanswerable metrics while reducing the
answerable metrics.
Decoder LLMs. We explored several SOTA De-
",Question: What is the RougeL score of the FLAN-T5-Large model in the zero-shot setup?,Answer: 18.6
71,"DEV
TEST
Answerable
Un-
Answerable
Un-
Retriever
Generator
RougeL
R RougeLp Len ans% RougeL
R RougeLp Len ans%
GOLD
GPT 3.5
39.8 58.9
30.0 444
37.0
40.3 56.3
29.9 375
31.3
E5-base-v2
GPT 3.5
34.0 52.8
30.0 459
27.3
35.0 48.9
31.4 373
20.2
GOLD
Mistral-7B-Instruct
39.0 56.0
29.0 384
18.6
35.4 53.4
29.2 411
16.3
E5-base-v2
Mistral-7B-Instruct
31.3 49.4
30.1 436
11.7
29.4 47.5
29.9 463
9.3
GOLD
CLAPNQ-T5-LG
57.3 68.3
51.0 317
89.5
57.8 69.5
51.7 351
86.8
all-MiniLM-L6v2 CLAPNQ-T5-LG
36.6 46.4
52.6 300
49.8
37.9 48.7
52.9 323
47.0
BGE-base
CLAPNQ-T5-LG
40.7 52.3
54.2 331
41.9
41.7 52.4
54.8 331
44.4
E5-base-v2
CLAPNQ-T5-LG
42.8 54.3
53.8 343
40.1
41.6 51.3
55.7 321
45.9
E5-base-v2
E5-CLAPNQ-T5-LG
30.4 37.5
34.3 204
82.7
26.7 32.9
33.0 195
84.6
E5-base-v2
E5-G-CLAPNQ-T5-LG
33.3 40.4
37.0 227
78.8
34.5 41.8
38.0 236
81.0
Table 6: Full RAG results with top 3 passages on CLAPNQ-T5-LG and LLMs using various retrievers.
The metrics reported are RougeL, Recall, RougeLp, Length and Unanswerable accuracy. Each RAG setup
can be compared to its GOLD setup where there is no retrieval.
coder models: LLama, Mistral, GPT 3.5 turbo and
GPT 4 turbo. The SOTA LLMs have poor unan-
swerable performance but better recall. They do
not like to say “I don’t know"" and almost always
provide an answer. This is evident with all models
but worst with Mistral and GPT 4. Interestingly,
GPT 3.5 performed better than GPT 4, particularly
for unanswerable. The LLMs tend to provide an-
swers that are far too long, particularly for GPT 4
at an average of 759 /797 characters, and therefore
are not concise. This is apparent from the high Re-
call but low RougeL. The low RougeLp indicates
that the answers may not be faithful to the passage.
Fine Tuned Encoder Decoder Model. We use
FLAN-T5-Large for our fine-tuned (FT) experi-
ment, which we call CLAPNQ-T5-LG (See imple-
mentation details in Appendix C). CLAPNQ-T5-
LG has good unanswerable performance and good
recall. It is clear that the answers are concise and it
learns the appropriate answer length. It is closest to
the average length of the reference responses which
is 272 dev and 300 test characters. RougeL and
Recall highlight that the answers are most cohesive
and complete and RougeLp shows that it learns
to extract the answer from the passage, while the
other models are considerably less extractive.
We also explore a smaller training size to help
measure whether performance can be improved
when a small amount of labeled data is available.
This is an important use case because labeling data
in a new domain is costly. We call this experi-
ment CLAPNQ-T5-LG-200 as it was trained using
200 examples (an equal amount of answerable and
unanswerable questions) with 10 random samples
and report the average. The RougeL and unan-
swerable metrics are better than the SOTA Decoder
LLMs, but worse than training on the full dataset.
The model tends to say unanswerable too much.
Full Passage Baseline. We compare to a base-
line where the entire passage is taken as the answer.
This performs very well in the automated metrics
but it is clearly not concise as indicated by the
length. The RougeL score highlights the differ-
ence of the LLMs to CLAPNQ-T5-LG which are
considerably lower than providing the full passage.
The difference between the average length of the
generated answers, the reference answer, and the
passage length are an indicator of how difficult the
extraction task is. The answer must discard two
thirds of the passage to be appropriately concise.
4.3
Full RAG Pipeline
In our full RAG pipeline experiments we retrieve
the top passages using the best performing retrieval
model, E5-base-v2, and then perform generation
on the same prompts as in Section 4.2, however in-
stead of the gold passage, the top retrieved passages
are included in the prompt. It is possible that the
gold passage will not be in the top N passages mak-
ing the question unanswerable based on retrieval.
The RAG task is far more difficult than the GOLD
generation task as the model needs to learn which
passages are irrelevant to the question. We experi-
mented with including the top 3 and top 5 passages
",Question: What is the average length of the reference responses in dev and test characters?,Answer: 272 dev and 300 test characters.
72,"in the prompt. Based on the retrieval results in
Table 4, 5 documents has a 4 point improvement
over 3 documents. However, in our experiments
including 5 passages in the prompt increased the
noise and did not provide an improvement.
In the RAG experiments we explored each dense
retriever with CLAPNQ-T5-LG, and the best re-
triever on the dev set, E5 Base, with the best per-
forming generation models: GPT 3.5, Mistral-7b-
Instruct and CLAPNQ-T5-LG. Results are shown
in Table 6 and we compare against the best GOLD
generation baselines for each model from Table 5 to
show the gap for RAG. GOLD can be considered as
an upper bound as we would not expect the retriever
to perform better than having only the grounded
passage for the automated metrics. In all cases per-
formance drops considerably for CLAPNQ-T5-LG
with a very large drop in % unanswerable. Per-
formance is also reduced for zero-shot GPT 3.5
and Mistral but not as much as CLAPNQ-T5-LG.
A human evaluation and discussion that compares
RAG to Gold is in Sections 5 and 6.
We also explored two fine-tuned models that in-
corporated RAG during training. They follow the
same approach as CLAPNQ-T5-LG, but instead
of the gold passage, the top 3 retrieval passages
are included during training. In the second version,
E5-G-CLAPNQ-T5-LG we ensure the gold pas-
sage is kept in the top 3 passages during training,
at a randomly chosen position, even if it was not
originally included. These models perform better
on the unanswerable questions than CLAPNQ-
T5-LG but much worse on the answerable ques-
tions. The RougeL score of E5-G-CLAPNQ-T5-
LG (51.6/52.1) on the answerable questions that
were answered is better than CLAPNQ-T5-LG
(46.7/44.5) for the dev and test sets, but only a
little more than half the answerable questions were
answered. We leave further experimentation on
optimizing these models as future work.
5
Human Evaluation
In addition to reporting automated metrics we also
performed a human evaluation on the GOLD and
RAG setups to explore how appropriate and faith-
ful users think the responses are as used in the
literature (Es et al., 2023). For each question and
answer, we asked three annotators to indicate on
a scale of 1 (No) - 4 (Yes) whether the answer
looks appropriate (i.e. looks correct or answer rel-
evance) and whether it is faithful to the passage.
Model Faithful Approp F+A Win-Rate
Gold
CLAPNQ-T5-LG
3.7
3.7
3.7
66%
GPT 3.5
3.3
3.6
3.4
34%
Reference
3.9
3.8
3.8
57%
RAG
CLAPNQ-T5-LG
3.8
3.2
3.4
42%
GPT 3.5
3.0
3.6
3.2
35%
Reference
3.0
3.5
3.0
33%
Table 7: Human Evaluation metrics on Faithful (F)
and Appropriate (A) on a 4-point scale and win-
rate. F+A is the harmonic mean of F and A.
These metrics are only measured for the answer-
able questions. During the RAG evaluation we also
asked the annotators to select which of the top 3
retrieved passages were relevant to the answering
the question. If a question was marked faithful, we
asked the annotators to select which passages were
relevant to the answer. Finally, they performed a
pair-wise comparison of the answers to indicate
preference to compute win-rate. Ties were accept-
able but they were asked to do so sparingly. The
answers were shown to the annotators randomly
and they did not know which model produced the
answer. Instructions and a task screenshot are in
Appendix A.
The human evaluation was for the GOLD and
RAG setups. 40 answerable and 10 unanswerable
questions, with an equal amount of questions were
randomly sampled from both the dev and test sets
being included for each setup. The annotators that
performed this task are the same annotators that
worked on creating the dataset, however these an-
notations were done at a later time period. We
compare CLAPNQ-T5-LG, GPT 3.5 (The best per-
forming decoder LLM), and the reference answer.
The evaluation is shown in Table 7.
In the GOLD setup, agreement was high for ap-
propriateness (73%), faithfulness (88%), and win-
rate (86%). The annotators preferred the CLAPNQ-
T5-LG answers the most and GPT 3.5 answers
the least. We investigated several examples where
the CLAPNQ-T5-LG answers were preferred to
the reference answer and both answers were good
but the annotators preferred the direct copying by
CLAPNQ-T5-LG. The reference and CLAPNQ-
T5-LG answers were highly faithful and appropri-
ate but GPT 3.5 was less faithful. This highlights
the importance of being faithful to the passage as
an answer can look correct but not be grounded in
the passage which may indicate factually incorrect
",Question: What is the RougeL score of E5-G-CLAPNQ-T5-LG on the answerable questions that were answered?,Answer: 51.6/52.1
73,"answers. The human evaluation shows that a model
can successfully learn to generate faithful and ap-
propriate responses, but the SOTA LLM models
don’t perform as well on this task.
In the RAG setup, agreement was very high for
faithfulness (91%) and win-rate (90%) but much
lower for appropriateness (68%). The annotators
preferred the CLAPNQ-T5-LG answers the most
with little difference in preference between the
reference and GPT 3.5 answers. The CLAPNQ-
T5-LG answers were very faithful while GPT 3.5
and the reference were less faithful. The GPT
3.5 and reference answers were more appropriate
while CLAPNQ-T5-LG was least appropriate. The
changes from the GOLD setup highlight the impor-
tance of evaluating the RAG pipeline. The refer-
ence answers may not be in the retrieved passages
even though they are correct. However, being faith-
ful to the passages can provide an inappropriate
answer if the retrieved passages are not relevant to
the question. According to two or more annotators,
26/40 answerable questions had multiple relevant
passages and 4/40 had no relevant passages. 38,
39 and 32 of CLAPNQ-T5-LG, GPT 3.5 and refer-
ence responses were considered faithful to one or
more passages. 50% of the unanswerable questions
had relevant passages.
6
Discussion
In this section we describe some challenges we’ve
encountered. We describe them here and provide
examples in Appendix D.
Unanswerable Questions: While it is unlikely
that the unanswerable questions have an answer in
the randomly picked passage, we find that in some
cases, there is actually an answer (Appendix D,
Table 8). There are other cases where the answer
to an unanswerable question may appear correct
when looking at the passage, but the passage may
not be relevant (Appendix D, Table 9).
Generation: GPT 3.5 and Mistral will have an-
swers that are correct but not faithful to the passage
(Appendix D, Table 10,11). Since the prompts
request that the answer use the passage, such an an-
swer should not be provided, or the response should
explain that the answer was found elsewhere. In
many cases GPT 3.5 and Mistral give an answer
that is considerably longer than CLAPNQ-T5-LG
and the reference (Appendix D, Table 12). The
recall is high, but the answer is not concise and
has extra irrelevant information. During the human
evaluation the annotators tend to prefer the concise
answers and will often mark long answers as less
appropriate.
RAG: The answers can change considerably
due to the multiple passages in RAG compared to
GOLD (Appendix D, Table 13, 14,15). In the RAG
setting the automated metrics are much lower than
the GOLD setting. However, the answers may be
good but just have different information which was
found only in the provided passages (Appendix D,
Table 13). If irrelevant passages are retrieved, (Ap-
pendix D, Table 16), the reference answer will have
low extractiveness, but the other answers may still
be incorrect while being grounded which is difficult
to identify without human evaluation.
7
Future Directions
The automated evaluation, human evaluation and
discussion highlight several areas of future direc-
tions: 1) Unanswerable Questions: Many of the
LLMs struggle with the unanswerable questions
and often try to provide an answer. 2) Concise An-
swers: Many of the LLMs like to provide very long
answers that are not concise, which is not preferred
by humans. 3) Irrelevant Retrieval: The models
will try to answer RAG questions even when the
passages are irrelevant, either by being unfaithful
or incorrect. 4) Multiple correct answers: It is
harder to evaluate RAG correctly because the an-
swers could be correct but different than the gold.
5) Dataset Enhancements: We hope to add more
grounded reference answers, a multilingual version,
and other domains.
8
Conclusion
We have presented CLAPNQ, a new benchmark
dataset for evaluating the full RAG pipeline.
CLAPNQ has the properties of being concise,
complete, cohesive, faithful to the passage and
unanswerable questions.
A FT model can per-
form well when the correct passages are pro-
vided during retrieval, while SOTA LLMs are be-
hind in faithfulness, conciseness and unanswer-
ability. Finally, we’ve provided a human evalua-
tion, discussion, and specific areas of future im-
provements.
CLAPNQ is publicly available at
https://github.com/primeqa/clapnq.
",What percentage of answerable questions had multiple relevant passages according to two or more annotators?,Answer: 65%
74,"Ethics Statement
Limitations
As with any manually annotated dataset, there are
likely to be some incorrect and unclear answers.
We did out best to mitigate this as described in
Section 3. We believe in general, that the dataset
quality is strong and can be used as is as a bench-
mark for RAG. CLAPNQ is built from Natural
Questions (Kwiatkowski et al., 2019), therefore
any limitations in Natural Questions and Wikipedia
may also be present in CLAPNQ.
Intended Use
CLAPNQ and CLAPNQ-T5-LG are intended to
be used to advance research in RAG. CLAPNQ is
being released with an Apache 2.0 license. We do
not approve of any adversarial or harmful uses of
our work.
Biases
NQ train and dev have been included in training
of most, if not all, LLMs which may lead to bi-
ases, particularly since CLAPNQ dev is part of
NQ train. However, all models have this same ad-
vantage. While the questions and passages have
been seen by all models the CLAPNQ answers are
new and remain hidden. Any biases in NQ and
Wikipedia may also be present in CLAPNQ.
References
Vaibhav Adlakha, Shehzaad Dhuliawala, Kaheer
Suleman, Harm de Vries, and Siva Reddy. 2022.
TopiOCQA: Open-domain conversational ques-
tion answering with topic switching. Transac-
tions of the Association for Computational Lin-
guistics, 10:468–483.
Samuel Joseph Amouyal, Tomer Wolfson, Ohad
Rubin, Ori Yoran, Jonathan Herzig, and Jonathan
Berant. 2023. Qampari: An open-domain ques-
tion answering benchmark for questions with
many answers from multiple paragraphs.
Bernd Bohnet, Vinh Q. Tran, Pat Verga, Roee
Aharoni, Daniel Andor, Livio Baldini Soares,
Massimiliano Ciaramita, Jacob Eisenstein, Kuz-
man Ganchev, Jonathan Herzig, Kai Hui, Tom
Kwiatkowski, Ji Ma, Jianmo Ni, Lierni Ses-
torain Saralegui, Tal Schuster, William W. Co-
hen, Michael Collins, Dipanjan Das, Donald
Metzler, Slav Petrov, and Kellie Webster. 2022.
Attributed question answering: Evaluation and
modeling for attributed large language models.
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christo-
pher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners.
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo,
Defu Lian, and Zheng Liu. 2024.
Bge m3-
embedding: Multi-lingual, multi-functionality,
multi-granularity text embeddings through self-
knowledge distillation.
Jiawei Chen, Hongyu Lin, Xianpei Han, and
Le Sun. 2023. Benchmarking large language
models in retrieval-augmented generation.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E.
Gonzalez, Ion Stoica, and Eric P. Xing. 2023.
Vicuna: An open-source chatbot impressing gpt-
4 with 90%* chatgpt quality.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Yunxuan Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha
Brahma, Albert Webson, Shixiang Shane Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen,
Aakanksha Chowdhery, Alex Castro-Ros, Marie
Pellat, Kevin Robinson, Dasha Valter, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent
Zhao, Yanping Huang, Andrew Dai, Hongkun
Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob
Devlin, Adam Roberts, Denny Zhou, Quoc V.
Le, and Jason Wei. 2022. Scaling instruction-
finetuned language models.
Yang Deng, Wai Lam, Yuexiang Xie, Daoyuan
Chen, Yaliang Li, Min Yang, and Ying Shen.
2020. Joint learning of answer selection and
answer summary generation in community ques-
tion answering. In The Thirty-Fourth AAAI Con-
ference on Artificial Intelligence, AAAI 2020,
",Question: What license is CLAPNQ being released with?,Answer: CLAPNQ is being released with an Apache 2.0 license.
75,"The Thirty-Second Innovative Applications of
Artificial Intelligence Conference, IAAI 2020,
The Tenth AAAI Symposium on Educational Ad-
vances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020, pages 7651–
7658. AAAI Press.
Shahul Es, Jithin James, Luis Espinosa-Anke, and
Steven Schockaert. 2023. Ragas: Automated
evaluation of retrieval augmented generation.
Angela Fan, Yacine Jernite, Ethan Perez, David
Grangier, Jason Weston, and Michael Auli. 2019.
ELI5: Long form question answering. In Pro-
ceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages
3558–3567, Florence, Italy. Association for
Computational Linguistics.
Adam Fisch, Alon Talmor, Danqi Chen, Eunsol
Choi, Minjoon Seo, Patrick Lewis, Robin Jia,
and Sewon Min, editors. 2021. Proceedings of
the 3rd Workshop on Machine Reading for Ques-
tion Answering. Association for Computational
Linguistics, Punta Cana, Dominican Republic.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi
Chen. 2023. Enabling large language models to
generate text with citations.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang
Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,
Qianyu Guo, Meng Wang, and Haofen Wang.
2024. Retrieval-augmented generation for large
language models: A survey.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Ming-Wei Chang. 2020. Realm:
Retrieval-augmented
language
model
pre-
training.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur
Mensch, Chris Bamford, Devendra Singh Chap-
lot, Diego de las Casas, Florian Bressand,
Gianna Lengyel, Guillaume Lample, Lucile
Saulnier, Lélio Renard Lavaud, Marie-Anne
Lachaux, Pierre Stock, Teven Le Scao, Thibaut
Lavril, Thomas Wang, Timothée Lacroix, and
William El Sayed. 2023. Mistral 7b.
Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei
Sha, and Eugene Ie. 2020. Aquamuse: Auto-
matically generating datasets for query-based
multi-document summarization.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob
Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, An-
drew M. Dai, Jakob Uszkoreit, Quoc Le, and
Slav Petrov. 2019. Natural questions: A bench-
mark for question answering research. Trans-
actions of the Association for Computational
Linguistics, 7:452–466.
Kenton Lee, Ming-Wei Chang, and Kristina
Toutanova. 2019. Latent retrieval for weakly
supervised open domain question answering.
In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics,
pages 6086–6096, Florence, Italy. Association
for Computational Linguistics.
Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen tau
Yih, Tim Rocktäschel, Sebastian Riedel, and
Douwe Kiela. 2021. Retrieval-augmented gener-
ation for knowledge-intensive nlp tasks.
Stephanie Lin, Jacob Hilton, and Owain Evans.
2022.
TruthfulQA: Measuring how models
mimic human falsehoods. In Proceedings of the
60th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 3214–3252, Dublin, Ireland. Association
for Computational Linguistics.
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo
Chen, Hao Zhou, Fandong Meng, Jie Zhou, and
Xu Sun. 2023. Recall: A benchmark for llms
robustness against external counterfactual knowl-
edge.
Chaitanya Malaviya, Subin Lee, Sihao Chen, Eliz-
abeth Sieber, Mark Yatskar, and Dan Roth.
2023. Expertqa: Expert-curated questions and
attributed answers.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Infor-
mation Retrieval. Cambridge University Press,
Cambridge, UK.
Sewon Min, Julian Michael, Hannaneh Hajishirzi,
and Luke Zettlemoyer. 2020. AmbigQA: An-
swering ambiguous open-domain questions. In
","Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics,
pages 3341–3353, Online. Association for Com-
putational Linguistics.
Sewon Min, Julian Michael, Hannaneh Hajishirzi,
and Luke Zettlemoyer. 2020. Ambigqa: An-
swering ambiguous open-domain questions.
In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics,
pages 3341–3353, Online. Association for Com-
putational Linguistics.
Sewon Min, Julian Michael, Hannaneh Hajishirzi,
and Luke Zettlemoyer. 2020. Ambigqa: An-
swering ambiguous open-domain questions.
In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics,
pages 3341–3353, Online. Association for Com-
putational Linguistics.
What is the title of the conference where the paper ""ELI5: Long form question answering"" was presented?

(Note: I'll be happy to help you refine your question if needed)",Answer: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.
76,"Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP), pages 5783–5797, Online. Associa-
tion for Computational Linguistics.
Fabio Petroni, Aleksandra Piktus, Angela Fan,
Patrick Lewis, Majid Yazdani, Nicola De Cao,
James
Thorne,
Yacine
Jernite,
Vladimir
Karpukhin, Jean Maillard, Vassilis Plachouras,
Tim Rocktäschel, and Sebastian Riedel. 2021.
KILT: a benchmark for knowledge intensive lan-
guage tasks. In Proceedings of the 2021 Con-
ference of the North American Chapter of the
Association for Computational Linguistics: Hu-
man Language Technologies, pages 2523–2544,
Online. Association for Computational Linguis-
tics.
Pranav Rajpurkar, Robin Jia, and Percy Liang.
2018. Know what you don’t know: Unanswer-
able questions for squad.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopy-
rev, and Percy Liang. 2016. SQuAD: 100,000+
questions for machine comprehension of text. In
Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing,
pages 2383–2392, Austin, Texas. Association
for Computational Linguistics.
S. Robertson. 2009. The Probabilistic Relevance
Framework: BM25 and Beyond. Foundations
and Trends® in Information Retrieval, 3(4):333–
389.
Anna Rogers, Matt Gardner, and Isabelle Augen-
stein. 2023.
Qa dataset explosion: A taxon-
omy of nlp resources for question answering and
reading comprehension. ACM Comput. Surv.,
55(10).
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and
Ming-Wei Chang. 2022. ASQA: Factoid ques-
tions meet long-form answers. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing, pages 8273–8288,
Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model.
https://github.com/tatsu-lab/
stanford_alpaca.
Nandan Thakur, Nils Reimers, Andreas Rücklé,
Abhishek Srivastava, and Iryna Gurevych. 2021.
Beir: A heterogeneous benchmark for zero-shot
evaluation of information retrieval models. In
Proceedings of the Neural Information Process-
ing Systems Track on Datasets and Benchmarks,
volume 1. Curran.
Hugo Touvron, Louis Martin, Kevin Stone, Pe-
ter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal
Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernan-
des, Jeremy Fu, Wenyin Fu, Brian Fuller, Cyn-
thia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou,
Hakan Inan, Marcin Kardas, Viktor Kerkez,
Madian Khabsa, Isabel Kloumann, Artem Ko-
renev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich,
Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin
Nie, Andrew Poulton, Jeremy Reizenstein, Rashi
Rungta, Kalyan Saladi, Alan Schelten, Ruan
Silva, Eric Michael Smith, Ranjan Subramanian,
Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela
Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom. 2023. Llama 2: Open
foundation and fine-tuned chat models.
Ellen M. Voorhees and Donna K. Harman. 2005.
TREC: Experiment and Evaluation in Informa-
tion Retrieval (Digital Libraries and Electronic
Publishing). The MIT Press.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-
jumder, and Furu Wei. 2024. Text embeddings
by weakly-supervised contrastive pre-training.
Howard Yen, Tianyu Gao, Jinhyuk Lee, and Danqi
Chen. 2023.
MoQA: Benchmarking multi-
type open-domain question answering. In Pro-
ceedings of the Third DialDoc Workshop on
Document-grounded Dialogue and Conversa-
tional Question Answering, pages 8–29, Toronto,
Canada. Association for Computational Linguis-
tics.
",Question: What is the name of the benchmark for knowledge-intensive language tasks introduced in 2021?,Answer: KILT
77,"Figure 2: The Round 1 annotation task for CLAPNQ. The annotator had to select the title/sentences
needed to answer the question, and then provide a concise answer.
A
Annotation Tasks
All annotation tasks were performed using Appen.
They are described in Section 3 and 5 of the main
paper. We provide screenshots and further instruc-
tions below.
A.1
Dataset Creation
The CLAPNQ dataset was created in two rounds.
A screenshot of round 1 is shown in Figure 2 and
Figure 4. A small handful of the questions (1 in
train, and 9 in dev) are high-quality annotations
from the initial pilot rounds. These examples have
several reference answers.
A.2
Human Evaluation
The human evaluation was performed a portion of
the dev and test sets. Human eval on the GOLD
generation task is shown in Figure 3. The RAG
version had two additional questions regarding pas-
sage relevance as described in Section 5. We plan
on releasing the human evaluation annotations as
part of the dataset release. The general instructions
to the annotator were as follows: In this task, you
will review the same question and passage and, for
each one, rate the quality of the answer to the ques-
tion. On each page, you will see 3 different answers
to the same question. Read the question and pas-
sage and answer how well you are confident in the
question, passage, and know the correct answer.
For each model answer, (given the same context
and passage): The answer to the model is in red.
Please make your judgements on this red answer
span. indicate if the answer is an “I don’t know”
or if the answer is completely incoherent. For each
model response, answer the following questions on
a scale of 1-4: 1) DO NOT USE THE PASSAGE
TO ANSWER THIS QUESTION: Does the response
to the question look appropriate, useful, concise,
and complete? 2) Is the response faithful to the pas-
sage? Evaluate each metric independently. Finally,
also perform a head to head comparison of the
model responses by answering the following ques-
tion for every pair of answers: Which response do
you prefer in terms of faithfulness, appropriateness
and naturalness?
B
Prompts
The Flan-T5 (Chung et al., 2022) prompt which
was used for most models is: {title}: {passage}
Please answer a question about this article.
If
the question is unanswerable, say “unanswerable”.
user: {question}, answer:
",Question: What is the name of the platform used to perform all annotation tasks?,Answer: Appen.
78,"Figure 3: The human evaluation task used to compare the model answers in random order. The individual
questions per answer are shown here for one model.
The GPT Prompt is based on chat completion from
OpenAI9: {‘role’: ‘system’, ’content’: “Generate
next agent response, given the following docu-
ment(s). If you cannot base your answer on the
document, please state that you do not have an an-
swer.’}, {‘role’: ‘system’, ‘content’: “[title]: {title}
[document]: {passage}, {‘role’: ‘user’, ‘content’:
question}’}
The Llama Prompt is the default Llama 2
prompt (Touvron et al., 2023):
<s>[INST]
<<SYS>> You are a helpful, respectful and hon-
est assistant. Always answer as helpfully as pos-
sible, while being safe. Your answers should not
include any harmful, unethical, racist, sexist, toxic,
dangerous, or illegal content. Please ensure that
your responses are socially unbiased and positive
in nature. If a question does not make any sense,
or is not factually coherent, explain why instead
of answering something not correct. If you don’t
know the answer to a question, please don’t share
false information. <</SYS>> [document]: {ti-
tle} {passage}. [conversation]: question},. Answer
with no more than 150 words. If you cannot base
your answer on the above document(s), please state
9https://learn.microsoft.com/en-us/azure/
ai-services/openai/reference
that you do not have an answer. [/INST]
C
Implementation Details
We used HuggingFace transformers10 for all train-
ing experiments. We experimented with several
variations and our final CLAPNQ-T5-LG model
was trained for 6 epochs, checking after each epoch
against the development set to keep the best model.
CLAPNQ-T5-LG has a learning rate of 1e −4,
batch size of 32, max input length of 412 tokens
with an output length of 100 tokens. We kept the
T5 context length during training to keep within
GPU constraints and improve training speed. We
cut off the end of the 368 passages (10% of the
training data) that did not fit in the context. The
prompts were not truncated during evaluation.
In the small sample size experiments, we ac-
commodate for the smaller size by increasing the
learning rate to 1e −3. In the RAG CLAPNQ-T5-
LG experiments, the context is considerably longer
so we increase the context size to 1024. This main-
tains a similar amount of data that needs to be cut
off as in the other experiments at 317 and 346 for
10https://huggingface.co/docs/transformers/model_
doc/flan-t5
",Question: What is the batch size used in the training of the CLAPNQ-T5-LG model?,Question: What is the batch size used in the training of the CLAPNQ-T5-LG model?
79,"Figure 4: The Round 2 annotation task for CLAPNQ. The annotator had to verify and update the answer
provided in Round 1 if needed. They also had to provide how they edited the answer.
the E5-CLAPNQ-T5-LG and E5-G-CLAPNQ-T5-
LG models (<10%). We accommodate for these
experiments with the longer context size by using
a batch size of 8 and 10 epochs.
D
Examples
We provide several examples of output comparing
the various LLMs as described in Section 6. See
Figures 8-16.
","What is the batch size used in the experiments?

Question: What is the batch size used in the experiments?",Answer: 8
80,"Figure 5: The human evaluation task used to compare the model answers in random order. The head-to-
head comparison for win-rate is shown here.
Question: who are the characters in the black cat
Passage: Sephiria Arks ( Sefiria ¯
Akusu ) , known as Number I , is the commander of the Numbers and is trusted
enough by Chronos to be given total control over killing Creed . Sephiria was one of the few members who were
“ born for Chronos ” , as she stated that when she was an infant she underwent surgery to enhance her recovery
time from injuries . Sven once commented that her eyes allowed her to stare into the recesses of one ’s soul .
This empathy is her greatest strength , which allows her to utilize her powers of manipulation to their utmost
extent . She will do anything to finish her missions and will sacrifice whatever it takes in order to succeed in
meeting an objective . However , she does show some inner conflict in her manipulation , even questioning
Belze about her actions . She controls all of the Chronos Numbers ’ actions as well as many of the independent
operatives in the hunt for Creed with little interference from the Elders .
Responses
GPT 3.5
In the Black Cat series, there are several characters. One of the main characters is Sephiria Arks, also known
as Number I. She is the commander of the Numbers and has complete control over killing Creed. Sephiria
underwent surgery as an infant to enhance her recovery time from injuries and has the ability to stare into the
recesses of one’s soul. She is highly skilled in manipulation and will do whatever it takes to succeed in her
missions. Sephiria controls the actions of the Chronos Numbers and many independent operatives in the hunt for
Creed.
CLAPNQ-T5-LG
Sephiria Arks, known as Number I, is the commander of the Numbers and is trusted enough by Chronos to be
given total control over killing Creed. Sven once commented that her eyes allowed her to stare into the recesses
of one’s soul. This empathy is her greatest strength, which allows her to utilize her powers of manipulation to
their utmost extent.
Reference
No answer
Table 8: This is an Unanswerable question that is actually partially answerable from the passage. It only
mentions one character.
",Question: Who are the characters in the Black Cat?,Answer: Sephiria Arks and Creed.
81,"Question: what type of volcano is one tree hill
Passage: Maungakiekie One Tree Hill Maungakiekie / One Tree Hill after the removal of the “ one tree ” .
Highest point Elevation 182 m ( 597 ft ) Coordinates 36 °54 ’ 0 ” S 174 °46 ’ 59 ” E · / · 36.90000 °S 174.78306
°E · / - 36.90000 ; 174.78306 Coordinates : 36 °54 ’ 0 ” S 174 °46 ’ 59 ” E · / · 36.90000 °S 174.78306 °E · / -
36.90000 ; 174.78306 Geography Location North Island , New Zealand Geology Volcanic arc / belt Auckland
volcanic field Location
Responses
GPT 3.5
One Tree Hill is part of the Auckland volcanic field, which is a volcanic arc/belt.
CLAPNQ-T5-LG
No answer
Reference
No answer
Table 9: This is a difficult example which confused most annotators during the human evaluation. It
appears from the passage that the GPT answer may be correct but the answer is “scoria cone volcano”
which is not in the passage.
","Answer: scoria cone volcano

Here is your question:

Question: What type of volcano is Maungakiekie One Tree Hill?",Answer: scoria cone volcano
82,"Question: who is given credit for inventing the printing press
Passage: The world ’s first movable type printing press technology for printing paper books was made of
porcelain materials and was invented around AD 1040 in China during the Northern Song Dynasty by the
inventor Bi Sheng ( 990 – 1051 ) . Subsequently in 1377 , the world ’s oldest extant movable metal print book ,
Jikji , was printed in Korea during the Goryeo dynasty . Because of this , the diffusion of both movable - type
systems was , to some degree , limited to primarily East Asia , although various sporadic reports of movable
type technology were brought back to Europe by Christian missionaries , traders and business people who were
returning to Europe after having worked in China for several years and influenced the development of printing
technology in Europe . Some of these medieval European accounts are still preserved in the library archives
of the Vatican and Oxford University among many others . Around 1450 Johannes Gutenberg , introduced the
metal movable - type printing press in Europe , along with innovations in casting the type based on a matrix
and hand mould . The small number of alphabetic characters needed for European languages was an important
factor . Gutenberg was the first to create his type pieces from an alloy of lead , tin , and antimony – and these
materials remained standard for 550 years .
Responses
GPT 3.5
The invention of the printing press is commonly credited to Johannes Gutenberg, who introduced the metal
movable-type printing press in Europe around 1450. Gutenberg’s innovations in casting the type based on a
matrix and hand mould, along with the use of an alloy of lead, tin, and antimony for creating the type pieces, are
considered significant advancements in printing technology.
Mistral
Johannes Gutenberg of Mainz, Germany is given credit for inventing the metal movable - type printing system
in Europe and the printing press.
CLAPNQ-T5-LG
The world’s first movable type printing press technology for printing paper books was invented around AD 1040
in China during the Northern Song Dynasty by the inventor Bi Sheng (990 – 1051). Around 1450 Johannes
Gutenberg introduced the metal movable - type printing press in Europe.
Reference
The world ’s first movable type printing press technology for printing paper books was made of porcelain
materials and was invented around AD 1040 in China during the Northern Song Dynasty by the inventor Bi
Sheng ( 990 – 1051 ) . Around 1450 Johannes Gutenberg , introduced the metal movable - type printing press in
Europe.
Table 10: The GPT and Mistral answers look good but are only partially faithful to the passage, because
they does not give any credit to the innovation in China that is also mentioned in the passage. The GPT
answer is also too long.
",Question: Who is given credit for inventing the printing press?,Answer: Johannes Gutenberg and Bi Sheng
83,"Question: who played scotty in the new star trek movie
Passage: Urban was born in Wellington , New Zealand . His father , a German immigrant , owned a leather
goods store , and his mother once worked for Film Facilities in Wellington . Through his mother , the young
Urban was exposed to classic New Zealand cinema , and developed an interest in the film industry . Urban
attended St Mark ’s Church School , where he showed an early love for public performance . His first acting
role came at age eight , when he had a single line in one episode of the New Zealand television series Pioneer
Woman . Though continuing to take part in school stage productions , he did not act professionally again until
after high school .
Responses
GPT 3.5
Simon Pegg played Scotty in the new Star Trek movies.
CLAPNQ-T5-LG
No answer
Reference
No answer
Table 11: GPT finds the correct answer but it is not grounded in the document.
","Create your question:

Question: Who played Scotty in the new Star Trek movies?",Answer: Simon Pegg
84,"Question: where would a commercial quantity of cobalt-60 be produced
Passage: Cobalt - 60 , Co , is a synthetic radioactive isotope of cobalt with a half - life of 5.2714 years . It is
produced artificially in nuclear reactors . Deliberate industrial production depends on neutron activation of bulk
samples of the monoisotopic and mononuclidic cobalt isotope Co . Measurable quantities are also produced as
a by - product of typical nuclear power plant operation and may be detected externally when leaks occur . In
the latter case ( in the absence of added cobalt ) the incidentally produced Co is largely the result of multiple
stages of neutron activation of iron isotopes in the reactor ’s steel structures via the creation of Co precursor .
The simplest case of the latter would result from the activation of Fe . Co decays by beta decay to the stable
isotope nickel - 60 ( Ni ) . The activated nickel nucleus emits two gamma rays with energies of 1.17 and 1.33
MeV , hence the overall nuclear equation of the reaction is Co +n →Co →Ni +e + ν+ gamma rays .
Responses
GPT 3.5
A commercial quantity of cobalt-60 is typically produced through neutron activation of bulk samples of the
monoisotopic and mononuclidic cobalt isotope Co. This industrial production process usually takes place in
nuclear reactors. Additionally, measurable quantities of cobalt-60 can also be produced as a by-product of
typical nuclear power plant operations. However, it’s important to note that cobalt-60 is highly regulated and
controlled due to its radioactive nature.
CLAPNQ-T5-LG
Cobalt-60 is produced artificially in nuclear reactors. Measurable quantities are also produced as a by - product
of typical nuclear power plant operation.
Mistral
Cobalt-60 is typically produced in nuclear reactors through the irradiation of natural cobalt. Commercial
quantities are produced by specialized companies in several countries including Argentina, Canada, and Russia,
which are the largest suppliers of Cobalt-60 in the world. These companies have dedicated facilities for the
production of Cobalt-60 using nuclear reactors. The irradiated cobalt is then extracted and purified to produce
the commercial product.
Reference
It is produced artificially in nuclear reactors. Measurable quantities are also produced as a by - product of typical
nuclear power plant operation.
Table 12: In this example the GPT and Mistral response are too long and have both irrelevant and extra
information. Mistral is also hallucinating.
",Question: Where are commercial quantities of cobalt-60 produced?,"Answer: In nuclear reactors and specialized facilities in countries such as Argentina, Canada, and Russia."
85,"Question: where are nimbus clouds found in the sky
Passage 1 (GOLD): Nimbostratus has a diffuse cloud base generally found anywhere from near surface in the
low levels to about 3,000 m ( 9,800 ft ) in the middle level of the troposphere . Although usually dark at its base ,
it often appears illuminated from within to a surface observer . Nimbostratus usually has a thickness of about
2000 to 4000 m . Though found worldwide , nimbostratus occurs more commonly in the middle latitudes . It is
coded C 2 on the SYNOP report .
Passage 2: Nimbostratus occurs along a warm front or occluded front where the slowly rising warm air mass
creates nimbostratus along with shallower stratus clouds producing less rain , these clouds being preceded by
higher - level clouds such as cirrostratus and altostratus . Often , when an altostratus cloud thickens and descends
into lower altitudes , it will become nimbostratus .
Passage 3: Multi-level nimbostratus is physically related to other stratiform genus - types by way of being
non-convective in nature . However , the other sheet - like clouds usually each occupy only one or two levels
at the same time . Stratus clouds are low - level and form from near ground level to 2,000 metres ( 6,600 ft )
at all latitudes . In the middle level are the altostratus clouds that form from 2,000 metres ( 6,600 ft ) to 7,000
metres ( 23,000 ft ) in polar areas , 7,000 metres ( 23,000 ft ) in temperate areas , and 7,600 metres ( 24,900 ft )
in tropical areas ...
Responses
GPT 3.5
Gold Nimbostratus clouds are generally found anywhere from near the surface in the low levels to about 3,000 m
(9,800 ft) in the middle level of the troposphere. They can be found worldwide, but they occur more commonly
in the middle latitudes.
RAG Nimbostratus clouds are found in the middle level of the troposphere, typically anywhere from near the
surface to about 3,000 meters (9,800 feet) in altitude.
Mistral
Gold Nimbostratus clouds are found in the middle levels of the troposphere, at altitudes ranging from near the
surface up to about 3,000 meters (9,800 ft).
RAG Nimbostratus clouds are found in the middle levels of the troposphere, from near surface to about 3,000 m
( 9,800 ft ) in altitude. They are often preceded by higher-level clouds such as cirrostratus and altostratus and are
characterized by their thick, dark, and uniform appearance. Nimbostratus clouds are usually associated with
frontal systems and are responsible for producing continuous and heavy precipitation.
CLAPNQ-T5-LG
Gold Nimbostratus has a diffuse cloud base generally found anywhere from near surface in the low levels to
about 3,000 m ( 9,800 ft ) in the middle level of the troposphere. Though found worldwide, nimbostratus occurs
more commonly in the middle latitudes.
RAG Nimbostratus has a diffuse cloud base generally found anywhere from near surface in the low levels to
about 3,000 m ( 9,800 ft ) in the middle level of the troposphere. Stratus clouds are low - level and form from
near ground level to 2,000 metres ( 6,600 ft ) at all latitudes. In the middle level are the altostratus clouds that
form from 2,000 metres ( 6,600 ft ) to 7,000 metres ( 23,000 ft ) in polar areas, 7,000 metres ( 23,000 ft ) in
temperate areas, and 7,600 metres ( 24,900 ft ) in tropical areas.
Reference
Nimbostratus has a diffuse cloud base generally found anywhere from near surface in the low levels to about
3,000 m in the middle level of the troposphere. Though found worldwide, nimbostratus occurs more commonly
in the middle latitudes.
Table 13: In this example we compare the responses when just the GOLD answer is provided and the top
3 passages are provided when the first passage is the GOLD passage. All models provide good responses
when only the gold answer is provided. In the non-RAG setting CLAPNQ-T5-LG and Mistral provided
irrelevant information from the other passages about other clouds. All three missed the sentence that they
can be found world-wide in the RAG setup.
",Question: At what altitude are nimbostratus clouds generally found?,"Answer: Nimbostratus clouds are generally found anywhere from near the surface in the low levels to about 3,000 m (9,800 ft) in the middle level of the troposphere."
86,"Question: who was glumdalclitch how did she help gulliver
Responses
GPT 3.5 Gold
Glumdalclitch was a nine-year-old girl, the daughter of a giant farmer who captured Gulliver. She became fond
of Gulliver and treated him like a pet, creating a miniature bed-chamber for him and playing with him as if
he were a doll. Glumdalclitch was also a skilled seamstress and made dolls’ clothes for Gulliver. While she
provided companionship and amusement for Gulliver, it doesn’t mention any specific ways in which she helped
him beyond that.
Mistral Gold
Glumdalclitch was a nine-year-old girl from Brobdingnag who captured Gulliver and made him her pet. She
was only forty feet tall, making Gulliver appear gigantic in comparison. Despite her small size, she was skilled
in sewing and made clothes for Gulliver from the finest Brobdingnagian fabric. However, the fabric irritated
Gulliver’s skin. Glumdalclitch was amused by Gulliver and enjoyed playing with him as if he were a doll. She
gave him the pet name of Glumdalclitch, which means ¨
little nurseïn the Brobdingnagian language. Despite being
a captive, Gulliver grew very fond of Glumdalclitch and was proud to perform stunts for her entertainment.
CLAPNQ-T5-LG Gold
Glumdalclitch is a skilled seamstress with a talent for making dolls’ clothes. She makes Gulliver her pet, creates
a small travel case for him, and is amused to play with him as if he were a doll. Gulliver grows very proud of the
stunts that he performs for Glumdalclitch’s amusement.
Reference
Glumdalclitch is a nine-year-old, forty feet tall daughter of the farmer who captured Gulliver. She made Gulliver
her pet, created a bed for him out of a travel case and made him clothes.
Table 14: An example that can be used with Table 15 to compare the GOLD setup with the RAG setup
when the GOLD passage is not retrieved during RAG.
",Question: What was Glumdalclitch's occupation or skill?,Answer: Glumdalclitch was a skilled seamstress.
87,"Question: who was glumdalclitch how did she help gulliver
Passage 1: When the Queen of Brobdingnag takes Gulliver into her court , he has Glumdalclitch brought to
court with him . The prideful Gulliver thinks of himself as being greatly honored and promoted by moving
to court , but never ceases to love and seek the approval of the little girl who first helped him . Indeed , he
remembers her fondly even after returning to England .
Passage 2: Glumdalclitch is the name Gulliver gives his “ nurse ” in Book II of Jonathan Swift ’s Gulliver ’s
Travels . In Book I , Gulliver travels to the land of Lilliput . Leaving there , he travels to the land of Brobdingnag
. In Lilliput , Gulliver was a giant , and in Brobdingnag , he is a dwarf , with the proportions reversed .
Passage 3: This article is written like a personal reflection or opinion essay that states a Wikipedia editor ’s
personal feelings about a topic . Please help improve it by rewriting it in an encyclopedic style . ( October 2017
) ( Learn how and when to remove this template message ) Gulliver
Responses
GPT 3.5 RAG
Glumdalclitch was a character in Jonathan Swift’s novel Gulliver’s Travels. She was the nurse or caretaker of
Gulliver in the land of Brobdingnag. Glumdalclitch helped Gulliver by taking care of him, ensuring his safety,
and assisting him during his time in the court of the Queen of Brobdingnag. While Gulliver felt honored and
promoted in the court, he still cherished and sought approval from Glumdalclitch, showing his deep affection for
her.
Mistral RAG
Glumdalclitch was a little girl whom Gulliver met in Brobdingnag, a land of giants, where he was a dwarf.
She helped him by nursing him and taking care of him when he was ill. After Gulliver’s return to England, he
remembered her fondly.
CLAPNQ-T5-LG RAG
Glumdalclitch is the name Gulliver gives his nurse” in Book II of Jonathan Swift’s Gulliver’s Travels. In Lilliput,
Gulliver was a giant, and in Brobdingnag, he is a dwarf, with the proportions reversed.
Reference
Glumdalclitch is a nine-year-old, forty feet tall daughter of the farmer who captured Gulliver. She made Gulliver
her pet, created a bed for him out of a travel case and made him clothes.
Table 15: Alternative appropriate and relevant answers based on the retrieved passages compared to the
GOLD setup in Table 14.
",Question: What was Glumdalclitch's age when she helped Gulliver?,Answer: 9 years old.
88,"Conversation
User: why did the us demand trade with japan
Passages
Passage 1
The United States reacted by seeking to bring the Japanese war effort to a complete halt by imposing a full
embargo on all trade between the United States to Japan on 1 August 1941 , demanding that Japan withdraw
all troops from both China and Indochina . Japan was dependent on the United States for 80 percent of its oil ,
resulting in an economic and military crisis for Japan that could not continue its war effort with China without
access to petroleum and oil products . Attack
Passage 2
The U.S. embargoes gave Japan a sense of urgency . It would either have to agree to Washington ’s demands or
use force to gain access to the resources it needed .
Passage 3
Japan ’s goal after 1931 was economic dominance of most of East Asia , often expressed in Pan-Asian terms
of “ Asia for the Asians . ” . Japan was determined to dominate the China market , which the U.S. and other
European powers had been dominating . On October 19 , 1939 , the American Ambassador to Japan , Joseph C.
Grew , in a formal address to the America - Japan Society stated :
Responses
gpt3.5-turbo
The United States demanded a halt to trade with Japan in response to Japan’s aggressive actions in East Asia
and its invasion of China. The U.S. sought to put pressure on Japan to withdraw its troops from China and
Indochina by imposing an embargo on trade. Additionally, the U.S. was concerned about Japan’s growing
economic dominance in the region, particularly in the China market, which had previously been dominated by
the U.S. and other European powers. By cutting off trade, the U.S. hoped to weaken Japan’s war effort and force
a resolution to the conflict through diplomatic means.
mistral
The United States demanded trade with Japan not because it wanted to help Japan, but because it wanted to
ensure its own economic interests and prevent Japan from dominating the China market and expanding its
military power in the Pacific. The U.S. embargo on oil and other resources was a response to Japan’s aggression
in China and its refusal to withdraw its troops from that region. This economic pressure forced Japan into a
corner and ultimately led to its decision to attack the U.S. naval base at Pearl Harbor on December 7, 1941. The
attack was a desperate move by Japan to secure the resources it needed to continue its war efforts.
flan-t5-large
Japan was dependent on the United States for 80 percent of its oil, resulting in an economic and military crisis
for Japan that could not continue its war effort with China without access to petroleum and oil products.
Table 16: Example of the RAG setting where all retrieved passages are irrelevant. The question asks
about US demanding trade with Japan, while the passages talk about the reverse situation - US stopping
(embargo) trade with Japan or Japan wanting trade.
",Question: What percentage of its oil did Japan rely on the United States for?,Answer: 80 percent.
