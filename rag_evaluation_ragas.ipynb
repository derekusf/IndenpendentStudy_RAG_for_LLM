{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.6.2)\n",
      "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 41.0/81.3 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 1.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=7b3d376794383df8ce8e5b5631c506c17a8e98d1119d1cf85f32e87e18cc1c70\n",
      "  Stored in directory: c:\\users\\derek\\appdata\\local\\pip\\cache\\wheels\\03\\f5\\1a\\23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "base_docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2024-06-19', 'Title': 'R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation', 'Authors': 'Fuda Ye, Shuangyin Li, Yongqi Zhang, Lei Chen', 'Summary': \"Retrieval augmented generation (RAG) has been applied in many scenarios to\\naugment large language models (LLMs) with external documents provided by\\nretrievers. However, a semantic gap exists between LLMs and retrievers due to\\ndifferences in their training objectives and architectures. This misalignment\\nforces LLMs to passively accept the documents provided by the retrievers,\\nleading to incomprehension in the generation process, where the LLMs are\\nburdened with the task of distinguishing these documents using their inherent\\nknowledge. This paper proposes R$^2$AG, a novel enhanced RAG framework to fill\\nthis gap by incorporating Retrieval information into Retrieval Augmented\\nGeneration. Specifically, R$^2$AG utilizes the nuanced features from the\\nretrievers and employs a R$^2$-Former to capture retrieval information. Then, a\\nretrieval-aware prompting strategy is designed to integrate retrieval\\ninformation into LLMs' generation. Notably, R$^2$AG suits low-source scenarios\\nwhere LLMs and retrievers are frozen. Extensive experiments across five\\ndatasets validate the effectiveness, robustness, and efficiency of R$^2$AG. Our\\nanalysis reveals that retrieval information serves as an anchor to aid LLMs in\\nthe generation process, thereby filling the semantic gap.\"}\n",
      "{'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}\n",
      "{'Published': '2024-05-12', 'Title': 'DuetRAG: Collaborative Retrieval-Augmented Generation', 'Authors': 'Dian Jiao, Li Cai, Jingsheng Huang, Wenqiao Zhang, Siliang Tang, Yueting Zhuang', 'Summary': \"Retrieval-Augmented Generation (RAG) methods augment the input of Large\\nLanguage Models (LLMs) with relevant retrieved passages, reducing factual\\nerrors in knowledge-intensive tasks. However, contemporary RAG approaches\\nsuffer from irrelevant knowledge retrieval issues in complex domain questions\\n(e.g., HotPot QA) due to the lack of corresponding domain knowledge, leading to\\nlow-quality generations. To address this issue, we propose a novel\\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our\\nbootstrapping philosophy is to simultaneously integrate the domain fintuning\\nand RAG models to improve the knowledge retrieval quality, thereby enhancing\\ngeneration quality. Finally, we demonstrate DuetRAG' s matches with expert\\nhuman researchers on HotPot QA.\"}\n",
      "{'Published': '2023-12-09', 'Title': 'Context Tuning for Retrieval Augmented Generation', 'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\"}\n",
      "{'Published': '2024-02-16', 'Title': 'Corrective Retrieval Augmented Generation', 'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling', 'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\\naccuracy of generated texts cannot be secured solely by the parametric\\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\\ndocuments, raising concerns about how the model behaves if retrieval goes\\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\\nretrieval evaluator is designed to assess the overall quality of retrieved\\ndocuments for a query, returning a confidence degree based on which different\\nknowledge retrieval actions can be triggered. Since retrieval from static and\\nlimited corpora can only return sub-optimal documents, large-scale web searches\\nare utilized as an extension for augmenting the retrieval results. Besides, a\\ndecompose-then-recompose algorithm is designed for retrieved documents to\\nselectively focus on key information and filter out irrelevant information in\\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\\nRAG-based approaches. Experiments on four datasets covering short- and\\nlong-form generation tasks show that CRAG can significantly improve the\\nperformance of RAG-based approaches.'}\n"
     ]
    }
   ],
   "source": [
    "for doc in base_docs:\n",
    "  print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 1. text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = DOCUMENT+\"rag.txt\"\n",
    "txt_loader = TextLoader(txt_path)\n",
    "text_documents = txt_loader.load()\n",
    "#text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Type 2. PDF document\n",
    "\n",
    "We use PyMuPDFLoader in this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "pdf_path = DOCUMENT+ \"2005.11401v4.pdf\"\n",
    "pdf_loader = PyMuPDFLoader(pdf_path)\n",
    "pdf_documents = pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(text_documents)\n",
    "#documents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "pdf_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk Online Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "doc_chunks = text_splitter.split_documents(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_chunks + pdf_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using openAI embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to persist the vectordb with Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE\")\n",
    "vectordb = Chroma.from_documents(documents=doc_chunks,  embedding=embeddings, persist_directory=persist_directory)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipline 2. Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is retrieval augmented generation\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is. Here the on memory vectorstore is used. \n",
    "There is opportunity to improve efficiency of search when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever = vectorstore.as_retriever()\n",
    "\n",
    "#Load vectordb from persisted store\n",
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = os.getenv(\"ARXIVSTORE\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "newvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "retriever = newvectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Augmented Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Response Generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Using on-cloud OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = setup | prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval-augmented generation is a text generation approach that involves using retrieval sources, retrieval metrics, and generation models to enhance the generation process. It has shown remarkable advantages and achieved state-of-the-art performance in many natural language processing tasks.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(user_query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}, page_content='et al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm'),\n",
       " Document(metadata={'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}, page_content='et al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm'),\n",
       " Document(metadata={'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}, page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-'),\n",
       " Document(metadata={'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Published': '2022-02-13', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.', 'Title': 'A Survey on Retrieval-Augmented Text Generation'}, page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_retrieval = retriever.invoke(user_query)\n",
    "test_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG stands for Retrieval Augmented Generation, which is a framework that combines large language models (LLMs) with external documents provided by retrievers to improve performance in various tasks.\n",
      "To implement RAG (Retrieval Augmented Generation), you need to focus on enhancing tool retrieval, which can lead to improvements in plan generation. Additionally, you can experiment with different methods such as CoT, RECOMP, CRAG, Self-RAG, LongLLMLingua, and R^2AG to enhance the RAG framework.\n",
      "To evaluate RAG application, one can compare different methods such as standard RAG using various LLMs and enhanced RAG using the same foundation LLM. Additionally, one can evaluate standard RAG baselines where LLMs generate responses given the query prepended with retrieved documents. Experiments across multiple datasets can be conducted to validate the effectiveness, robustness, and efficiency of RAG applications.\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        user_input = input(\"Enter a query: \")\n",
    "        if user_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            response = chain.invoke(user_input)\n",
    "            print(response)\n",
    "        except Exception as err:\n",
    "            print('Exception occurred. Please try again', str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas\n",
      "  Downloading ragas-0.1.10-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (1.26.4)\n",
      "Collecting datasets (from ragas)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (0.2.6)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (0.2.11)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (0.2.6)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (0.1.14)\n",
      "Requirement already satisfied: openai>1 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (1.35.7)\n",
      "Collecting pysbd>=0.3.4 (from ragas)\n",
      "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from ragas) (1.6.0)\n",
      "Collecting appdirs (from ragas)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (2.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from openai>1->ragas) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (3.15.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->ragas)\n",
      "  Downloading pyarrow-16.1.0-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->ragas)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets->ragas)\n",
      "  Using cached pandas-2.2.2-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (2.32.3)\n",
      "Collecting xxhash (from datasets->ragas)\n",
      "  Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->ragas)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->ragas)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (0.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from datasets->ragas) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain->ragas) (2.0.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain->ragas) (0.2.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain->ragas) (0.1.81)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain->ragas) (8.4.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from tiktoken->ragas) (2024.5.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from aiohttp->datasets->ragas) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (2.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from requests>=2.32.2->datasets->ragas) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from tqdm>4->openai>1->ragas) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from pandas->datasets->ragas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets->ragas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets->ragas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\derek\\onedrive\\1 - technology\\workspace\\rag_win\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
      "Downloading ragas-0.1.10-py3-none-any.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.5/91.5 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.1/71.1 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "   ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 122.9/547.8 kB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 317.4/547.8 kB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 512.0/547.8 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 547.8/547.8 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "   ---------------------------------------- 0.0/316.1 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 174.1/316.1 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 316.1/316.1 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp312-cp312-win_amd64.whl (25.8 MB)\n",
      "   ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/25.8 MB 8.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.3/25.8 MB 4.2 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.5/25.8 MB 4.1 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.7/25.8 MB 4.2 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.9/25.8 MB 4.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.2/25.8 MB 4.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.5/25.8 MB 4.7 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.5/25.8 MB 4.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.8/25.8 MB 4.4 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.2/25.8 MB 4.8 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.6/25.8 MB 5.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.9/25.8 MB 5.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.3/25.8 MB 5.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.6/25.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.8/25.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.1/25.8 MB 5.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.5/25.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.6/25.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/25.8 MB 5.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.0/25.8 MB 5.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.3/25.8 MB 5.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.7/25.8 MB 5.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.2/25.8 MB 5.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.3/25.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.7/25.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.2/25.8 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.4/25.8 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.9/25.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.9/25.8 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.0/25.8 MB 5.7 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.5/25.8 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.6/25.8 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.1/25.8 MB 5.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.6/25.8 MB 5.2 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 9.9/25.8 MB 5.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.8 MB 5.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 10.6/25.8 MB 5.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.9/25.8 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.4/25.8 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.9/25.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.5/25.8 MB 6.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.9/25.8 MB 6.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 13.4/25.8 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.9/25.8 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.3/25.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.8/25.8 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 15.2/25.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/25.8 MB 6.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.9/25.8 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.4/25.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.6/25.8 MB 6.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.0/25.8 MB 6.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.0/25.8 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.2/25.8 MB 6.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.6/25.8 MB 6.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 18.0/25.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.5/25.8 MB 7.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.9/25.8 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.4/25.8 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.9/25.8 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.4/25.8 MB 8.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.6/25.8 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.1/25.8 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.5/25.8 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.3/25.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.8/25.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.1/25.8 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.4/25.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.9/25.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.3/25.8 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.7/25.8 MB 8.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.1/25.8 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.4/25.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.8/25.8 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.8/25.8 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 122.9/146.7 kB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 146.7/146.7 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.2-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/11.5 MB 8.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/11.5 MB 9.6 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/11.5 MB 9.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/11.5 MB 10.6 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.5/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.5/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.5 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.3/11.5 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.8/11.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.3/11.5 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.5 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.4/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.9/11.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.4/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.9/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.8/11.5 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.3/11.5 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.8/11.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp312-cp312-win_amd64.whl (29 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "   -------------------------- ------------ 348.2/505.5 kB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 505.5/505.5 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 345.4/345.4 kB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, appdirs, xxhash, tzdata, pysbd, pyarrow-hotfix, pyarrow, fsspec, dill, pandas, multiprocess, datasets, ragas\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed appdirs-1.4.4 datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pandas-2.2.2 pyarrow-16.1.0 pyarrow-hotfix-0.6 pysbd-0.3.4 pytz-2024.1 ragas-0.1.10 tzdata-2024.1 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthesis Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# documents = load your documents\n",
    "\n",
    "# generator with openai models\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0) \n",
    "critic_llm = ChatOpenAI(model=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.                   \n",
      "Generating: 100%|██████████| 5/5 [01:11<00:00, 14.28s/it]\n"
     ]
    }
   ],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "# Change resulting question type distribution\n",
    "distributions = {\n",
    "    simple: 0.5,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.1\n",
    "}\n",
    "\n",
    "try:\n",
    "    testset = generator.generate_with_langchain_docs(base_docs, test_size=5, distributions = distributions) \n",
    "except Exception as e:\n",
    "    print (e)\n",
    "\n",
    "# use generator.generate_with_llamaindex_docs if you use llama-index as document loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler Testset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\AppData\\Local\\Temp\\ipykernel_32152\\3495969448.py:1: DeprecationWarning: The function with_openai was deprecated in 0.1.4, and will be removed in the 0.2.0 release. Use from_langchain instead.\n",
      "  simple_generator = TestsetGenerator.with_openai()\n",
      "Exception in thread Thread-79:                                      \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1073, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 87, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 631, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 109, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\executor.py\", line 104, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 691, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 651, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"C:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 836, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 674, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1289, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\openai\\_base_client.py\", line 1599, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-16k in organization org-IK8mpHva4T4MJMeZULAv14xw on tokens per min (TPM): Limit 60000, Used 59707, Requested 314. Please try again in 21ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m simple_generator \u001b[38;5;241m=\u001b[39m TestsetGenerator\u001b[38;5;241m.\u001b[39mwith_openai()\n\u001b[1;32m----> 3\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43msimple_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43msimple\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_context\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[1;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[0;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    217\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\docstore.py:215\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[1;34m(self, docs, show_progress)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[0;32m    211\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    212\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[0;32m    214\u001b[0m ]\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\ragas\\testset\\docstore.py:254\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m    252\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:362: RuntimeWarning: coroutine 'Executor.wrap_callable_with_index.<locals>.wrapped_callable_async' was never awaited\n",
      "  self._tasks: set[asyncio.Task] = set()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "simple_generator = TestsetGenerator.with_openai()\n",
    "\n",
    "testset = simple_generator.generate_with_langchain_docs(doc_chunks, test_size=10, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What challenges do large language models (LLMs...</td>\n",
       "      <td>[Corrective Retrieval Augmented Generation\\nSh...</td>\n",
       "      <td>Large language models (LLMs) face challenges s...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'Published': '2024-02-16', 'Title': 'Correct...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does incorporating relevant context in pla...</td>\n",
       "      <td>[etuned Semantic\\nSearch\\n73.48\\n88.52\\n95.13\\...</td>\n",
       "      <td>Incorporating relevant context in plan generat...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'Published': '2023-12-09', 'Title': 'Context...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the retrieval evaluator in CRAG impro...</td>\n",
       "      <td>[Corrective Retrieval Augmented Generation\\nSh...</td>\n",
       "      <td>The retrieval evaluator in CRAG assesses the o...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'Published': '2024-02-16', 'Title': 'Correct...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What LLM with a 32k token limit powers DuReade...</td>\n",
       "      <td>[\\n0.0265\\n0.0830\\n0.0156\\n0.2666\\n0.0329\\nCRA...</td>\n",
       "      <td>The foundation LLM for DuReader's improved RAG...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'Published': '2024-06-19', 'Title': 'R^2AG: ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What foundation LLM is used for the DuReader d...</td>\n",
       "      <td>[\\n0.0265\\n0.0830\\n0.0156\\n0.2666\\n0.0329\\nCRA...</td>\n",
       "      <td>The foundation LLM used for the DuReader datas...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'Published': '2024-06-19', 'Title': 'R^2AG: ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What challenges do large language models (LLMs...   \n",
       "1  How does incorporating relevant context in pla...   \n",
       "2  How does the retrieval evaluator in CRAG impro...   \n",
       "3  What LLM with a 32k token limit powers DuReade...   \n",
       "4  What foundation LLM is used for the DuReader d...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Corrective Retrieval Augmented Generation\\nSh...   \n",
       "1  [etuned Semantic\\nSearch\\n73.48\\n88.52\\n95.13\\...   \n",
       "2  [Corrective Retrieval Augmented Generation\\nSh...   \n",
       "3  [\\n0.0265\\n0.0830\\n0.0156\\n0.2666\\n0.0329\\nCRA...   \n",
       "4  [\\n0.0265\\n0.0830\\n0.0156\\n0.2666\\n0.0329\\nCRA...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Large language models (LLMs) face challenges s...         simple   \n",
       "1  Incorporating relevant context in plan generat...         simple   \n",
       "2  The retrieval evaluator in CRAG assesses the o...  multi_context   \n",
       "3  The foundation LLM for DuReader's improved RAG...  multi_context   \n",
       "4  The foundation LLM used for the DuReader datas...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'Published': '2024-02-16', 'Title': 'Correct...          True  \n",
       "1  [{'Published': '2023-12-09', 'Title': 'Context...          True  \n",
       "2  [{'Published': '2024-02-16', 'Title': 'Correct...          True  \n",
       "3  [{'Published': '2024-06-19', 'Title': 'R^2AG: ...          True  \n",
       "4  [{'Published': '2024-06-19', 'Title': 'R^2AG: ...          True  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation on our RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = testset.to_pandas()[\"question\"].to_list()\n",
    "ground_truth = testset.to_pandas()[\"ground_truth\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What challenges do large language models (LLMs) face that Corrective Retrieval Augmented Generation (CRAG) aims to address?',\n",
       " 'How does incorporating relevant context in plan generation help reduce hallucination in the context-tuned planner?',\n",
       " 'How does the retrieval evaluator in CRAG improve doc use for targeted knowledge creation?',\n",
       " \"What LLM with a 32k token limit powers DuReader's improved RAG, and how does its F1 score match up?\",\n",
       " 'What foundation LLM is used for the DuReader dataset, and how does its performance compare with other methods?']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Large language models (LLMs) face challenges such as hallucinations, factual errors, and the inability to secure the accuracy of generated texts solely by the parametric knowledge they encapsulate. Corrective Retrieval Augmented Generation (CRAG) aims to address these challenges by improving the robustness of generation through a lightweight retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm for retrieved documents.',\n",
       " 'Incorporating relevant context in plan generation helps reduce hallucination in the context-tuned planner, as evidenced by the upper bound, which effectively employs oracle retrievers.',\n",
       " 'The retrieval evaluator in CRAG assesses the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. This allows for selective focus on key information and filtering out irrelevant information in the retrieved documents, thereby improving the utilization of documents for targeted knowledge creation.',\n",
       " \"The foundation LLM for DuReader's improved RAG is Qwen1.50.5B with a 32k token limit, and its F1 score is 0.1395.\",\n",
       " 'The foundation LLM used for the DuReader dataset is Qwen1.50.5B, with a maximum context length of 32k tokens. It is categorized under frozen LLMs. In terms of performance, RAFT, a fine-tuned LLM, has an F1 score of 0.2423 and a Rouge score of 0.2740, while R2AG+RAFT, another fine-tuned LLM, shows slightly better performance with an F1 score of 0.2507 and a Rouge score of 0.2734. These results indicate that fine-tuned LLMs outperform the foundation LLM Qwen1.50.5B on the DuReader dataset.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\OneDrive\\1 - Technology\\Workspace\\rag_win\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "for query in questions:\n",
    "    data[\"question\"].append(query)\n",
    "    data[\"answer\"].append(chain.invoke(query))\n",
    "    data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}, page_content='5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed'),\n",
       " Document(metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}, page_content='5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed'),\n",
       " Document(metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}, page_content='denced by the upper bound, helps in reducing hal-\\nlucination.\\n5\\nConclusion\\nOur work introduces context tuning, a novel compo-\\nnent that enhances RAG-based planning by equip-\\nping it with essential context-seeking capabilities'),\n",
       " Document(metadata={'Authors': 'Raviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi', 'Published': '2023-12-09', 'Summary': \"Large language models (LLMs) have the remarkable ability to solve new tasks\\nwith just a few examples, but they need access to the right tools. Retrieval\\nAugmented Generation (RAG) addresses this problem by retrieving a list of\\nrelevant tools for a given task. However, RAG's tool retrieval step requires\\nall the required information to be explicitly present in the query. This is a\\nlimitation, as semantic search, the widely adopted tool retrieval method, can\\nfail when the query is incomplete or lacks context. To address this limitation,\\nwe propose Context Tuning for RAG, which employs a smart context retrieval\\nsystem to fetch relevant information that improves both tool retrieval and plan\\ngeneration. Our lightweight context retrieval model uses numerical,\\ncategorical, and habitual usage signals to retrieve and rank context items. Our\\nempirical results demonstrate that context tuning significantly enhances\\nsemantic search, achieving a 3.5-fold and 1.5-fold improvement in Recall@K for\\ncontext retrieval and tool retrieval tasks respectively, and resulting in an\\n11.6% increase in LLM-based planner accuracy. Additionally, we show that our\\nproposed lightweight model using Reciprocal Rank Fusion (RRF) with LambdaMART\\noutperforms GPT-4 based retrieval. Moreover, we observe context augmentation at\\nplan generation, even after tool retrieval, reduces hallucination.\", 'Title': 'Context Tuning for Retrieval Augmented Generation'}, page_content='denced by the upper bound, helps in reducing hal-\\nlucination.\\n5\\nConclusion\\nOur work introduces context tuning, a novel compo-\\nnent that enhances RAG-based planning by equip-\\nping it with essential context-seeking capabilities')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truth'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 20/20 [00:09<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What challenges do large language models (LLMs) face that Corrective Retrieval Augmented Generation (CRAG) aims to address?</td>\n",
       "      <td>[show that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have attracted\\nincreasing attention and exhibited impressive abili-\\nties to understand instructions and generate fluent, show that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have attracted\\nincreasing attention and exhibited impressive abili-\\nties to understand instructions and generate fluent, covering short- and long-form generation tasks\\nshow that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have att...</td>\n",
       "      <td>Large language models (LLMs) face challenges of hallucinations, where the accuracy of generated texts cannot be guaranteed solely by the parametric knowledge they encapsulate. Corrective Retrieval Augmented Generation (CRAG) aims to improve the robustness of generation by addressing concerns about the relevance of retrieved documents and potential issues if retrieval goes wrong.</td>\n",
       "      <td>Large language models (LLMs) face challenges such as hallucinations, factual errors, and the inability to secure the accuracy of generated texts solely by the parametric knowledge they encapsulate. Corrective Retrieval Augmented Generation (CRAG) aims to address these challenges by improving the robustness of generation through a lightweight retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm for retrieved documents.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does incorporating relevant context in plan generation help reduce hallucination in the context-tuned planner?</td>\n",
       "      <td>[5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed, 5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed, denced by the upper bound, helps in reducing hal-\\nlucination.\\n5\\nConclusion\\nOur work introduces context tuning, a novel compo-\\nnent that enhances RAG-based planning by equip-\\nping it wit...</td>\n",
       "      <td>Incorporating relevant context in plan generation helps reduce hallucination in the context-tuned planner by providing essential information and signals that guide the generation process, leading to more accurate and informed decisions.</td>\n",
       "      <td>Incorporating relevant context in plan generation helps reduce hallucination in the context-tuned planner, as evidenced by the upper bound, which effectively employs oracle retrievers.</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the retrieval evaluator in CRAG improve doc use for targeted knowledge creation?</td>\n",
       "      <td>[retrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-, retrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-, heavily on the relevance of retrieved docu-\\nments, raising concerns about how the model\\nbehaves if retrieval goes wrong. To this end, we\\npropose the Corrective Retrieval Augmented\\nGeneration (CRAG) to improve the robustness\\nof generation., heavily on the relevance of retrieved do...</td>\n",
       "      <td>The retrieval evaluator in CRAG assesses the overall quality of retrieved documents for a query and returns a confidence degree based on which different knowledge retrieval actions can be triggered. This helps in selectively focusing on key information and filtering out irrelevant information in the retrieved documents, thus improving the use of documents for targeted knowledge creation.</td>\n",
       "      <td>The retrieval evaluator in CRAG assesses the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. This allows for selective focus on key information and filtering out irrelevant information in the retrieved documents, thereby improving the utilization of documents for targeted knowledge creation.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What LLM with a 32k token limit powers DuReader's improved RAG, and how does its F1 score match up?</td>\n",
       "      <td>[LLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains\\neffective, demonstrating its robustness and effi-\\nciency in handling extensive text outputs. These re-, LLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains\\neffective, demonstrating its robustness and effi-\\nciency in handling extensive text outputs. These re-, (1) Compared with foundation LLMs using stan-\\ndard RAG, R2AG can significantly increase perfor-\\nmance. Even in multi-hot datasets, R2AG improves\\nLLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains, (1) Compared with foundation LLMs ...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>The foundation LLM for DuReader's improved RAG is Qwen1.50.5B with a 32k token limit, and its F1 score is 0.1395.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What foundation LLM is used for the DuReader dataset, and how does its performance compare with other methods?</td>\n",
       "      <td>[Table 2: Performance comparison on DuReader dataset.\\nas the foundation LLM for enhanced RAG methods,\\nwhich has a maximum context length of 4k tokens.\\nFor NQ-20 and NQ-30 datasets, LongChat1.57B\\nis selected as the foundation LLM, which extends, Table 2: Performance comparison on DuReader dataset.\\nas the foundation LLM for enhanced RAG methods,\\nwhich has a maximum context length of 4k tokens.\\nFor NQ-20 and NQ-30 datasets, LongChat1.57B\\nis selected as the foundation LLM, which extends, datasets. For DuReader dataset, we measure per-\\nformance by F1 score and Rouge (Lin, 2004).\\n4.2\\nBaselines\\nTo fully evaluate R2AG, we compared two types of\\nmethods: standard RAG using various LLM...</td>\n",
       "      <td>The foundation LLM used for the DuReader dataset is LongChat1.57B, and its performance is compared with other methods in terms of F1 score and Rouge (Lin, 2004).</td>\n",
       "      <td>The foundation LLM used for the DuReader dataset is Qwen1.50.5B, with a maximum context length of 32k tokens. It is categorized under frozen LLMs. In terms of performance, RAFT, a fine-tuned LLM, has an F1 score of 0.2423 and a Rouge score of 0.2740, while R2AG+RAFT, another fine-tuned LLM, shows slightly better performance with an F1 score of 0.2507 and a Rouge score of 0.2734. These results indicate that fine-tuned LLMs outperform the foundation LLM Qwen1.50.5B on the DuReader dataset.</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      question  \\\n",
       "0  What challenges do large language models (LLMs) face that Corrective Retrieval Augmented Generation (CRAG) aims to address?   \n",
       "1           How does incorporating relevant context in plan generation help reduce hallucination in the context-tuned planner?   \n",
       "2                                    How does the retrieval evaluator in CRAG improve doc use for targeted knowledge creation?   \n",
       "3                          What LLM with a 32k token limit powers DuReader's improved RAG, and how does its F1 score match up?   \n",
       "4               What foundation LLM is used for the DuReader dataset, and how does its performance compare with other methods?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      contexts  \\\n",
       "0  [show that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have attracted\\nincreasing attention and exhibited impressive abili-\\nties to understand instructions and generate fluent, show that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have attracted\\nincreasing attention and exhibited impressive abili-\\nties to understand instructions and generate fluent, covering short- and long-form generation tasks\\nshow that CRAG can significantly improve the\\nperformance of RAG-based approaches.1\\n1\\nIntroduction\\nLarge language models (LLMs) have att...   \n",
       "1  [5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed, 5. We show that context augmentation at plan\\ngeneration reduces hallucinations.\\n2\\nRelated Work\\nUsing retrieval to incorporate tools into plan gen-\\neration with LLMs has emerged as a burgeoning\\narea of research, with ongoing investigations aimed, denced by the upper bound, helps in reducing hal-\\nlucination.\\n5\\nConclusion\\nOur work introduces context tuning, a novel compo-\\nnent that enhances RAG-based planning by equip-\\nping it wit...   \n",
       "2  [retrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-, retrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly im-, heavily on the relevance of retrieved docu-\\nments, raising concerns about how the model\\nbehaves if retrieval goes wrong. To this end, we\\npropose the Corrective Retrieval Augmented\\nGeneration (CRAG) to improve the robustness\\nof generation., heavily on the relevance of retrieved do...   \n",
       "3  [LLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains\\neffective, demonstrating its robustness and effi-\\nciency in handling extensive text outputs. These re-, LLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains\\neffective, demonstrating its robustness and effi-\\nciency in handling extensive text outputs. These re-, (1) Compared with foundation LLMs using stan-\\ndard RAG, R2AG can significantly increase perfor-\\nmance. Even in multi-hot datasets, R2AG improves\\nLLMs’ ability for complex reasoning. In DuReader\\ndataset, with a token length of 16k, R2AG remains, (1) Compared with foundation LLMs ...   \n",
       "4  [Table 2: Performance comparison on DuReader dataset.\\nas the foundation LLM for enhanced RAG methods,\\nwhich has a maximum context length of 4k tokens.\\nFor NQ-20 and NQ-30 datasets, LongChat1.57B\\nis selected as the foundation LLM, which extends, Table 2: Performance comparison on DuReader dataset.\\nas the foundation LLM for enhanced RAG methods,\\nwhich has a maximum context length of 4k tokens.\\nFor NQ-20 and NQ-30 datasets, LongChat1.57B\\nis selected as the foundation LLM, which extends, datasets. For DuReader dataset, we measure per-\\nformance by F1 score and Rouge (Lin, 2004).\\n4.2\\nBaselines\\nTo fully evaluate R2AG, we compared two types of\\nmethods: standard RAG using various LLM...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                   answer  \\\n",
       "0           Large language models (LLMs) face challenges of hallucinations, where the accuracy of generated texts cannot be guaranteed solely by the parametric knowledge they encapsulate. Corrective Retrieval Augmented Generation (CRAG) aims to improve the robustness of generation by addressing concerns about the relevance of retrieved documents and potential issues if retrieval goes wrong.   \n",
       "1                                                                                                                                                            Incorporating relevant context in plan generation helps reduce hallucination in the context-tuned planner by providing essential information and signals that guide the generation process, leading to more accurate and informed decisions.   \n",
       "2  The retrieval evaluator in CRAG assesses the overall quality of retrieved documents for a query and returns a confidence degree based on which different knowledge retrieval actions can be triggered. This helps in selectively focusing on key information and filtering out irrelevant information in the retrieved documents, thus improving the use of documents for targeted knowledge creation.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                           I don't know.   \n",
       "4                                                                                                                                                                                                                                       The foundation LLM used for the DuReader dataset is LongChat1.57B, and its performance is compared with other methods in terms of F1 score and Rouge (Lin, 2004).   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ground_truth  \\\n",
       "0                                      Large language models (LLMs) face challenges such as hallucinations, factual errors, and the inability to secure the accuracy of generated texts solely by the parametric knowledge they encapsulate. Corrective Retrieval Augmented Generation (CRAG) aims to address these challenges by improving the robustness of generation through a lightweight retrieval evaluator, large-scale web searches, and a decompose-then-recompose algorithm for retrieved documents.   \n",
       "1                                                                                                                                                                                                                                                                                                                      Incorporating relevant context in plan generation helps reduce hallucination in the context-tuned planner, as evidenced by the upper bound, which effectively employs oracle retrievers.   \n",
       "2                                                                                                 The retrieval evaluator in CRAG assesses the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. This allows for selective focus on key information and filtering out irrelevant information in the retrieved documents, thereby improving the utilization of documents for targeted knowledge creation.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                             The foundation LLM for DuReader's improved RAG is Qwen1.50.5B with a 32k token limit, and its F1 score is 0.1395.   \n",
       "4  The foundation LLM used for the DuReader dataset is Qwen1.50.5B, with a maximum context length of 32k tokens. It is categorized under frozen LLMs. In terms of performance, RAFT, a fine-tuned LLM, has an F1 score of 0.2423 and a Rouge score of 0.2740, while R2AG+RAFT, another fine-tuned LLM, shows slightly better performance with an F1 score of 0.2507 and a Rouge score of 0.2734. These results indicate that fine-tuned LLMs outperform the foundation LLM Qwen1.50.5B on the DuReader dataset.   \n",
       "\n",
       "   faithfulness  \n",
       "0          0.00  \n",
       "1          0.25  \n",
       "2          0.00  \n",
       "3          0.00  \n",
       "4          0.00  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result_pd = result.to_pandas()\n",
    "pd.set_option(\"display.max_colwidth\", 700)\n",
    "result_pd[[\"question\", \"contexts\", \"answer\", \"ground_truth\",\"faithfulness\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
